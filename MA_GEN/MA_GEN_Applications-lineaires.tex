\chapter{Applications linéaires}
\chaptertoc

\section[Généralités]{Généralités sur les applications linéaires}

\subsection{Définition}

    \begin{defi}{Application linéaire}{}
        \begin{soient}
            \begin{multicols}{2}
                \item $E$ et $F$ deux $\mathbb{K}$-ev 
                \item $f : E \to F$
            \end{multicols}
        \end{soient}
        On dit que $f$ est \textbf{linéaire} si l’une de ces conditions équivalentes est vérifiée :
        \begin{enumerate}[label=\textcolor{myyellow}{\textit{(\alph*)}}]
            \item \begin{itemize}
                \item $\forall x,y \in E, f(x+y) = f(x) + f(y)$
                \item $\forall \lambda \in \mathbb{K}, \forall x \in E, f(\lambda x) = \lambda f(x)$
            \end{itemize}
            \item $\forall \lambda,\mu \in \mathbb{K}, \forall x,y \in E, f(\lambda x + \mu y)= \lambda f(x) + \mu f(y)$
            \item $\forall \lambda \in \mathbb{K}, \forall x,y \in E, f( x + \lambda y)= f(x) + \lambda f(y)$
        \end{enumerate}
        On note $\mathcal{L}(E,F)$ l’ensemble des applications linéaires de $F$ dans $E$, qui est un sev de $\mathcal{F}(E,F)$.

        \textbf{Vocabulaire des applications linéaires}
        \begin{enumerate}[label=\textcolor{myyellow}{(\arabic*)}]
            \item On dit que $f$ est une \textbf{forme linéaire} si $F = \mathbb{K}$.
            \item On dit que $f$ est un \textbf{endomorphisme} si $E = F$ (On note alors $f \in \mathcal{L}(E)$).
            \item On dit que $f$ est un \textbf{isomorphisme} si $f$ est bijective. 
            \item On dit que $f$ est un \textbf{automorphisme} si $E = F$ et $f$ est bijective (On note alors $f \in \GL(E)$).
        \end{enumerate}
        On remarque que $(\GL(E),\circ)$ est un groupe, non commutatif.
    \end{defi}

    \begin{demo}{Justification}{myyellow}
        Par démonstration tournante, on ne peine pas à montrer l’équivalence de ces propositions.

        Le seul point « délicat » est que $f(0) = 0$. Pour cela, on retiendra que $f(0 + 0) = f(0) + f(0)$.

        Pour montrer que $\GL(E),\circ$ est un groupe, on utilise la propriété un peu plus générale pour les isomorphismes $f \in \mathcal{L}(E,F) \implies f^{-1} \in \mathcal{L}(F,E)$. En effet, 
        \begin{align*}
            f \left(f^{-1} (\lambda x + y)\right) &= \lambda x + y \\
            f \left(\lambda f^{-1}(x) + f^{-1}(y)\right) &= \lambda f(f^{-1}(x)) + f (f^{-1}(y)) \\
             & = \lambda x + y 
        \end{align*}
        et $f$ est injective.
    \end{demo}

    \begin{prop}{Image et image réciproque d’un sev par une application linéaire}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
            \item $E'$ un sev de $E$
            \item $F'$ un sev de $F$
        \end{soient}
        \begin{alors}
            \item $f(E')$ est un sev de $F$.
            \item $f^{-1}(F')$ est un sev de $E$.
        \end{alors}
    \end{prop}
    
    \begin{demo}{Idée}{myolive}
        Écrire à chaque fois les trois points que vérifie un sev.
    \end{demo}

\subsection{Image et noyau d’une AL}

    \begin{defi}{Noyau et image d’une application linéaire}{}
        Soit $f \in \mathcal{L}(E,F)$.

        On appelle
        \begin{itemize}
            \item \textbf{noyau} de $f$ l’ensemble des $x \in E$ tel que $f(x) = 0$. On note 
            \begin{align*}
                \ker(f) &= \big\{ x \in E, f(x) = 0 \big\} \\
                &= f^{-1}\left(\{0\}\right)
            \end{align*}
            \item \textbf{image} de $f$ l’ensemble noté $\im(f)$ défini par 
            \begin{align*}
                \im(f) &= \big\{ y \in F, \exists \in E, f(x) = y \big\} \\
                &= f(E)
            \end{align*}
        \end{itemize}
    \end{defi}

    \begin{prop}{Image d’une CL, d’une famille génératrice, d’une famille libre}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
            \item $\mathcal{F} = (x_1, \ldots, x_s) \in E^s$
            \item $(\lambda_1, \ldots, \lambda_s) \in \mathbb{K}^s$
        \end{soient}
        \begin{alors}
            \item $f\left(\sum\limits_{i=1}^{s} \lambda_i x_i\right) = \sum\limits_{i=1}^{s} \lambda_i f(x_i)$
            \item $f \left( \vect(x_1, \ldots, x_s) \right) = \vect \left( f(x_1), \ldots, f(x_s) \right) = \vect\left(f(\mathcal{F})\right)$.
            \item Si $\mathcal{F}$ est libre, et $f$ injective, alors 
            \[ (f(x_1), \ldots, f(x_s)) \text{ est libre} \] 
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item Par linéarité de $f$.
            \item $ E = \vect(\mathcal{F})$ donc $\im(f) = f \left( \vect(\mathcal{F})\right) = \vect(f(\mathcal{F}))$.
            \item Soit $(\lambda_1 ,\ldots,\lambda_s)$ tq $\sum\limits_{k=1}^s \lambda_k f(x_k) = 0_F$
    
            Par linéarité de $f$, \[ \sum\limits_{k=1}^s \lambda_k f(x_k) = f \left(\sum\limits_{k=1}^s \lambda_k x_k\right)= 0_F \] 
            Or $f$ est injective donc $\sum\limits_{k=1}^s \lambda_k x_k = 0_E$ et la famille $(x_1,\ldots,x_s)$ est libre donc 
            \[ \lambda_1 = \ldots = \lambda_s = 0 \]
        \end{enumerate}
    \end{demo}

    \begin{prop}{Caractérisation des AL injectives et surjectives}{}
        Soit $f \in \mathcal{L}(E,F)$.
        
        \begin{alors}
            \item $\ker(f)$ est un sev de $E$ et $f$ est injective \textit{ssi} $\ker(f) = \{0\}$.
            \item $\im(f)$ est un sev de $F$ et $f$ est surjective \textit{ssi} $\im(f) = F$.
        \end{alors}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item On sait que $f(0) = 0$, donc $0 \in \ker(f)$. De plus, si $x,y \in \ker(f)$ et $\lambda \in \mathbb{K}$, 
            \[ f(x+\lambda y) = f(x) + \lambda f(y) = 0 \]   
            donc $x + \lambda y \in \ker(f)$, d’où $\ker(f)$ est un sev de $E$.

            \begin{itemize}
                \item Si $f$ est injective et $x \in \ker(f)$, alors $f(x) = 0$ et $f(0) = 0$ donc par injectivité de $f$, $x = 0$ \textit{i.e.} $\ker(f) = \{0\}$.
                \item Si $\ker(f) = \{0\}$ et $x,x' \in E$ tels que $f(x) = f(x')$. Alors 
                \[ f(x - x') = f(x) - f(x') = 0 \]   
                donc $x - x' \in \ker(f)$ \textit{i.e.} $x = x'$ d’où $f$ est injective.
            \end{itemize}
            \item $f(0) = 0$ donc $0 \in \im(f)$. Soient $y,y' \in \im(f)$ et $\lambda \in \mathbb{K}$. Il existe $x, x' \in E$ tels que 
            \[ \et{f(x) = y}{f(x') = y'} \]
            Donc 
            \begin{align*}
                y + \lambda y' 
                &= f(x) + \lambda f(x') \\
                &= f(x + \lambda x')
            \end{align*}
            Donc $y + \lambda y' \in \im(f)$.

            De plus, $f$ est sujective \textit{ssi} $f(E) = F$ \textit{i.e. ssi} $\im(f) = F$
        \end{enumerate}
    \end{demo}

    \begin{omed}{Application \textcolor{black}{(Sur un espace de polynômes)}}{myolive}
        Soit $E = \mathbb{C}[X]$, $p$ un entier naturel et $f$ l’application de $E$ dans $E$ définie par $f(P) = (1 - pX)P + X^2 P'$. $f$ est-elle injective ? surjective ?

        Il est facile de vérifier que $f$ est linéaire. 
        
        Pour savoir si elle est injective, calculons sont noyau. Soit $P = \sum_{i=0}^{n} a_i X^i$ un polynôme. Alors 
        \[ f(P) = a_0 + \sum_{i=1}^{n} (a_i - (p+1-i))a_{i-1} X^i + (n-p)a_n X^{n+1} \]   
        Ainsi, si $f(P) = 0$, on a $a_0 = 0$ et $a_i = (p + 1 - i)a_{i - 1}$ pour $i \in \intervalleEntier{1}{n}$, d’où $a_i = 0$. Ainsi, $P = 0$, et $f$ est injective.
        
        D’autre part, si $P$ est un polynôme de degré $n$, alors on remarque que 
        \begin{itemize}
            \item pour $n = p$, le degré de $f(P)$ est inférieur ou égal à $n$, et donc différent de $p+1$ ;
            \item pour $n \neq p$, $f(P)$ est de degré $n + 1 \neq p+ 1$
        \end{itemize}
        Ainsi, $\im(f)$ ne contient pas de polynômes de degré $p + 1$, donc $f$ n’est pas surjective.
    \end{omed}

\subsection{Applications linéaires en dimension finie}

    \subsubsection{Construction et caractérisation}

    \begin{theo}{Construction d’une application linéaire}{}
        \begin{soient}
            \item $E,F$ deux $\mathbb{K}$-ev de dimension finie
            \item $\mathcal{B} = (e_1, \ldots, e_n)$ une base de $E$
            \item $\mathcal{F} = (f_1,\ldots,f_n)$ une famille de $n$ vecteurs de $F$ 
        \end{soient}
        \begin{alors}
            \item Il existe une unique application linéaire $f \in \mathcal{L}(E,F)$ telle que 
            \[ \forall i \in \intervalleEntier{1}{n}, f(e_i) = f_i \]
            \item $f$ est injective \textit{ssi} $\mathcal{F}$ est libre.
            \item $f$ est surjective \textit{ssi} $\mathcal{F}$ est génératrice.
        \end{alors}
    \end{theo}

    \begin{demo}{Preuve}{myred}
        \begin{itemize}
            \item \textbf{Existence} \quad Soit $x \in E$. Comme $\mathcal{B}$ est une base de $E$, il existe $(\lambda_1, \ldots, \lambda_n) \in \mathbb{K}^n$ tels que 
            \[ x = \sum_{i=1}^{n} \lambda_i x_i \]
            On pose $f(x) = \sum_{i=1}^{n} \lambda_i f_i$, avec $f : E \to F$. Pour tout $i \in \intervalleEntier{1}{n}$, $f(e_i) = f_i$.

            Soient $x,y \in E$ et $\alpha \in \mathbb{K}$. On montre aisément que $f(x  + \alpha y) = f(x) + \alpha f(y)$, d’où $f$ est linéaire.
            \item \textbf{Unicité} \quad Soit $g \in \mathcal{L}(E,F)$ respectant les mêmes propriétés que $f$. Soit $x \in E$.
            \begin{align*}
                f(x)
                &= \sum_{i=1}^{n} \lambda_i f_i \\
                &= \sum_{i=1}^{n} \lambda_i g(e_i) \\
                &= g\left(\sum_{i=1}^{n} \lambda_i e_i\right) \\
                &= g(x)
            \end{align*}
            \item \textbf{Surjectivité} \quad 
            \begin{align*}
                f \text{ est surjective}
                &\iff \im(f) = F \\
                &\iff f(E) = F \\
                &\quad \downarrow \quad \mathcal{B} \text{ génératrice} \\
                &\iff f\left(\vect(e_1,\ldots,e_n)\right) = F \\
                &\quad \downarrow \quad f \text{ est linéaire} \\
                &\iff \vect\left(f_1,\ldots,f_n\right) = F \\
                &\iff \mathcal{F} \text{ est génératrice}
            \end{align*}
            \item \textbf{Injectivité} \quad Si $f$ est injective, et $(\lambda_1, \ldots, \lambda_n) \in \mathbb{K}^n$ tels que 
            \[ \sum_{i=1}^{n} \lambda_i f_i = 0 \quad \left(\iff f\left( \sum_{i=1}^{n} \lambda_i e_i \right) = 0 = f(0)\right) \]
            alors $\sum_{i=1}^{n} \lambda_i e_i = 0$, et comme $\mathcal{B}$ est une base (donc est libre), $\lambda_1,\ldots,\lambda_n = 0$ donc $(f_1,\ldots,f_n$) est libre. 

            Réciproquement, si $(f_1,\ldots,f_n)$ est libre et $x \in \ker(f)$, $f(x) = \sum_{i=1}^{n} \lambda_i f_i= 0$ d’où $\lambda_1,\ldots,\lambda_n = 0$ \textit{i.e.} $x = 0$. Par conséquence, $\ker(f) = \{0\}$ \textit{i.e.} $f$ est injective.
        \end{itemize}
    \end{demo}

    \begin{defi}{Espaces vectoriels isomorphes}{}
        On dit que deux $\mathbb{K}$-ev $E$ et $F$ sont \textbf{isomorphes} s’il existe $f \in \mathcal{L}(E,F)$ qui est un isomorphisme.
    \end{defi}

    \begin{coro}{}{}
        Soient $E$ et $F$ deux $\mathbb{K}$-ev de dimension finie.

        Alors $E$ et $F$ sont isomorphes \textit{ssi} $\dim(E) = \dim(F)$.
    \end{coro}

    \begin{demo}{Preuve}{myorange}
        \begin{itemize}
            \item[\textcolor{myorange}{$\implies$}] Si $E$ et $F$ sont isomorphes, il existe $f \in \mathcal{L}(E,F)$ un isomorphisme. On pose $\mathcal{B} = (e_1,\ldots,e_n)$ une base de $E$, et $\mathcal{F} = (f(e_1),\ldots,f(e_n))$. D’après la proposition précédente, $\mathcal{F}$ est libre et génératrice, donc c’est une base de $F$ d’où $\dim(F) = n = \dim(E)$.
            \item[\textcolor{myorange}{$\impliedby$}] Si $\dim(E) = \dim(F)$, il existe $\mathcal{B} = (e_1,\ldots, e_n)$ une base de $E$ et $\mathcal{B}'=(f_1,\ldots,f_n)$ une base de $F$. De plus, d’après la proposition précédente, il existe $f \in \mathcal{L}(E,F)$ telle que $\forall i \in \intervalleEntier{1}{n}, f(e_i) = f_i$. Comme $\mathcal{B}'$ est libre et génératrice, $f$ est injective et surjective, et donc $f$ est un isomorphisme de $E \to F$.
        \end{itemize}
    \end{demo}

    \begin{theo}{Caractérisation des isomorphismes à l’aide de bases}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        Alors \begin{align*}
            f \text{ est un isomorphisme} & \iff \exists \, \mathcal{B} \text{ base de } E \text{, telle que } f(\mathcal{B}) \text{ est une base de } F \\
            & \iff \forall \, \mathcal{B} \text{ base de } E, \, f(\mathcal{B}) \text{ est une base de } F
        \end{align*}
    \end{theo}
    
    \begin{demo}{Démonstration tournante}{myred}
        \begin{itemize}[leftmargin=3cm]
            \item[\textbf{(i)} $\implies$ \textbf{(iii)}] Soit $\mathcal{B}$ une base de $E$.
            
            $\im(f) = \vect(f(\mathcal{B}))$ car $f$ est linéaire. 
            
            $\im(f) = F$ car $f$ est surjective.
            
            Donc $f(\mathcal{B})$ engendre $F$ et $\et{\mathcal{B} \text{ est libre}}{f \text{ est injective}}$ donc $f(\mathcal{B})$ est libre.
            
            Donc $f(\mathcal{B})$ est une base de $F$.
            \item[\textbf{(iii)} $\implies$ \textbf{(ii)}] Immédiatemment.
            \item[\textbf{(ii)} $\implies$ \textbf{(i)}] Soit $\mathcal{B} = (e_1, \ldots, e_n) $ une base de $E$ tq $f(\mathcal{B})$ est une base de $F$.
            
            $f(E) = f(\vect(\mathcal{B})) = \vect(f(\mathcal{B})) = F$ donc $f$ est surjective.
            
            Soit $x = \sum\limits_{i=1}^n \lambda_i e_i \in \ker(f)$.
            
            Alors $\sum\limits_{i=1}^n \lambda_i f(e_i) = f \left( \sum\limits_{i=1}^n \lambda_i e_i\right) = 0_F$. Or $(f(e_1),\ldots, f(e_n))$ est libre donc $\lambda_1 = \ldots = \lambda_n = 0$ puis $x = 0_E$ donc $f$ est injective.
        \end{itemize}
    \end{demo}

    \subsubsection{Rang d’une AL}

    \begin{defi}{Rang d’une application linéaire}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(\mathcal{E},\mathcal{F})$
        \end{soient}
    
        Alors on dit que l’application est de \textbf{rang} fini lorsque $\im(f)$ est de dimension finie. Dans ce cas, le rang de $f$ est la dimension de $\im(f)$.
    \end{defi}
    
    \begin{prop}{Propriétés d’une application linéaire}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(\mathcal{E},\mathcal{F})$
        \end{soient}
        \begin{alors}
            \item Si $F$ est de dimension finie, 
            \[ \et{f \text{ est de rang fini}}{\rg(f) \leq \dim(F) \text{ (avec égalité si surjective)}} \]
            \item Si $E$ est de dimension finie, 
            \[ \et{f \text{ est de rang fini}}{\rg(f) \leq \dim(E) \text{ (avec égalité si injective)}} \]
        \end{alors}
    \end{prop}
    
    \begin{demo}{Logique}{myolive}
        Pour \textbf{(i)}, $\im(f) \subset F$.
    
        Pour \textbf{(ii)}, si $(e_1,\ldots,e_n)$ est une base de $E$, $\dim(\im(f)) \leq \card(f(e_1),\ldots,f(e_n))$
    \end{demo}
    
    \begin{prop}{Invariance du rang par composition par un isomorphisme}{}
        \begin{soient}
            \item $E,E',F,F'$ quatre $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
            \item $u \in \mathcal{L}(E',E)$ bijective
            \item $v \in \mathcal{L}(F,F')$ bijective
        \end{soient}
        On suppose que $f$ est de rang fini.
    
        Alors \[ \et{\rg(f \circ u) = \rg(f)}{\rg(v \circ f) = \rg(f)} \]
    \end{prop}

    \begin{theo}{Théorème du rang, première version}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        On suppose que $\ker(f)$ admet un supplémentaire $S$ dans $E$.
    
        Alors \[ \restrcorestr{f}{S}{\im(f)} \text{ est un isomorphisme} \]
    \end{theo}
    
    \begin{demo}{Démonstration}{myred}
        On vérifie est $g = \restrcorestr{f}{S}{\im(f)}$ est bien définie et linéaire.
        
        Si $x \in \ker(g)$, $\et{x \in S}{g(x)=f(x)=0_E}$ donc $x \in S \cap \ker(f) = \big\{ 0_E \big\}$ i.e. $g$ est injective.
        
        Si $y \in \im(f), \, \exists \, x \in E, \, y = f(x)$. On décompose $x$ dans $\ker(f) \oplus S$ et donc $\exists x_S \in S, \, y = f(x_S)$ et donc $g$ est surjective.
    \end{demo}
    
    \begin{theo}{Théorème du rang, deuxième version}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        On suppose que $E$ est de dimension finie.
    
        Alors \[ \rg(f) + \dim(\ker(f)) = \dim(E) \]
        Ce qui se réécrit $\dim(\im(f)) + \dim(\ker(f)) = \dim(E)$.
    \end{theo}
    
    \begin{demo}{Preuve}{myred}
        On sait que $\ker(f)$ admet un supplémentaire $S$ dans $E$ car en est un sev.
        
        Donc $\dim(E) = \dim(S) + \dim(\ker(f))$
        
        Or $\restrcorestr{f}{S}{\im(f)}$ est un isomorphisme donc $\dim(S) = \dim(\im(f))$ 
        
        Donc $\dim(E) = \dim(\im(f)) + \dim(\ker(f))$
    \end{demo}
    
    \begin{theo}{Théorème du rang, troisième version}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        Alors \begin{align*}
            f \text{ est bijective} & \iff \et{f \text{ est injective}}{\dim(E) = \dim(F)} \\
            & \iff \et{f \text{ est surjective}}{\dim(E) = \dim(F)}
        \end{align*}
    \end{theo}
    
    \begin{demo}{Démonstration}{myred}
        \begin{itemize}[leftmargin=2.5cm]
            \item[\textbf{(i)} $\implies$ \textbf{(ii)}] Donc $f$ injective, et par le théorème du rang, comme $\dim(\ker(f)) = 0$, on a $\dim(\im(f)) = \dim(E)$. Or $\im(f) = F$ car $f$ bijective, donc $\dim(E) = \dim(F)$
            \item[\textbf{(ii)} $\implies$ \textbf{(iii)}] Par le théorème du rang, on a $\dim(E) = \dim(\im(f)) + \underbrace{\dim(\ker(f))}_{=0}$, donc $\dim(\im(f)) = \dim(F)$. Or $\im(f) \subset F$, donc $\im(f) = F$ et $f$ est surjective.
            \item[\textbf{(iii)} $\implies$ \textbf{(i)}] Par le théorème du rang, $\dim(\ker(f)) = \dim(E) - \dim(\im(f)) = \dim(E) - \dim(F) = 0$ donc $f$ est injective. Donc $f$ est bijective.
        \end{itemize}
    \end{demo}

    \begin{prop}{Rang d’une composeé}{}
        Soient $E,F,G$ 3 $\mathbb{K}$-ev, avec $E$ de dimension finie, et $f,g \in \mathcal{L}(E,F) \times \mathcal{L}(F,G)$. 

        \begin{alors}
            \item $\rg(g \circ f) \leq \min(\rg(g), \rg(f))$ 
            \item $\rg(g \circ f) = \rg(f) - \dim(\im(f) \cap \ker(g))$
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item \begin{align*}
                \rg(g \circ f)
                &= \dim(\im(g \circ f)) \\
                &= \dim((g \circ f)(E)) \\
                &= \dim(g(f(E)))
            \end{align*}
            Or $f(E) \subset F$ donc $g(f(E)) \subset g(F)$ qui sont des sev, d’où $\dim(g(f(E))) \leq \dim(g(F))$, \textit{i.e.}
            \[ \rg(g \circ f) \leq \rg(g) \]
            L’inégalité avec $\rg(f)$ est donnée par la seconde proposition.
            \item On applique la formule du rang à $\restr{g}{\im(f)}$ :
            \[ \rg(\restr{g}{\im(f)}) = \dim(\im(f)) - \dim(\ker(\restr{g}{\im(f)})) \]   
            Or \begin{align*}
                \im(\restr{g}{\im(f)}) 
                &= \restr{g}{\im(f)}\left(\im(f)\right) \\
                &= g(\im(f)) = \im(g \circ f)
            \end{align*}
            et \begin{align*}
                \ker(\restr{g}{\im(f)}) 
                &= \big\{ x \in F, x \in \im(f) \text{ et } g(x) = 0 \big\} \\
                &= \im(f) \cap \ker(g)
            \end{align*}
            D’où 
            \[ \rg(g \circ f) = \dim(\im(g \circ f)) = \rg(f) - \dim(\im(f) \cap \ker(g)) \]
        \end{enumerate}
    \end{demo}

    \subsubsection{Formes linéaires et hyperplans}

    \begin{defitheo}{Espace dual, formes coordonnées et base duale}{}
        L’espace $\mathcal{L}(E,\mathbb{K})$ des formes linéaires est appelé \textbf{espace dual} et est noté $E^*$. 

        Si $E$ est de dimension finie, $\dim(E) = \dim(E^*)$. Si $\mathcal{B}= (e_1,\ldots,e_n)$ est une base de $E$, on appelle \textbf{formes coordonnées} associées à la base $\mathcal{B}$ les formes linéaires 
        \[ e_i^* : x =\sum_{i=1}^{n} x_i e_i \longmapsto x_i \]    
        En particulier, pour tout $j \in \intervalleEntier{1}{n}$, $e_i^*(e_j) = \delta_{i,j}$. 

        La famille $(e_1^*, \ldots, e_n^*)$ des formes coordonnées associées à la base $\mathcal{B}$ est une base de l’espace dual $E^*$.
    \end{defitheo}

    \begin{demo}{Démonstration}{mypurple}
        \begin{itemize}
            \item Soit $\varphi \in \mathcal{L}(E,\mathbb{K})$. Pour tout $x = \sum_{i=1}^{n} x_i e_i$, 
            \[ \varphi(x) = \sum_{i=1}^{n} x_i \varphi(e_i) = \sum_{i=1}^{n} \alpha_i e_i^*(x) \quad \text{en posant } \alpha_i = \varphi(e_i) \in \mathbb{K} \]    
            Ainsi, la famille $(e_1^*, \ldots, e_n^*)$ engendre bien $\mathcal{L}(E,\mathbb{K})$.
            \item Soient $\alpha_1,\ldots,\alpha_n \in \mathbb{K}$ tels que $\sum_{i=1}^{n} \alpha_i e_i^* = 0_{E^*}$. En évaluant l’égalité en $e_j$, on obtient 
            \[ \sum_{i=1}^{n} \alpha_i \delta_{i,j} = \alpha_j = 0_{\mathbb{K}} \]   
        \end{itemize}
    \end{demo}

    \begin{defitheo}{Hyperplan et caractérisation}{}
        On appelle \textbf{hyperplan} de $E$ tout noyau de forme linéaire \textit{non nulle}. 

        $H$ est un hyperplan de $E$ \textit{ssi} il existe $u \in E$ non nul tel que $E = H \oplus \vect(u)$.

        De plus, si $E$ est de dimension finie $n$, $H$ est un hyperplan de $E$ \textit{ssi} $\dim(H) = n-1$.
    \end{defitheo}

    \begin{demo}{Preuve}{mypurple}
        \begin{itemize}
            \item[\textcolor{mypurple}{$\impliedby$}] Tout vecteur $x$ de $E$ s’écrit de façon unique $x = h + \lambda u$. On introduit alors l’unique application linéaire $\varphi$ définie par $\varphi(x) = \lambda$, qui est une forme linéaire non nulle. On a ainsi $x \in H \iff \varphi(x) = 0$ d’où $H = \ker(\varphi)$.
            \item[\textcolor{mypurple}{$\implies$}] Il existe $u \in E$ tel que $\varphi(u) \neq 0$.  Montrons que $E = H \oplus \vect(u)$ par \textit{analyse-synthèse}.
            \begin{itemize}
                \item Supposons que $x = h + \lambda u$ avec $h \in H$ et $\lambda \in \mathbb{K}$. Alors $\varphi(x) = \lambda \varphi(u)$. Ainsi 
                \[ x = \left( x - \frac{\varphi(x)}{\varphi(u)}u \right) + \frac{\varphi(x)}{\varphi(u)}u \]    
                \item Réciproquement, si $x$ est de telle forme, $\varphi\left( x - \frac{\varphi(x)}{\varphi(u)}u \right) = 0$ 
            \end{itemize}
        \end{itemize}
        On obtient la caractérisation en dimension finie par le théorème du rang.
    \end{demo}
    
    \begin{coro}{Équation d’un hyperplan}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev non-nul de dimension finie
            \item $(e_1, \ldots, e_n)$ une base de $E$
            \item $H$ un hyperplan de $E$
        \end{soient}
        Alors
        \[ \exists (a_1,\ldots,a_n) \in \mathbb{K}^n \backslash \left\{ (0, \ldots, 0) \right\} \text{ tel que } \forall (x_1,\ldots,x_n) \in \mathbb{K}^n, \, \left( \sum\limits_{i=1}^n x_i e_i \in H \iff \sum\limits_{i=1}^n a_i x_i = 0 \right) \]
        L’équation $ \sum\limits_{i=1}^n a_i x_i = 0$ est appelée \textbf{équation de l’hyperplan} de la base $(e_1,\ldots,e_n)$.
    \end{coro}
    
    \begin{demo}{Démonstration}{myorange}
        Soit $H = \ker(\varphi)$ un hyperplan de $E$, $x \in E$, $(e_1,\ldots,e_n)$ une base de $E$ et $\alpha_i = \varphi(e_i)$.
        \[ x = \sum_{i=1}^{n} x_i e_i \in H \iff \varphi(x) = 0 \iff \sum_{i=1}^{n} \alpha_i x_i = 0 \]
    \end{demo}

    \begin{prop}{}{}
        Soient $\varphi, \psi$ deux formes linéaires non nulles sur $E$, de même noyau $H$. 

        Il existe $\lambda \in \mathbb{K}^*$ tel que $\varphi = \lambda \psi$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Comme $E = H \oplus \vect(u)$, si $\varphi = \lambda \psi$, alors $\varphi(u) = \lambda \psi(u)$, d’où $\lambda = \frac{\varphi(u)}{\psi(u)}$. Réciproquement, si $\lambda = \frac{\varphi(u)}{\psi(u)}$, $\varphi$ et $\lambda \psi$ coïncident sur $H$ et sur $\vect(u)$.
    \end{demo}

    \begin{prop}{Intersection de $p$ hyperplans}{}
        Soient $E$ un espace de dimension $n$ et $p$ un entier inférieur ou égal à $n$.
        \begin{alors}
            \item L’intersection de $p$ hyperplans de $E$ est un sev de dimension au moins $n-p$.
            \item Tout sous-espace de dimension $n-p$ est l’intersection de $p$ hyperplans de $E$.
        \end{alors}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item Considérons $p$ hyperplans de $E$, notés $H_1,\ldots,H_n$. Pour tout $i \in \intervalleEntier{1}{p}$, notons $\varphi_i$ une forme linéaire non nulle de $E$ telle que $H_i = \ker(\varphi_i)$. On pose 
            \[ \fonction{\varphi}{E}{\mathbb{K}^p}{x}{(\varphi_1(x),\ldots,\varphi_p(x))} \]   
            $\ker(\varphi) = \bigcap_{i=1}^p \ker(\varphi_i) = \bigcap_{i=1}^p H_i$ et comme $\Im(\varphi) \subset \mathbb{K}^p$, d’après le théorème du rang on a 
            \[ \dim\left( \bigcap_{i=1}^p H_i \right) = \dim(E) - \rg(\varphi) \geq n-p \]  
            \item Considérons maintenant un sous-espace $F$ de $E$ de dimension $n-p$. Notons $(e_{p+1}, \ldots, e_n)$ une base de $F$ que l’on complète en une base $(e_1,\ldots, e_n)$ de $E$. On pose alors 
            \[ \forall i \in \intervalleEntier{1}{p}, \quad H_i = \vect_{\substack{1 \leq k \leq n \\ k \neq i}} (e_k) \]
            On a alors que $H_i$ sont des hyperplans, d’intersection $F$.
        \end{enumerate}
    \end{demo}

\subsection{Projecteurs et symétries}

    \begin{theo}{Existence de projecteurs et symétries pour une somme directe}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
            \item $f_1 \in \mathcal{L}(E_1,F)$ et $f_2 \in \mathcal{L}(E_2,F)$
        \end{soient}
        Alors \[ \exists ! f \in \mathcal{L}(E,F), \, \left\{ \begin{array}{l}
        \restr{f}{E_1} = f_1 \\
        \restr{f}{E_2} = f_2
        \end{array} \right. \]
    \end{theo}

    \begin{demo}{Idée}{myred}
        Dans l’analyse, on montre que \[ \fonction{f}{E_1 \oplus E_2}{F}{x_1 + x_2}{f_1(x_1) + f_2(x_2)} \] et dans la synthèse on vérifie que $f$ est bien définie et linéaire.
    \end{demo}

    \subsubsection{Projecteurs}

    \begin{defi}{Projecteur (ou projection)}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
        \end{soient}
        Le \textbf{projecteur} (ou la \textbf{projection}) sur $E_1$ parallèlement à $E_2$ est l’application $p_1$, qui à tout vecteur $x$ de $E$ associe sa composante selon $E_1$, i.e. 
        \[ \fonction{p_1}{E_1 \oplus E_2}{E}{x_1 + x_2}{x_1} \]
        Pour $x \in E$, $p_1(x)$ est le projeté de $x$ sur $E_1$ parallèlement à $E_2$.

        De la même façon, on définit $p_2$ le projecteur sur $E_2$ parallèlement à $E_1$. On dit que $p_1$ et $p_2$ sont des \textbf{projecteurs associés}, et 
        \[ p_1 + p_2 = \id_E \]
    \end{defi}
    
    \begin{prop}{Propriétés du projecteur}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
            \item $p$ le projecteur sur $E_1$ parallèlement à $E_2$
        \end{soient}
        \begin{alors}
            \item $p \in \mathcal{L}(E)$.
            \item $\left.\kern-\nulldelimiterspace p \vphantom{\big|} \right|_{E_1}^{E_1} = \id_{E_1}$ et $\restr{p}{E_2} = 0$.
            \item $\ker(p) = E_2$ et $\im(p) = E_1 = \ker(p - \id_E)$
        \end{alors}
    \end{prop}
    
    \begin{theo}{Caractérisation des projecteurs}{}
        \begin{soient}
            \item $E$ un ev
            \item $p \in \mathcal{F}(E,E)$
        \end{soient}
        Alors \[ p \text{ est un projecteur} \iff \left\{ \begin{array}{l}
            p \circ p = p \\
            p \in \mathcal{L}(E)
        \end{array} \right. \]
        Dans ce cas, $p$ est le projecteur sur $\im(p) = \ker(p-\id_E)$ parallèlement à $\ker(p)$
    \end{theo}
    
    \begin{demo}{Démonstration}{myred}
        \begin{itemize}
            \item[$\implies$] On pose $E_1 = \im(p)$ et $E_2 = \ker(p)$, et on déduit facilement $p \circ p = p$ et $p \in \mathcal{L}(E)$.
            \item[$\impliedby$] Soit $x \in E$.
    
            Alors $x = p(x) + (x-p(x))$.
            \begin{align*}
                p(x) & \in \im(p) \\
                p(x-p(x)) &= p(x)-p(p(x)) \\
                 & = 0
            \end{align*} 
            Donc $E = \im(p) + \ker(p)$. Or $\im(p) \cap \ker(p) = \{ 0_E \}$ donc $E = \im(p) \oplus \ker(p)$ et $p$ est bien le projecteur sur $\im(p)$ parallèlement à $\ker(p)$ car $x = \underbrace{p(x)}_{\in \im(p)} + \underbrace{(x -p(x))}_{\in \ker(p)}$
        \end{itemize}
    \end{demo}
    
    \begin{prop}{Caractérisation du projeté}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
            \item $p$ le projecteur sur $E_1$ parallèlement à $E_2$
            \item $x,y \in E$
        \end{soient}
        Alors \[ y = p(x) \iff \left\{ \begin{array}{l}
            y \in E_1 \\
            x-y \in E_2
        \end{array} \right. \]
    \end{prop}
    
    \begin{demo}{Démonstration}{myolive}
        \begin{itemize}
            \item[$\implies$] Si $y = p(x)$, alors $p(x-y) = p(x)- p(p(x)) = 0_E$.
    
            Donc $\et{y \in E_1 = \im(p)}{x-y \in E_2 = \ker(f)}$
            \item[$\impliedby$] $ x = y + (x-y)$ et $\et{y \in E_1}{x-y \in E_2}$ donc $p(x) = y$.
        \end{itemize}
    \end{demo}
    
    \subsubsection{Symétries}
    
    \begin{defi}{Symétrie}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
        \end{soient}
        La \textbf{symétrie} par rapport à $E_1$ de direction $E_2$ est l’application \[ \fonction{s}{E_1 \oplus E_2}{E}{x_1 + x_2}{x_1 - x_2} \]
    \end{defi}
    
    \begin{prop}{Lien entre symétrie et projecteur}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
            \item $p$ le projecteur sur $E_1$ parallèlement à $E_2$
            \item $s$ la symétrie par rapport à $E_1$ de direction $E_2$
        \end{soient}
        Alors \[ \id_E + s = 2p \]
    \end{prop}
    
    \begin{prop}{Propriétés de la symétrie}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
            \item $s$ la symétrie par rapport à $E_1$ de direction $E_2$
        \end{soient}
        \begin{alors}
            \item $s \in \GL(E)$.
            \item $\left.\kern-\nulldelimiterspace s \vphantom{\big|} \right|_{E_1}^{E_1} = \id_{E_1}$ et $\left.\kern-\nulldelimiterspace s \vphantom{\big|} \right|_{E_2}^{E_2} = - \id_{E_2}$.
            \item $E_1 = \ker(s-\id_E)$ et $E_2 = \ker(s + \id_E)$.
        \end{alors}
    \end{prop}
    
    \begin{theo}{Caractérisation des symétries}{}
        \begin{soient}
            \item $E$ un ev
            \item $s \in \mathcal{F}(E,E)$
        \end{soient}
        Alors \[ s \text{ est une symétrie} \iff \left\{ \begin{array}{l}
            s \circ s = \id_E \\
            s \in \mathcal{L}(E)
        \end{array} \right. \]
        Dans ce cas, $s$ est la symétrie par rapport à $\ker(s-\id_E)$ de direction $\ker(s + \id_E)$.
    \end{theo}
    
    \begin{demo}{Démonstration}{myred}
        Se référer à celle sur les projecteurs.
    \end{demo}
    
    \begin{prop}{Caractérisation du symétrique}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-ev
            \item $E_1$ et $E_2$ deux sev de $E$, tels que $E = E_1 \oplus E_2$
            \item $s$ la symétrie par rapport à $E_1$ de direction $E_2$
            \item $x,y \in E$
        \end{soient}
        Alors \[ y = s(x) \iff \left\{ \begin{array}{l}
            x + y \in E_1 \\
            x - y \in E_2
        \end{array} \right. \]
    \end{prop}
    
    \begin{demo}{Explication}{myolive}
        On pose $E_1 = \ker(s -\id_E)$ et $E_2 = \ker(s + \id_E)$.
        \begin{align*}
            \et{x+y \in E_1}{x-y \in E_2} & \iff \et{s(x+y) = x+y}{s(x-y) = -(x-y)} \\
            & \iff \et{s(x) + s(y) = x +y }{s(x)- s(y) = -x +y} \\
            & \iff y = s(x)
        \end{align*}
    \end{demo}

\subsection{Équations linéaires}

    \begin{defi}{Équation linéaire}{}
        On appelle \textbf{équation linéaire} toute équation du type $ \varphi (u) = b $ où 
        \[ \left\{ \begin{array}{l}
            \varphi \text{ est un application linéaire connue entre deux espaces } E \text{ et } F \\
            b \in F \text{ est connue} \\
            u \in E \text{ est inconnue}
        \end{array} \right. \]
        L’équation est dite homogène si $b= 0$.
    
        Résoudre l’équation linéaire $\varphi (u) = b$, c’est déterminer l’ensemble des solutions.
    \end{defi}
    
    \begin{omed}{Remarque}{myyellow}
        Si l’équation est homogène, $\ker(\varphi)$ est l’ensemble des solutions.
    \end{omed}
    
    \begin{prop}{Structure de l’ensemble des solutions d’une équation linéaire}{}
        \begin{soient}
            \item $E$ et $F$ deux $\mathbb{K}$-ev
            \item $\varphi \in \mathcal{L}(E,F)$
            \item $b \in F$
        \end{soient}
        Alors l’ensemble des solutions de l’équation linéaire $ \varphi (u) = b $ est 
        \begin{enumerate}
            \item $\emptyset$ si $b \notin \im(\varphi)$.
            \item $ u_0 + \ker(\varphi) = \enstq{u_0 + u}{u \in \ker(\varphi)}$ si $b \in \im(\varphi)$ et $\varphi(u_0) = b$.
        \end{enumerate}
    \end{prop}

\section[Matrice d’une a.l.]{Matrice d’une application linéaire dans des bases}

\subsection{Matrice d’un vecteur et d’une application linéaire}

    \begin{defi}{Matrice d’un vecteur, d’une famille de vecteurs et d’une application linéaire}{}
        \begin{soient}
            \item $E$ et $F$ deux espaces vectoriels de dimension finie $n$ et $p$
            \item $\mathcal{B} = (e_1, \ldots, e_n)$ une base de $E$ et $\mathcal{B}' = (f_1,\ldots,f_p)$ une base de $F$.
            \item $x_1,\ldots,x_p$ des vecteurs de $E$
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        \begin{itemize}
            \item La matrice du vecteur $x_1$ dans $E$ est la matrice colonne $\mat{\mathcal{B}}{x_1}$ de $\mk{n,1}$ formée des coordonnées de $x_1$ dans $\mathcal{B}$.
            \item La matrice de la famille de vecteurs $x_1,\ldots,x_p$ de la base $\mathcal{B}$ est la matrice $\mat{\mathcal{B}}{x_1,\ldots,x_p}$ de $\mk{n,p}$ dont la $j$-ème colonne est formée des coordonnées de $x_j$ dans $\mathcal{B}$.
            \item La matrice de $f$ dans les bases $\mathcal{B}$ et $\mathcal{B}'$ est $\mat{\mathcal{B},\mathcal{B}'}{f} = \mat{\mathcal{B}'}{f(e_1),\ldots,f(e_n)}$. C’est la matrice de $\mk{p,n}$ dont la $j$-ème colonne est formée des coordonnées de $f(e_j)$ dans $\mathcal{B}'$.
        \end{itemize}
    \end{defi}

    \begin{theo}{Isomorphismes}{}
        \begin{soient}
            \item $E$ et $F$ deux espaces vectoriels de dimension finie $n$ et $p$
            \item $\mathcal{B} = (e_1, \ldots, e_n)$ une base de $E$ et $\mathcal{B}' = (f_1,\ldots,f_p)$ une base de $F$.
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        \begin{alors}
            \item l’application qui à un vecteur associe sa matrice \[ \fonction{\varphi}{E}{\mk{n,1}}{x}{\mat{\mathcal{B}}{x} = \begin{pmatrix}
                x_1 \\
                \vdots \\
                x_n
            \end{pmatrix}} \] est un isomorphisme.
            \item l’application \[ \fonction{\varphi}{\mathcal{L}(E,F)}{\mk{p,n}}{f}{\mat{\mathcal{B},\mathcal{B}'}{f}} \] est un isomorphisme.
        \end{alors}
    \end{theo}

    \begin{coro}{}{}
        Soient $E$ et $F$ deux espaces vectoriels non-nuls de dimensions finies.

        Alors $\mathcal{L}(E,F)$ est de dimension finie égale à $np = \dim(E)\dim(F)$.
    \end{coro}

    \begin{demo}{Démonstration}{myorange}
         \[ \fonction{\varphi}{\mathcal{L}(E,F)}{\mk{p,n}}{f}{\mat{\mathcal{B},\mathcal{B}'}{f}} \] est un isomorphisme donc $\dim(\mathcal{L}(E,F)) = \dim(\mk{p,n}) = np$.
    \end{demo}

    \begin{prop}{Théorèmes généraux}{}
        \begin{soit}
            \item $E$, $F$ et $G$ des ev de dimensions finies 
            \item $\mathcal{B}$, $\mathcal{B}'$ et $\mathcal{B}''$ trois bases de $E$, $F$ et $G$ respectivement.
            \item $f, f_1, f_2 \in \mathcal{L}(E,F)$ et $g \in \mathcal{L}(F,G)$
            \item $x \in E$
            \item $\lambda \in \mathbb{K}$
        \end{soit}
        \begin{alors}
            \item \textbf{Image d’un vecteur par une application linéaire} \[ \mat{\mathcal{B}'}{f(x)} = \mat{\mathcal{B},\mathcal{B}'}{f} \mat{\mathcal{B}}{x} \]
            \item \textbf{Matrice d’une CL} \[ \mat{\mathcal{B},\mathcal{B}'}{\lambda f_1 + f_2} = \lambda \mat{\mathcal{B},\mathcal{B}'}{f_1} + \mat{\mathcal{B},\mathcal{B}'}{f_2} \]
            \item \textbf{Matrice d’une composée} \[ \mat{\mathcal{B},\mathcal{B}''}{g \circ f} = \mat{\mathcal{B}',\mathcal{B}''}{g} \mat{\mathcal{B},\mathcal{B}'}{f} \]
        \end{alors}
    \end{prop}

    \begin{demo}{Démonstration}{myred}
        \textbf{(iii)} \quad Pour montrer que les deux matrices sont égales, cherchons à les « évaluer » en un vecteur $x \in E$ de coordonnées $x_1,\ldots,x_n$ dans $E$.
        \begin{align*}
            \mat{\mathcal{B},\mathcal{B}''}{g \circ f}\mat{\mathcal{B}}{x} &= \mat{\mathcal{B}''}{(g \circ f)(x)} \\
            \mat{\mathcal{B}',\mathcal{B}''}{g} \mat{\mathcal{B},\mathcal{B}'}{f} \mat{\mathcal{B}}{x} &=  \mat{\mathcal{B}',\mathcal{B}''}{g} \mat{\mathcal{B}'}{f(x)} \\
            &= \mat{\mathcal{B}''}{g(f(x))}
        \end{align*}
        On obtient le même résultat, donc ces matrices sont égales.
    \end{demo}

    \begin{coro}{Caractérisation des isomorphismes par les matrices}{}
        \begin{soient}
            \item $E,F$ deux espaces vectoriels non-nuls de dimensions finies
            \item $\mathcal{B}$ une base de $E$ et $\mathcal{B}'$ une base de $F$
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        On suppose que $\dim(E) = \dim(F)$

        Alors \[ f \text{ est bijective} \iff \mat{\mathcal{B},\mathcal{B}'}{f} \text{ est inversible} \]
        Dans ce cas, $\mat{\mathcal{B},\mathcal{B}'}{f^{-1}} = \left(\mat{\mathcal{B},\mathcal{B}'}{f}\right)^{-1}$
    \end{coro}

\subsection{Produits de matrices par blocs}

    \subsubsection{Sous-espaces stables par un endomorphisme}

    \begin{defi}{Sev stable}{}
        Soit $E$ un $\mathbb{K}$-ev, $f \in \mathcal{L}(E)$ et $F$ un sev de $E$.

        On dit que $F$ est \textbf{stable} par $f$ si  $f(F) \subset F$, \textit{i.e.}
        \[ \forall x \in F, \quad f(x) \in F \]
    \end{defi}

    \begin{defi}{Endomorphisme induit}{}
        Soit $F$ un sev de $E$ stable par $f$.

        On appelle endomorphisme induit par $f$ sur $F$ l’application 
        \[ \fonction{\tilde{f}}{F}{F}{x}{f(x)} \]
        On garde parfois la même notation.
    \end{defi}

    \begin{prop}{}{}
        Soit $F$ un sev $E$ un ev de dimension finie, $f \in \mathcal{L}(E)$. On suppose que $F$ est stable par $f$. 

        Soit $\mathcal{B}_F = (e_1,\ldots, e_p)$ une base $F$, que l’on complète en $\mathcal{B}_E = (e_1,\ldots,e_p , \ldots, e_n)$ une base de $E$.

        Alors 
        \[ \mat{\mathcal{B}_E}{f} = \left( \begin{array}{c | c}
            \mat{\mathcal{B}_F}{\tilde{f}} & B \\
            \hline 
            0_{n,p} & C 
        \end{array} \right) \]
        On dit que c’est une décomposition par blocs de $\mat{\mathcal{B}(E)}{f}$. 
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Par construction de la matrice.
    \end{demo}

    \begin{omed}{Remarque}{myolive}
        Soit $f \in \mathcal{L}(E)$ et $\mathcal{B} = (e_1,\ldots,e_n)$ une base de $E$ telle que 
        \[ \mat{\mathcal{B}_E}{f} = \left( \begin{array}{c | c}
            A & B \\
            \hline 
            0_{n,p} & C 
        \end{array} \right) \]
        Alors, si $\mathcal{F} = \vect(e_1,\ldots,e_p)$, $F$ est stable par $f$.
    \end{omed}

    \subsubsection{Produit de matrices par blocs}

    \begin{prop}{}{}
        Soit $M,N \in \mk{n}$.

        On peut écrire $M = \left( \begin{array}{c | c}
            A & B_{n_1,p_2} \\
            \hline 
            C_{n_2, p_1} & D 
        \end{array} \right)$ 
        et 
        $N = \left( \begin{array}{c | c}
            E & F_{p_1, q_2} \\
            \hline 
            G_{p_2, q_1} & H 
        \end{array} \right)$

        Alors 
        \[ MN = \left( \begin{array}{c | c}
            AE + BG & AF + BH_{n_1,q_2} \\ 
            \hline 
            \underbrace{CE + DG}_{n_2,q_1} & CF+DH
        \end{array} \right) \]
    \end{prop}

    \begin{omed}{Remarque}{myolive}
        Tout se passe comme pour un produit de matrices de $2$.
    \end{omed}
    
    \begin{demo}{Preuve}{myolive}
        Considérons $k$ le coefficient $[MN]_{i,j}$ avec $i \leq n$ et $j \leq q$.
    
        $k$ est le produit de la $i$-ème ligne de $M$ par la $j$-ème colonne de $N$. Or $L_i(M) = L_i(A) \cup L_i(B)$ et $C_j(N) = C_j(E) \cup C_j(G)$. Ainsi,
        \begin{align*}
            L_i C_j 
            &= L_i(A) C_j(E) + L_i(B) C_j(G) \\
            &= [AE]_{i,j} + [BG]_{i,j} \\
            &= [AE + BG]_{i,j}
        \end{align*}
        On procède de même pour les autres blocs.
    \end{demo}

    \begin{omed}{Application}{myolive}
        Posons 
        \[ M = \left( \begin{array}{c | c}
            I_n & 0 \\
            \hline 
            A & I_n
        \end{array} \right) \in \mk{2n} \]
        Prouvons que $M$ est inversible et calculons $M^{-1}$.

        Posons
        \[ N = \left( \begin{array}{c | c}
            X & Y \\
            \hline 
            Z & T 
        \end{array} \right) \in \mk{2n} \]
        On a 
        \begin{align*}
            MN = I_n 
            &\iff \left( \begin{array}{c | c}
                I_n & 0 \\
                \hline 
                A & I_n
            \end{array} \right) \left( \begin{array}{c | c}
                X & Y \\
                \hline 
                Z & T 
            \end{array} \right) = \left( \begin{array}{c | c}
                I_n & 0 \\
                \hline 
                0 & I_n
            \end{array} \right) \\
            & \iff \left( \begin{array}{c | c}
                X & Y \\
                \hline 
                AX + Z & AY + T
            \end{array} \right) = \left( \begin{array}{c | c}
                I_n & 0 \\
                \hline 
                0 & I_n
            \end{array} \right) \\
            &\iff \left\{ 
            \begin{array}{l}
                X = I_n \\
                Y = 0 \\
                A + Z = 0 \\
                T = I_n 
            \end{array}
            \right.
        \end{align*}
        Donc $M$ est inversible et $M^{-1} = \left( \begin{array}{c | c}
            I_n & 0 \\
            \hline 
            -A & I_n
        \end{array} \right)$
    \end{omed}

    \begin{exo}{}{}
        Soit 
        \[ M = \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            3 & -1 & 0 & 0 \\
            1 & 0 & -1 & 0 \\
            0 & 1 & -3 & 1 
        \end{pmatrix} \]    
        Calculer efficacement $M^2$.
    \end{exo}

    \begin{demo}{Solution}{nfpgreen}
        On se ramène à une matrice par blocs 
        \[ M = \left( \begin{array}{cc | cc}
            1 & 0 & 0 & 0 \\
            3 & -1 & 0 & 0 \\
            \hline
            1 & 0 & -1 & 0 \\
            0 & 1 & -3 & 1 
        \end{array} \right) =  \left( \begin{array}{c | c}
            A & 0_2 \\
            \hline 
            I_2 & -A 
        \end{array} \right) \]
        Donc 
        \[ M^2 = \left( \begin{array}{c | c}
            A^2 & 0_2 \\
            \hline 
            0_2 & A^2 
        \end{array} \right) = I_4 \]
    \end{demo}

    \subsubsection{Matrices diagonales par blocs}

    \begin{prop}{}{}
        Soit $E$ un $\mathbb{K}$-ev de dimension finie, $F,G$ deux sevs de $E$ tels que $E = F \oplus G$, et $f \in \mathcal{L}(E)$ tel que $F$ et $G$ sont stables par $f$. 

        On note $\mathcal{B}_F$ une base de $F$ et $\mathcal{B}_G$ une base de $G$, et $\mathcal{B} = \mathcal{B}_F \cup \mathcal{B}_G$.

        Alors 
        \[ \mat{\mathcal{B}}{f} = 
        \left( \begin{array}{c | c}
            \mat{\mathcal{B}_F}{f} & 0 \\
            \hline 
            0 & \mat{\mathcal{B}_G}{f}
        \end{array} \right) \]
        On dit que $\mat{\mathcal{B}}{f}$ est diagonale par blocs. On peut la noter $\diag(A,B)$.
    \end{prop}

    \begin{prop}{Généralisation}{}
        \begin{soit}
            \item $E = \bigoplus_{i=1}^n E_i$
            \item $f \in \mathcal{L}(E)$
            \item $\mathcal{B}_i$ une base de $E_i$ pour $i \in \intervalleEntier{1}{n}$
            \item $\mathcal{B} = \bigcup_{i=1}^n \mathcal{B}_i$
        \end{soit}
        \begin{suppose}
            \item pour tout $i \in \intervalleEntier{1}{n}$, $E_i$ est stable par $f$
        \end{suppose}
        Alors 
        \[ \mat{\mathcal{B}}{f} = \left( \begin{array}{c | c | c}
            \mat{\mathcal{B}_1}{f} & 0 & 0 \\
            \hline 
            0 & \ddots & 0 \\
            \hline
            0 & 0 & \mat{\mathcal{B}_n}{f}
        \end{array} \right) = \diag(\mat{\mathcal{B}_1}{f}, \ldots, \mat{\mathcal{B}_n}{f}) \]
    \end{prop}

    \begin{prop}{}{}
        Le produit de deux matrices diagonales par bloc est diagonale par blocs (avec des blocs de même taille)
    \end{prop}

    \begin{prop}{}{}
        Soient $A,B$ des matrices diagonales par blocs. \begin{suppose}
            \item $A = \left( \begin{array}{c | c | c}
                A_1 & 0 & 0 \\
                \hline 
                0 & \ddots & 0 \\
                \hline
                0 & 0 & A_p
            \end{array} \right)$ avec, pour tout $i \in \intervalleEntier{1}{p}$, $A_i \in \mathcal{M}_{n_i}(\mathbb{K})$.
            \item $B = \left( \begin{array}{c | c | c}
                B_1 & 0 & 0 \\
                \hline 
                0 & \ddots & 0 \\
                \hline
                0 & 0 & B_p
            \end{array} \right)$ avec, pour tout $i \in \intervalleEntier{1}{p}$, $B_i \in \mathcal{M}_{n_i}(\mathbb{K})$.
        \end{suppose}
        Alors \[ AB = \left( \begin{array}{c | c | c}
                A_1 B_1 & 0 & 0 \\
                \hline
                0 & \ddots & 0 \\
                \hline
                0 & 0 & A_p B_p
            \end{array} \right) \]   
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Par récurrence sur $p$, \textit{i.e.} le nombre de blocs.
        \begin{itemize}
            \item \textbf{si $p = 2$} \quad On a 
            \[ \left( \begin{array}{c | c}
                A_1 & 0 \\
                \hline 
                0 & A_2
            \end{array} \right)\left( \begin{array}{c | c}
                B_1 & 0 \\
                \hline 
                0 & B_2
            \end{array} \right)  = \left( \begin{array}{c | c}
                A_1 B_1 & 0 \\
                \hline 
                0 & A_2 B_2
            \end{array} \right) \]   
            d’où l’initialisation.
            \item \textbf{Supposons l’ordre $k$} \quad Posons 
            \[ A =  \left( \begin{array}{c c c | c}
                A_1 & 0 \ldots & \ldots & 0 \\
                0 & \ddots & \ddots & \vdots \\
                \vdots & \ddots & A_p & \vdots \\
                \hline 
                0 & \ldots & \ldots  & A_{p+1}
            \end{array} \right) \esp{et} B = \left( \begin{array}{c c c | c}
                B_1 & 0 \ldots & \ldots & 0 \\
                0 & \ddots & \ddots & \vdots \\
                \vdots & \ddots & B_p & \vdots \\
                \hline 
                0 & \ldots & \ldots  & B_{p+1}
            \end{array} \right) \]    
            On obtient par produit, en utilisant l’hérédité, le résultat.
        \end{itemize}
    \end{demo}

    \begin{coro}{}{}
        Une matrice $A = \left( \begin{array}{c | c | c}
            A_1 & 0 & 0 \\
            \hline 
            0 & \ddots & 0 \\
            \hline 
            0 & 0 & A_p 
        \end{array} \right)$ diagonale par blocs est inversible si $\forall i \in \intervalleEntier{1}{p}$, $A_i$ est inversible.

        Dans ce cas, $A^{-1} = \left( \begin{array}{c | c | c}
            A_1^{-1} & 0 & 0 \\
            \hline 
            0 & \ddots & 0 \\
            \hline 
            0 & 0 & A_p^{-1} 
        \end{array} \right)$
    \end{coro}

    \begin{lem}{}{}
        Soit, pour tout $i \in \intervalleEntier{1}{p}$, $A_i \in \mathcal{M}_{n_i}(\mathbb{K})$ et $n = \sum_{i=1}^p n_i$. On a alors $A \in \mathcal{M}_n(\mathbb{K})$. 
        
        Considérons $\mathcal{S} \subset \mk{n}$ tel que 
        \begin{enumerate}[label=\textcolor{mybrown}{($h_{\alph*}$)}]
            \item $\mathcal{S}$ est un sev de $\mk{n}$
            \item $I_n \in \mathcal{S}$
            \item $\forall M, M' \in S, MM' \in S$
        \end{enumerate}
        On dit que $\mathcal{S}$ est une sous-algèbre de $\mk{n}$.

        Si $A \in \mathcal{S}$ et $A$ est inversible, alors $A^{-1} \in \mathcal{S}$.
    \end{lem}

    \begin{demo}{Démonstration du lemme}{mybrown}
        Considérons $\fonction{\varphi}{\mathcal{S}}{\mathcal{S}}{M}{AM}$, pour la matrice $A \in \mathcal{S}$.
        \begin{itemize}
            \item La fonction $\varphi$ est bien définie car $\mathcal{S}$ est stable par produit.
            \item $\varphi$ est linéaire par linéarité du produit de matrices.
            \item $\mathcal{S}$ est de dimension finie comme sev de $\mk{n}$, de dimension finie.
        \end{itemize}
        Ainsi,
        \[ \varphi \text{ surjective} \iff \varphi \text{ injective} \iff \varphi \text{ bijective} \]
        Soit $M \in \ker(\varphi)$ \textit{i.e.} $AM = 0$. Comme $A$ est inversible, on a $A^{-1} A M = 0$ d’où $M = 0$. Donc $\varphi$ est injective, donc surjective, et comme $I_n \in \mathcal{S}$, il existe $A' \in \mathcal{S}$, $\varphi(A') = I_n$, et donc $A A, = I_n$. En multipliant par $A^{-1}$, on obtient $A' = A^{-1}$, donc $A^{-1} \in \mathcal{S}$.
    \end{demo}

    \begin{demo}{Preuve du corollaire}{myorange}
        \begin{itemize}
            \item[\textcolor{myorange}{$\implies$}] Si pour tout $i \in \intervalleEntier{1}{p}, A_i$ est inversible, en posant $A' = \left( \begin{array}{c | c | c}
                A_1^{-1} & 0 & 0 \\
                \hline 
                0 & \ddots & 0 \\
                \hline 
                0 & 0 & A_p^{-1} 
            \end{array} \right)$, on obtient $AA' = I_n$ et donc $A$ est inversible et $A' = A^{-1}$
            \item[\textcolor{myorange}{$\impliedby$}] Supposons $A$ inversible. L’ensemble $\mathcal{S}$ des matrices diagonales par blocs est une sous-algèbre. D’après le lemme, $A^{-1}$ est diagonale par blocs, et on pose $A^{-1} = \left( \begin{array}{c | c | c}
                B_1 & 0 & 0 \\
                \hline 
                0 & \ddots & 0 \\
                \hline 
                0 & 0 & B_p 
            \end{array} \right)$ d’où $AA^{-1} = I_n = \left( \begin{array}{c | c | c}
                A_1 B_1 & 0 & 0 \\
                \hline 
                0 & \ddots & 0 \\
                \hline 
                0 & 0 & A_p B_p 
            \end{array} \right)$ \textit{i.e.} $A_1 B_1 = I_{n_1}, \ldots, A_p B_p = I_{n_p}$ d’où $A_1, \ldots, A_p$ sont des matrices inversibles d’inverse $B_1, \ldots, B_p$.
        \end{itemize}
    \end{demo}

    \begin{omed}{Application}{myolive}
        Soit $A = \begin{pmatrix}
            1 & 2 & 0 & 0 & 0 \\
            3 & 5 & 0 & 0 & 0 \\
            0 & 0 & 2 & 0 & 0 \\
            0 & 0 & 0 & 7 & 8 \\
            0 & 0 & 0 & 5 & 6
        \end{pmatrix}$. Montrons que $A$ est inversible, donnons $A^{-1}$ et $A^{10}$.
    \end{omed}
    
    \begin{demo}{Résolution}{myolive}
        Posons $A_1 = \begin{pmatrix}
            1 & 2 \\
            3 & 5
        \end{pmatrix}$, $A_2 = (2)$ et $A_3 = \begin{pmatrix}
            7 & 8 \\
            5 & 6
        \end{pmatrix}$. Pour tout $A_i$, $\det(A_i) \neq 0$ donc la matrice $A$ est invesible, et $A^{-1} = \left( \begin{array}{c | c | c}
            A_1^{-1} & 0 & 0 \\
            0 & A_2^{-1} & 0 \\
            0 & 0 & A_3^{-1}
        \end{array} \right)$. Or, si $A \in \mk{2}$, alors $A^2 - \tr(A) A + \det(A) I_2 = 0 \eqlabel{*}$, d’où $A(A - \tr(A)I_2) = -\det(A) I_2$ et si $A$ est inversible, 
        \[ A^{-1} = -\frac{1}{\det(A)}(A - \tr(A) I_2) \]   
        On trouve ainsi $A_1^{-1} = \begin{pmatrix}
            -5 & 2 \\
            3 & -1
        \end{pmatrix}$ et $A_3^{-1} = \begin{pmatrix}
            3 & - 4 \\
            -5/2 & -7/2
        \end{pmatrix}$ d’où la formule de $A^{-1}$. 
    
        Pour calculer, $A^10$, on se ramène au calcul de $A_1^{10}, A_2^{10}$ et $A_3^{10}$. 
        \begin{itemize}
            \item On a aisément $A_2^{10} = (2^10) = (1024)$.
            \item On sait que $A_1^2 - 6 A_1 - I_2 = 0$ d’après $(*)$. Par division euclidienne, 
            \[ X^10 = (X^2 - 6X - 1) Q(X) + cX + d \]   
            Les racines de $X^2 - 6X - 1$ sont $x = 3 \pm \sqrt{10}$, d’où $\et{(3 + \sqrt{10})^10 = c(3 + \sqrt{10}) + d}{(3 - \sqrt{10})^10 = c(3 - \sqrt{10}) + d}$, système qui donne $c$ et $d$. On en déduit que $A_1^{10} = cA_1 + d I_2$.
            \item On procède de la même façon pour trouver $A_3$, en utilisant $A_3^2 - 13 A_3 + 2 I_2 = 0$ d’après $(*)$.
        \end{itemize}
    \end{demo}

    \begin{omed}{Application}{myolive}
        Soient $A, B \in \mk{n}$. Posons $M_{A,B} = \begin{pmatrix}
            A & B & \ldots & B \\
            B & \ddots & \ddots & \vdots \\
            \vdots & \ddots & \ddots & B \\
            B & \ldots & B & A
        \end{pmatrix} \in \mk{kn}$. Donnons une CNS sur $A$ et $B$ pour que $M$ soit inversible.
    \end{omed}

    \begin{demo}{Démonstration}{myolive}
        Posons $\mathcal{S} = \enstq{M_{A,B} \in \mk{kn}}{A,B \in \mk{n}}$. 
        \begin{itemize}
            \item $M_{A,B} + \lambda M_{A',B'} = M_{A + \lambda A', B + \lambda B'}$ donc $\mathcal{S}$ est un sev de $\mk{kn}$.
            \item $I_{kn} = M_{I_n, 0} \in \mathcal{S}$.
            \item $M_{A,B} M_{A',B'} = M_{AA' + (k-1) BB', AB' + A'B + (k-2)BB'} \in \mathcal{S}$.
        \end{itemize}
        Donc $M_{A,B}$ est inversible, et il existe $A',B' \in \mk{n}$ telles que $M_{A,B}^{-1} = M_{A',B'}$.
        \begin{align*}
            M_{A,B}^{-1} = M_{A', B'}
            &\iff M_{A,B} M_{A',B'} = M_{I_n,0} \\
            &\iff \et{AA' + (k-1)BB' = I_n \eqlabel{1}}{AB' + BA' + (k-2)BB' = 0 \eqlabel{2}}
        \end{align*}
        On effectuant $(1) - (2)$, on obtient $AA' - AB' - BA' + BB' = I_n$ \textit{i.e.} $(A - B)(A' - B') = I_n$. On a donc comme \textcolor{myolive}{condition nécessaire} que \lilbox{myolive}{$A-B$ est inversible}. Et on a donc $A' - B' = (A-B)^{-1}$ \textit{i.e.} $A' = B' + (A-B)^{-1}$. En remplaçant dans $(1)$, cela donne \[ (A + (k-1)B)B' = I_n - A(A-B)^{-1} = (A-B-A)(A-B)^{-1} = -B(A-B)^1 \quad (3) \] 
        Une \textcolor{myolive}{condition suffisante} est donc \lilbox{myolive}{$(A + (k-1)B)$ est inversible}, car on aurait ainsi l’existence de $B'$ en multipliant par cet inverse dans $(3)$, puis on obtiendrait $A'$. 

        Il faut ensuite prouver que c’est une CNS, ce qu’on ne fera pas ici.
    \end{demo}

    \subsubsection{Stabilité du noyau et de l’image}

    \begin{prop}{Stabilité du noyau et de l’image d’endomorphismes commutant}{}
        \begin{soit}
            \item $E$ un $\mathbb{K}$-ev 
            \item $f,g \in \mathcal{L}(E)$
        \end{soit}
        \begin{suppose}
            \item $f \circ g = g \circ f$
        \end{suppose}
        Alors le noyau et l’image de $g$ sont stables par $f$, et inversement.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item \textbf{Montrons que $f(\ker(g)) \subset \ker(g)$} \quad Soit $x \in \ker(g)$. On a $g \circ f(x) = f \circ g(x) = 0$ d’où $f(x) \in \ker(g)$. 
            \item \textbf{Montrons que $f(\im(g)) \subset \im(g)$} \quad Soit $y \in \im(g)$. Il existe $x \in E$ tel que $g(x) = y$ d’où $f(y) = f \circ g(x) = g \circ f(x)$ donc $f(x) \in \im(g)$.
        \end{itemize}
    \end{demo}

    \begin{omed}{Exemple}{myolive}
        Considérons $f \in \mathcal{L}(E)$ tel que $f^2 + 2 f - 3 \id_E = 0$. Montrons que $E = \ker(f - \id_E) \oplus \ker(f + 3 \id_E)$.
        \begin{itemize}
            \item Soit $x \in \ker(f - \id_E) \cap \ker(f + 3 \id_E)$. Alors $f(x) - x = 0$ et $f(x) + 3x$. Donc $x = 0$.
            \item On remarque que $f^2 + 2f - 3 \id_E = (f - \id_E) \circ (f + 3 \id_E) = (f + 3 \id_E) \circ (f - \id_E) = 0$. On en déduit que $\im(f + 3 \id_E) \subset \ker(f - \id_E)$ et $\im(f - \id_E) \subset \ker(f + 3 \id_E)$.
            
            Soit $x \in E$. D’après les inclusions précédentes, il suffit de de décomposer $x$ en un élément de $\im(f + 3 \id_E)$ et un élément de $\im(f - \id(E))$. On remarque ainsi que 
            \[ x = \frac{1}{4} \left( \left(f(x) + 3x\right) - \left(f(x) - x\right) \right) \]
        \end{itemize}
    \end{omed}

    \begin{omed}{Généralisation \textcolor{black}{(Lemme des noyaux)}}{myolive}
        Soit $f \in \mathcal{L}(E)$. On suppose qu’il existe $P \in \mathbb{K}[X]$ tel que $P(f) = 0$ avec $P = P_1 P_2$ où $P_1 \wedge P_2 = 1$. Alors 
        \[ E = \ker(P_1(f)) \oplus \ker(P_2(f)) \]  
    \end{omed}

\subsection{Changement de base}

    \begin{defi}{Matrice de passage}{}
        \begin{soient}
            \item $E$ un espace vectoriel non-nul de dimension finie $n$
            \item $\mathcal{B} = (e_1, \ldots, e_n)$ et $\ovl{\mathcal{B}} = (\ovl{e_1}, \ldots, \ovl{e_n})$ deux bases de $E$.
        \end{soient}
        La \textbf{matrice de passage} $\pass{\mathcal{B}}{\ovl{\mathcal{B}}}$ de $\mathcal{B}$ à $\ovl{\mathcal{B}}$ est la matrice de $\mk{n}$ dont la $j$-ème colonne est formée des coordonnées de $\ovl{e_j}$ dans la base $\mathcal{B}$. 
        \[ \pass{\mathcal{B}}{\ovl{\mathcal{B}}} = \mat{\mathcal{B}}{\ovl{e_1},\ldots,\ovl{e_n}} = \mat{\ovl{\mathcal{B}},\mathcal{B}}{\id_E} \]
    \end{defi}

    \begin{prop}{Inversibilité d’une matrice de passage}{}
        \begin{soient}
            \item $E$ un espace vectoriel non-nul de dimension finie $n$
            \item $\mathcal{B} = (e_1,\ldots,e_n)$ et $\ovl{\mathcal{B}} = \left(\ovl{e_1}, \ldots, \ovl{e_n}\right)$ deux bases de $E$
        \end{soient}
        Alors $\pass{\mathcal{B}}{\bar{\mathcal{B}}}$ est inversible, et $\left(\pass{\mathcal{B}}{\ovl{\mathcal{B}}}\right)^{-1} = \pass{\ovl{\mathcal{B}}}{\mathcal{B}}$. De plus, cette matrice est unique.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        La fonction identité est un isomorphisme, donc sa matrice est inversible, et on remarque que $\id^{-1} = \id$. La définition de matrice de passage est donc conservée par cette fonction réciproque. Pour l’unicité, remarque que $P = \mat{\mathcal{B}}{f}$ pour $f \in \mathcal{L}(E)$ tel que $\forall i \in \intervalleEntier{1}{n},$, $f(e_i) = f_i$, qui est unique.
    \end{demo}

    \begin{theo}{Formules de changement de base}{}
        \begin{soient}
            \item $E$ et $F$ deux espaces vectoriels de dimensions finies
            \item $\mathcal{B}$ et $\ovl{\mathcal{B}}$ deux bases de $E$
            \item $\mathcal{B}'$ et $\ovl{\mathcal{B}'}$ deux bases de $F$
            \item $f \in \mathcal{L}(E,F)$
            \item $g \in \mathcal{L}(E)$
            \item $x \in E$
        \end{soient}
        \begin{alors}
            \item \textbf{Pour un vecteur}
            \[ \mat{\ovl{\mathcal{B}}}{x} = \pass{\ovl{\mathcal{B}}}{\mathcal{B}}\mat{\mathcal{B}}{x} \]
            \item \textbf{Pour une application linéaire} 
            \[ \mat{\ovl{\mathcal{B}}, \bar{\mathcal{B}'}}{f} = \left(\pass{\mathcal{B}'}{\ovl{\mathcal{B}'}}\right)^{-1} \mat{\mathcal{B},\mathcal{B}'}{f} \pass{\mathcal{B}}{\ovl{\mathcal{B}}} \]
            Cela se retient avec un schéma adéquat, que l’on remonte « à l’envers »
            \[ \overset{\overset{f}{\xrightarrow{\hspace*{8cm}}}}{\underset{\ovl{\mathcal{B}}}{E} \overset{\id_E}{\xrightarrow{\hspace*{2cm}}} \underset{\mathcal{B}}{E} \overset{f}{\xrightarrow{\hspace*{2cm}}} \underset{\mathcal{B}'}{F} \overset{\id_F}{\xrightarrow{\hspace*{2cm}}} \underset{\ovl{\mathcal{B}'}}{F}} \]  
            \item \textbf{Pour un endomorphisme} 
            \[ \mat{\ovl{\mathcal{B}}}{f} = P^{-1} \mat{\mathcal{B}}{f} P \]
        \end{alors}
    \end{theo}

    \begin{omed}{Application \textcolor{black}{(Matrice d’une projection)}}{myred}
        Dans $E = \mathbb{R}^3$, on cherche la matrice dans la bc de la projection sur le plan $P$ d’équation $z = x-y$ parallèlement à la droite $D$ d’équation $x = -y = z$.

        $(u = (1,0,1), v = (0,1,-1))$ est une base de $P$, et $(w = (1,-1,1))$ une base de $D$, donc $(u,v,w)$ est une base de $E$, dans laquelle $p$ admet pour matrice 
        \[ A = \left(\begin{array}{ccc}
            1&0&0\\
            0&1&0\\
            0&0&0
            \end{array}\right) \]
        Il suffit donc de déterminer la matrice de passage $Q$ de la base canonique à la base $(u,v,w)$, pour obtenir que $Q A Q^{-1}$ est la matrice de $p$ dans $E$. Après calculs, on obtient 
        \[ QAQ^{-1}=\left(
            \begin{array}{ccc}
            0&1&1\\
            1&0&-1\\
            -1&1&2
            \end{array}
            \right) \]   
    \end{omed}

\subsection{Image, noyau et rang d’une matrice}

    \begin{defi}{Image, noyau et rang d’une matrice}{}
        Soit $M \in \mk{n,p}$.
        \begin{itemize}
            \item Le \textbf{noyau} de la matrice $M$ est l’ensemble $\ker(M)=\left\{ X \in \mk{p,1}, \, MX=0_{n,1} \right\}$
            \item On note $C_1, \ldots, C_p$ les colonnes de $M$.

            L’\textbf{image} de la matrice $M$ est $\vect(C_1, \ldots, C_p)$.
            \item Le \textbf{rang} de la matrice $M$ est $\rg(M) = \rg(f_M)=\dim(\im(M)) = \dim(\vect(C_1,\ldots,C_p)) = \rg(C_1, \ldots, C_p)$. 
        \end{itemize}
    \end{defi}

    \begin{omed}{Remarque}{myyellow}
        Les opérations élémentaires sur les colonnes ne modifient pas l’image de la matrice.
        Les opérations élémentaires sur les lignes ne modifient pas le noyau de la matrice.
    \end{omed}

    \begin{prop}{Lien avec les AL}{}
        \begin{soient}
            \item $E$ et $F$ deux espaces vectoriels de dimensions finies $p$ et $n$
            \item $\mathcal{B} = (e_1, \ldots, e_p)$ et $\mathcal{B}' = (e_1', \ldots, e_n')$ des bases de $E$ et $F$ respectivement
            \item $f \in \mathcal{L}(E,F)$
        \end{soient}
        \begin{alors}
            \item $x \in \ker(f) \iff \mat{\mathcal{B}}{x} \in \ker(\mat{\mathcal{B},\mathcal{B}'}{f})$
            \item $y \in \im(f) \iff \mat{\mathcal{B}'}{y} \in \im(\mat{\mathcal{B},\mathcal{B}'}{f})$
        \end{alors}
        En particulier, $\dim(\ker(f)) = \dim(\ker(\mat{\mathcal{B},\mathcal{B}'}{f}))$ et $\rg(f) = \rg(\mat{\mathcal{B},\mathcal{B}'}{f})$
    \end{prop}

    \begin{prop}{Rang d’une famille de vecteur et d’une application linéaire}{}
        \begin{soit}
            \item $E$ et $F$ deux espaces vectoriels de dimensions finies
            \item $\mathcal{B}$ et $\mathcal{B}'$ des bases de $E$ et $F$ respectivement
            \item $x_1,\ldots,x_p$ des vecteurs de $E$
            \item $f \in \mathcal{L}(E,F)$
        \end{soit}
        \begin{alors}
            \item $\rg(x_1,\ldots,x_p) = \rg(\mat{\mathcal{B}}{x_1,\ldots,x_p})$
            \item $\rg(f) = \rg(\mat{\mathcal{B},\mathcal{B}'}{f})$
        \end{alors}
    \end{prop}

    \begin{omed}{Remarque}{myolive}
        Tous les calculs de rangs (application linéaire, famille de vecteurs) se ramènent au calcul du rang d’une matrice.
    \end{omed}

    \begin{prop}{}{}
        \begin{soit}
            \item $M \in \mk{n,p}$
            \item $P$ et $Q$ des matrices de $\mk{n}$
        \end{soit}

        \begin{alors}
            \item Si $N \in \GL_p(\mathbb{K})$, alors $\rg(MN) = \rg(M)$.
            \item Si $N \in \GL_n(\mathbb{K})$, alors $\rg(NM) = \rg(M)$.
            \item Si $P$ et $Q$ sont semblables, $\rg(P) = rg(Q)$.
            \item $\rg(M) = \rg(M^{\top})$
            \item \textbf{Théorème du rang} \quad $p = \rg(M) + \dim(\ker(M))$
            \item \[ M \text{ est inversible} \iff \ker(M) = \left\{ \begin{pmatrix}
            0 \\
            \vdots \\
            0
        \end{pmatrix} \right\} \iff \rg(M) = n \]
        \end{alors}
    \end{prop}

    \begin{coro}{}{}
        On ne modifie pas le rang d’une matrice en faisant des opérations élémentaires sur les lignes ou les colonnes.
    \end{coro}

    \begin{omed}{Méthode}{myorange}
        On peut appliquer un pivot de Gauss pour calculer le rang d’une matrice.
    \end{omed}

    \begin{theo}{}{}
        Soit $M \in \mk{n,p}$ et $r = \rg(M)$.

        Alors
        \[ \exists (P,Q) \in \left(\GL_n(\mathbb{K}) \times \GL_p(\mathbb{K})\right), \quad  M = P \begin{pmatrix}
                I_r & 0 \\
                0 & 0
            \end{pmatrix} Q \]
    \end{theo}

\subsection{Systèmes linéaires}

    \begin{defi}{Système linéaire sous forme matricielle}{}
        Soit $(\mathcal{S}) : \left\{ \begin{array}{l}
            a_{1,1}x_1 + \ldots + a_{1,p}x_p = b_1 \\
            \vdots \qquad \vdots \qquad \vdots \\
            a_{n,1}x_1 + \ldots + a_{n,p}x_p = b_n
        \end{array} \right.$ un \textbf{système linéaire}, d’inconnue $(x_1,\ldots,x_p) \in \mathbb{K}^p$
        On pose $A = \left((a_{i,j})\right)_{\substack{1 \leq i \leq n \\ 1 \leq j \leq p}}$, $X = \begin{pmatrix}
            x_1 \\
            \vdots \\
            x_p 
        \end{pmatrix}$ et $B = \begin{pmatrix}
            b_1 \\
            \vdots \\
            b_n
        \end{pmatrix}$
        \begin{itemize}
            \item La forme matricielle du système $(\mathcal{S})$ est $AX = B$.
            \item Le rang du système $(\mathcal{S})$ est $\rg(A)$.
        \end{itemize}
    \end{defi}

    \begin{prop}{}{}
        Soit $(\mathcal{S})$ un système linéaire de forme matricielle $AX = B$.
        \begin{enumerate}
            \item Le système homogène $AX = 0_{n,1}$ a pour ensemble de solution $\ker(A)$.
            \item Le système $AX = B$ est compatible si et seulement si $B \in \im(A)$.
            \item Si $A$ est carrée et inversible, le système $AX = B$ admet une unique solution. On dit dans ce cas que le système est un système de Cramer.
        \end{enumerate}
    \end{prop}  

\subsection{Matrices semblables}

    \begin{defitheo}{Matrices semblables}{}
        Soient $A,B \in \mk{n}$. On dit que $A$ et $B$ sont semblables s’il existe $P \in \GL_n(\mathbb{K})$ telle que 
        \[ A = P^{-1} B P \]   
        C’est une relation d’équivalence sur $\mk{n}$.
    \end{defitheo}

    \begin{demo}{Preuve}{mypurple}
        \begin{itemize}
            \item \textbf{Réflexivité} \quad Si $A \in mk{n}$, en prenant $P = I_n$, on obtient que $A$ est semblable à elle-même.
            \item \textbf{Symétrie} \quad Si $A$ et $B$ sont semblables, en prenant comme matrice inversible $P^{-1}$, on a $B = P A P^{-1}$.
            \item \textbf{Transitivité} \quad Si $A$ est semblable à $B$ par la matrice $P$, semblable à $C$ par la matrice $Q$, alors $C = (PQ)^{-1} A (PQ)$ d’où $A$ et $C$ sont semblables.
        \end{itemize}
    \end{demo}

    \begin{prop}{}{}
        Deux matrices $A$ et $B$ sont semblables \textit{ssi} ce sont les matrices d’un même endomorphisme.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item[\textcolor{myolive}{$\impliedby$}] Si $A = \mat{\mathcal{B}}{f}$ et $B = \mat{\mathcal{B}'}{f}$, alors $B = P^{-1} A P$ avec $P$ la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$.
            \item[\textcolor{myolive}{$\implies$}] Soient $A,B \in \mk{n}$ et $P \in \GL_n(\mathbb{K})$ telles que $B = P^{-1} A P$.
            
            Soit $E$ un $\mathbb{K}$-ev de dimension $n$ et $\mathcal{B}$ une base de $E$. Il existe $f \in \mathcal{L}(E)$ tel que $A = \mat{\mathcal{B}}{f}$ et $\mathcal{B}'$ une base de $E$ telle que $P$ est la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$. Dans ce cas, $\mat{\mathcal{B}'}{f} = P^{-1} A P = B$, d’où le résultat.
        \end{itemize}
    \end{demo}

\subsection{Trace}

    \subsubsection{Trace d’une matrice carrée}

    \begin{defitheo}{}{}
        Soit $M = (a_{i,j}) \in \mk{n}$. On appelle \textbf{trace} de $\mk{n}$ le scalaire \[ \sum_{i=1}^{n} a_{i,i} =: \tr(M) \]  
        \begin{enumerate}
            \item L’application $\fonction{\tr}{\mk{n}}{\mathbb{K}}{M}{\tr(M)}$ est linéaire ;
            \item Soient $A \in \mk{n,p}$ et $B \in \mk{p,n}$. Alors $\tr(AB) = \tr(BA)$.
            \item Deux matrices semblables ont toujours la même trace.
        \end{enumerate} 
    \end{defitheo}

    \begin{demo}{Preuve}{mypurple}
        \begin{enumerate}
            \item Par linéarité du produit.
            \item On a 
            \begin{align*}
                \tr(AB) 
                &= \sum_{i=1}^{n} \sum_{j=1}^{p} a_{i,j} b_{j,i} \\
                &= \sum_{j=1}^{p} \sum_{i=1}^{n} b_{j,i} a_{i,j} \\
                &= \tr(BA)
            \end{align*}
            \item Soient $A,B \in \mk{n}$ et $P \in \GL_n(\mathbb{K})$ telle que $A = P^{-1} B P$.
            \[ \tr(A) = \tr(P^{-1} B P) = \tr(B P P^{-1}) = \tr(B) \]
        \end{enumerate}
    \end{demo}

    \begin{omed}{Remarque}{mypurple}
        \begin{enumerate}[label=\textcolor{mypurple}{\arabic*.}]
            \item On peut trouver trois matrices $A,B,C$ telles que $\tr(ABC) \neq \tr(ACB)$, \textit{i.e.} on ne peut commuter que deux matrices.
            \item On dit que la trace est un \textbf{invariant de similitude}.
        \end{enumerate}
    \end{omed}

    \begin{omed}{Application \textcolor{black}{(Multiples de la trace)}}{mypurple}
        Soit $\varphi : \mk{n} \to \mathbb{K}$ linéaire telle que $\forall A,B, \in \mk{n}$, $\varphi(AB) = \varphi(BA)$. Montrons qu’il existe $\lambda \in \mathbb{K}$ tel que $\varphi = \lambda \tr$.
    \end{omed}

    \begin{demo}{Solution}{mypurple}
        On sait $(E_{i,j})$ forme une base de $\mk{n}$. De plus, si $(i,j,k,l) \in \intervalleEntier{1}{n}^2$, on a $E_{i,j} E_{k,l} = \delta_{j,k}E_{i,l}$. D’où, pour tout $i,j,k,l$, 
        \[ \varphi(E_{i,j} E_{k,l}) = \varphi(E_{k,l} E_{i,j}) \iff \delta_{j,k} \varphi(E_{i,l}) = \delta_{l,i} \varphi(E_{k,j}) \esp{\textdaggerdbl} \]   
        \begin{itemize}
            \item Si $j=k$ et $l \neq i$, on obtient par \textdaggerdbl $\varphi(E_{i,l}) = 0$.
            \item Si $j=k$ et $l = i$, on obtient par \textdaggerdbl $\varphi(E_{i,i}) = \varphi(E_{j,j})$
        \end{itemize}
        Donc, en posant $\lambda = \varphi(E_n)$, pour tout $i \in \intervalleEntier{1}{n}$, $\varphi(E_{i,i}) = \lambda = \lambda \tr(E_{i,i})$ et de la même façon pour les matrices $E_{i,j}, i \neq j$. Donc $\varphi$ et $\lambda \tr$ coïncident sur une base de $\mk{n}$, donc sont égales.
    \end{demo}

    \begin{omed}{Application \textcolor{black}{(Matrices de trace nulle)}}{myolive}
        \begin{enumerate}
            \item Soit $f \in \mathcal{L}(E)$. Montrons que $f$ est une homothétie \textit{ssi} pour tout $x \in E$, la famille $(x,f(x))$ est liée.
            
            Si $f$ est une homothéthie, alors $(x,f(x))$ est bien toujours liée.
            Réciproquement, on suppose que pour tout $x \in E$, il existe $\lambda_x$ tel que $f(x) = \lambda_x x$. Soient $x,y \in E$.
            \begin{itemize}
                \item Si $(x,y)$ est liée, on a $y = \mu x$ et
                \begin{align*}
                    f(x) &= f(\mu y) = \mu \lambda_y y \\
                    &= \lambda_x x = \mu \lambda_x y 
                \end{align*}
                d’où $\lambda_x = \lambda_y$.
                \item Si $(x,y)$ est libre, alors $f(x + y) = \lambda_{x+y}(x+y)$ d’où 
                \[ (\lambda_x - \lambda_{x+y})x + (\lambda_y - \lambda_{x+y})y = 0 \]   
                On a donc $\lambda_x = \lambda_y = \lambda_{x + y}$.
            \end{itemize}
            Donc $f$ est clairement une homothétie.
            \item Soit $M \in \mk{n}$ de trace nulle. Montrons que $M$ est semblable à une matrice n’ayant que des zéros sur sa diagonale. On raisonne par récurrence, dont l’initialisation est évidente.
            
            Soit $f$ l’application linéaire associée à $M$ dans la base canonique de $\mathbb{K}^n$. 
            \begin{itemize}
                \item Si $f$ est une homothétie, alors $M$ est un multiple de l’identité et comme sa trace est nulle, c’est la matrice nulle.
                \item Sinon, soit $x \in \mathbb{K}^n$ tel que $(x,f(x))$ est libre. Alors on peut compléter cette famille en base $(x,f(x), e_3,\ldots,e_n)$. Dans cette base, la matrice de $f$ est 
                \[ N=\left(\begin{array}{c|ccc}
                    0&*&\cdots&*\\
                    \hline
                    1&\\
                    0&&N'\\
                    \vdots&
                    \end{array}\right) \]
                Autrement dit, $M$ est semblable à $N$. 

                On peut appliquer l’hypothèse de récurrence à $N$, de sorte que $N'$ est semblable à une matrice de diagonale nulle de $\GL_{n-1}(\mathbb{K})$, \textit{i.e.} $Q^{-1} N' Q$. En posant 
                \[ P=\left(\begin{array}{c|ccc}
                    1&0&\cdots&0\\
                    \hline
                    0&\\
                    0&&Q\\
                    \vdots&
                    \end{array}\right) \]   
                on obtient que $P$ est inversible et que $P^{-1} N P$ est une matrice n’ayant que des zéros sur la diagonale, et $M$ est semblable à une telle matrice.
            \end{itemize}
        \end{enumerate}
    \end{omed} 

    \subsubsection{Trace d’un endomorphisme}

    \begin{defitheo}{Trace d’un endomorphisme}{}
        \begin{soit}
            \item $E$ un $\mathbb{K}$-ev
            \item $f \in \mathcal{L}(E)$
        \end{soit}
        Le scalaire $\tr\left(\mat{\mathcal{B}}{f}\right)$ ne dépend pas du choix de la base $\mathcal{B}$.

        On l’appelle \textbf{trace} de $f$, et on le note $\tr(f)$.
    \end{defitheo}

    \begin{demo}{Justification}{mypurple}
        Soient $\mathcal{B}$ et $\mathcal{B}'$ deux bases de $E$. et $P$ la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$. On sait que $P$ est inversible, et $\mat{\mathcal{B}'}{f} = P^{-1} \mat{\mathcal{B}}{f} P$, d’où $\tr\left(\mat{\mathcal{B}'}{f}\right) = \tr\left(\mat{\mathcal{B}}{f}\right)$.
    \end{demo}

    \begin{omed}{Exemples}{mypurple}
        \begin{enumerate}[label=\textcolor{mypurple}{\arabic*.}]
            \item Si $f = \lambda \id_E$ et $\mathcal{B}$ une base de $E$, alors $\mat{\mathcal{B}}{f} = \lambda I_n$ donc $\tr(f) = \lambda n$.
            \item Si $p$ est un projecteur de $E$, alors $E = \im(p) \oplus \ker(p)$. Soit $\mathcal{B}$ une base adaptée à cette décomposition. Alors $\mat{\mathcal{B}}{p} = \left[ \begin{array}{c | c}
                I_r & 0 \\
                \hline
                0 & 0
            \end{array} \right]$ où $r = \dim(\im(p))$ donc $\tr(p) = r = \rg(p)$.
        \end{enumerate}
    \end{omed}

    \begin{omed}{Application \textcolor{black}{(Trace d’un endomorphisme)}}{mypurple}
        Soit $A \in \mk{n}$ et $\fonction{\varphi}{\mk{n}}{\mk{n}}{M}{AM}$.
        Montrons que 
        \begin{enumerate}[label = \textcolor{mypurple}{\alph* |}]
            \item $\varphi$ est un endomorphisme de $\mk{n}$ ;
            \item puis donner sa trace.
        \end{enumerate}
    \end{omed}

    \begin{demo}{Résolution}{mypurple}
        \begin{enumerate}[label = \textcolor{mypurple}{\alph* |}]
            \item On a directement que $\varphi$ est linéaire, par linéarité de l’addition de matrices.
            \item Soit $\mathcal{B}$ une base formée des $E_{i,j}$, dans l’\textbf{\textsc{ordre lexicographique}}, \textit{i.e.} $\mathcal{B} = (E_{1,1}, E_{1,2}, \ldots, E_{1,n}, E_{2,1}, \ldots, E_{2,n}, E_{3,1}, \ldots, E_{n,n})$.
            
            On cherche les coefficients diagonaux $\mat{\mathcal{B}}{\varphi}$. On pose donc $A = (a_{k,l})$. Alors $AE_{i,j}$ est la matrice formée de la $i$-ème colonne de $A$ en $j$-ème colonne, avec des $0$ sinon. D’où $\varphi(E_{i,1})$ est la matrice constituée des de la $i$-ème colonne de $A$ en première place. D’où $\tr(\varphi) = \sum_{i=1}^{n} n a_{i,i} = n \tr(A)$.
            Pour mieux y voir, la forme de la matrice est \[ \mat{\mathcal{B}}{\varphi} = \left[ \begin{array}{c | c | c | c}
                a_{1,1} I_n & a_{1,2} I_n & \cdots & a_{1,n} I_n \\
                \hline
                a_{2,1} I_n & a_{2,2} I_n & \ddots & \vdots \\
                \hline
                \vdots & \ddots & \ddots & a_{n-1, n} I_n \\
                \hline
                a{n,1} I_n & \cdots & a_{n,n-1} I_n & a_{n,n} I_n
            \end{array} \right] \]
        \end{enumerate}
    \end{demo}

\newpage

\section{Déterminant}

\subsection{Forme multilinéaire alternée}

    \begin{defitheo}{Forme multilinéaire alternée}{}
        \begin{soient}
            \item $E_1,\ldots,E_n$ et $F$ des $\mathbb{K}$-espaces vectoriels
            \item $f : E_1 \times E_n \to F$ une application 
        \end{soient}
        \begin{itemize}[label=$\rightarrow$]
            \item On dit que $f$ est une \textbf{application $n$-linéaire} lorsque $f$ est linéaire par rapport à chacune de ses variables.
            \item On dit que $f$ est \textbf{alternée} lorsque  
            \[ \forall (u_1,\ldots,u_n) \in E^n, \, \left[\exists (i,j) \intervalleEntier{1}{n}^2, \, \et{i \neq j}{u_i = u_j} \implies f(u_1,\ldots,u_n) = 0\right] \]
            \item On dit que $f$ est \textbf{antisymétrique} lorsque 
            \[ \forall (u_1,\ldots,u_n) \in E^n, \, \forall (i,j) \in \intervalleEntier{1}{n}, \, \left[i \neq j \implies f(u_1, \ldots, u_i, \ldots,u_j, \ldots, u_n) = - f(u_1, \ldots, u_j, \ldots, u_i, \ldots, u_n) \right] \]
        \end{itemize}
        Pour un $\mathbb{K}$-ev, toute \textbf{forme} multilinéaire alternée $f$ de $E^n \to \mathbb{K}$
        \begin{enumerate}
            \item est nulle sur toute famille liée.
            \item ne change pas de valeur lorsque que l’on ajoute à l’une de ses variables une combinaison linéaire des autres.
            \item est antisymétrique.
        \end{enumerate}
    \end{defitheo}

    \begin{demo}{Démonstration}{mypurple}
        \begin{enumerate}
            \item Soit $(x_1,\ldots,x_n)$ une famille liée de $E$, avec disons $x_k = \sum_{i \neq k} \lambda_i x_i$ pour un certain $k \in \intervalleEntier{1}{n}$ et des $\lambda_{i \neq k} \in \mathbb{K}$. Alors 
            \[ f(x_1,\ldots,x_n) = f\left(\ldots,x_{k-1}, \sum_{i \neq k} \lambda_i x_i, x_{k+1}, \ldots\right) = \sum_{i \neq k} \lambda_i f(\ldots,x_{k-1}, \lilbox{mypurple}{$x_i$}, x_{k+1}, \ldots) = 0 \]    
            \item Soient $x_1,\ldots,x_n \in E$ et $\lambda_{i \neq k} \in \mathbb{K}$. Alors
            \[ f\left(\ldots,x_k + \sum_{i\neq k} \lambda_i x_i, \ldots\right) = f(\ldots,x_k,\ldots) + \sum_{i \neq k} \lambda_i f(\ldots, \lilbox{mypurple}{$x_i$}, \ldots) = f(\ldots,x_k,\ldots) \]   
            \item On remarque que 
            \[ f(.,\lilbox{myred}{$x_i + x_j$},., \lilbox{myred}{$x_i + x_j$},.) = f(.,\lilbox{myred}{$x_i$},.,\lilbox{myred}{$x_i$},.) + f(.,\lilbox{mygreen}{$x_i$},.,\lilbox{mygreen}{$x_j$},.) + f(.,\lilbox{mygreen}{$x_j$},.,\lilbox{mygreen}{$x_i$},.) + f(.,\lilbox{myred}{$x_j$},.,\lilbox{myred}{$x_j$},.) \] 
            donc $f(.,x_j,.,x_i,.) = -f(.,x_i,.,x_j,.)$
        \end{enumerate}
    \end{demo}

    À présent, tâchons de déterminer toutes les formes \lilbox{mypurple}{$n$}-linéaires d’un espace vectoriel $E$ de dimension finie \lilbox{mypurple}{$n$}.

    Pour cela, nous aurons besoin de quelques définitions préliminaires :

    \begin{defitheo}{Groupe symétrique}{}
        Soit $n$ un entier positif.

        On note $S_n$ l’ensemble des bijections de $\intervalleEntier{1}{n}$ dans $\intervalleEntier{1}{n}$, \textit{i.e.} des \textbf{permutations} de $\intervalleEntier{1}{n}$.

        L’ensemble $(S_n, \circ)$ est un groupe, dit \textbf{groupe symétrique}, de cardinal $n!$.
    \end{defitheo}

    \begin{demo}{Preuve}{mypurple}
        L’application $\id$ est l’élément neutre, la loi $\circ$ est associative et tout élément possède comme inverse son application réciproque. Pour le cardinal, dénombrer ($n$ choix pour le premier, $n-1$ choix pour le second\ldots).
    \end{demo}

    \begin{defitheo}{$p$-cycle}{}
        Soit $\sigma \in S_n$.

        On dit que $\sigma$ est un \textbf{$p$-cycle} s’il existe $x_1,\ldots,x_p \in \intervalleEntier{1}{n}$ tels que 
        \[ \forall i \in \intervalleEntier{1}{p-1}, \sigma(x_i) = x_{i + 1} \esp{et} \sigma(x_p) = \sigma(x_1) \]
        On note ce $p$-cycle $\sigma =: (x_1,\ldots,x_p)$, dit de support $\{ x_1,\ldots, x_p \}$.

        Une \textbf{transposition} est un $2$-cycle, et $\uptau_{i,j} = (i,j)$. En particulier, pour tout $p$-cycle $\sigma \in S_n$, il existe des transpositions $\uptau_1,\ldots,\uptau_N$ telles que 
        \[ \sigma = \uptau_1 \circ \cdots \circ \uptau_N \]   
        On dit que $S_n$ est engendré par les transpositions.
    \end{defitheo}

    \begin{demo}{Démonstration}{mypurple}
        On raisonne par récurrence sur $n$.
        \begin{itemize}
            \item Si $n = 2$, $S_2 = \{ \id, \uptau_{1,2} \}$.
            \item Si $n = 3$, $S_3 = \{ \id, \uptau_{1,2}, \uptau_{2,3}, \uptau_{1,3}, (1,2,3), (1,3,2) \}$ où l’on remarque que 
            \[ (1,2,3) = \uptau_{1,2} \circ \uptau_{2,3} \esp{et} (1,3,2) = \uptau_{1,3} \circ \uptau_{3,2} \]   
            \item Soit $n \geq 4$, et supposons le résultat vrai pour $S_n$. 
            
            Considérons $\sigma \in S_{n+1}$ et posons $i = \sigma^{-1}(n+1)$ et $\sigma' = \sigma \circ \uptau_{i,n+1}$. Alors $\sigma'(n+1) = n+1$ et donc $\sigma'(\intervalleEntier{1}{n}) = \intervalleEntier{1}{n}$, d’où $\sigma'$ est une permutation de $\intervalleEntier{1}{n}$. D’où 
            \[ \sigma' = \uptau_1 \circ \cdots \circ \uptau_N = \sigma \circ \uptau_{i,n+1} \esp{donc} \sigma = \uptau_1 \circ \cdots \circ \uptau_N \circ \uptau_{i,n+1} \]
        \end{itemize}
    \end{demo}

    \begin{defitheo}{Signature et nombre d’inversions}{}
        Pour tout $\sigma \in S_n$, on appelle \textbf{signature} de $\sigma$ le réel 
        \[ \varepsilon(\sigma) := \prod_{1 \leq i < j \leq n} \frac{\sigma(j) - \sigma(i)}{j - i} \]   
        De plus, on pose $N_{\sigma}$ le \textbf{nombre d’inversions} de $\sigma$, 
        \[ N_{\sigma} := \card\left\{ (i,j) \in \intervalleEntier{1}{n}^2, \quad i < j \text{ et } \sigma(i) > \sigma(j) \right\} \]

        Ainsi définis, 
        \begin{enumerate}
            \item $\varepsilon(\sigma \circ \gamma) = \varepsilon(\sigma)\varepsilon(\gamma)$ \quad (\textit{morphisme de groupe})
            \item $\varepsilon(\sigma) \in \{-1,1\}$
            \item $\varepsilon(\sigma^{-1}) = \varepsilon(\sigma)$
            \item $\varepsilon(\sigma) = (-1)^{N_{\sigma}}$
            \item $\varepsilon(\uptau_{i,j}) = -1$
            \item $\varepsilon((x_1,\ldots,x_p)) = (-1)^p$
        \end{enumerate}
    \end{defitheo}

    \begin{demo}{Preuve}{mypurple}
        \begin{enumerate}
            \item Soit $\sigma,\gamma \in S_n$
            \begin{align*}
                \varepsilon(\sigma \circ \gamma) 
                &= \prod_{1 \leq i < j \leq n} \frac{\sigma \circ \gamma(j) - \sigma \circ \gamma(i)}{j - i} \\
                &= \prod_{1 \leq i < j \leq n} \frac{\sigma \circ \gamma(j) - \sigma \circ \gamma(i)}{\gamma(j) - \gamma(i)} \times \frac{\gamma(j) - \gamma(i)}{j - i} \\
                &= \prod_{1 \leq i < j \leq n} \frac{\sigma \circ \gamma(j) - \sigma \circ \gamma(i)}{\gamma(j) - \gamma(i)} \times \prod_{1 \leq i < j \leq n} \frac{\gamma(j) - \gamma(i)}{j - i} \\
                &= \varepsilon(\sigma) \varepsilon(\gamma)
            \end{align*}
            \item $\varepsilon(\id) = \varepsilon(\sigma \circ \sigma) = \varepsilon(\sigma)^2 = 1$
            \item $\varepsilon(\id) = \varepsilon(\sigma \circ \sigma^{-1}) = \varepsilon(\sigma) \varepsilon(\sigma^{-1}) = 1$
            \item On remarque que 
            \[ N_{\sigma} = \card\left\{ (i,j) \in \intervalleEntier{1}{n}^2, \quad \frac{\sigma(j) - \sigma(i)}{j - i} < 0\right\} \]
            donc le signe de $\varepsilon(\sigma)$ est celui de $(-1)^{N_{\sigma}}$ et comme $\varepsilon(\sigma) \in \{ -1,1 \}$, on a $\varepsilon(\sigma) = (-1)^{N_{\sigma}}$.
            \item On remarque que si $i < j$, $N_{\uptau} = 2(j-i) + 1$ d’où le résultat en appliquant le résultat précédent.
            \item $(x_1,\ldots,x_p) = \uptau_{x_1,x_2} \circ \cdots \circ \uptau_{x_1,x_p}$. Donc $\varepsilon(\sigma) = \prod \varepsilon(\uptau) = (-1)^p$.
        \end{enumerate}
    \end{demo}

    Soient $E$ un tel $\mathbb{K}$-ev, $f$ une forme $n$-linéaire alternée de $E^n$, $\mathcal{B} = (e_1,\ldots,e_n)$ une base de $E$ et $(x_1,\ldots,x_n)$ une famille de $n$ vecteurs de $E$ de matrice $A$ dans $\mathcal{B}$.
    \begin{align*}
        f(x_1,\ldots,x_n) 
        &= f\left(\sum_{k_1=1}^{n}a_{k_1,1} e_{k_1}, \ldots, \sum_{k_n = 1}^{n} a_{k_n, n} e_{k_n}\right) \\
        &= \sum_{\substack{(k_1,\ldots,k_n) \\ \in \intervalleEntier{1}{n}^n}} a_{k_1, 1} \cdots a_{k_n,n} \underbrace{\lilbox{mypurple}{$f(e_{k_1},\ldots,e_{k_n})$}}_{= 0 \text{ si } \exists k_i = k_j} \\
        &= \sum_{\sigma \in S_n} \left(\prod_{i=1}^n a_{\sigma(i),i}\right)f\left(e_{\sigma(1)}, \ldots, e_{\sigma(n)}\right) 
    \end{align*}

    \begin{theo}{Caractérisation des formes multilinéaires alternées par la signature}{}
        Soient $E$ un $\mathbb{K}$-ev et $f$ une forme $n$-linéaire de $E^n$.

        $f$ est alternée \textit{ssi} pour tous $x_1,\ldots,x_n \in E$, et $\sigma \in S_n$,
        \[ f\left(x_{\sigma(1)},\ldots,x_{\sigma(n)}\right) = \varepsilon(\sigma) f(x_1,\ldots,x_n) \]   
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        \begin{itemize}
            \item[\textcolor{myred}{$\impliedby$}] Soit $x_1,\ldots,x_n \in E$. On suppose que $x_i = x_j$ pour certains $i,j \in \intervalleEntier{1}{n}$ pour lesquels $i < j$ et on note $\uptau$ la transposition $(i,j)$. Alors 
            \begin{align*}
                f(x_1,.,x_n) &= f(.,x_{i-1},x_j,x_{i+1},.,x_{j-1},x_i,x_{j+1},.) = f\left(x_{\uptau(1)},.,x_{\uptau(n)}\right) \\
                &= \varepsilon(\uptau)f(x_1,\ldots,x_n) = -f(x_1,\ldots,x_n)
            \end{align*}
            Donc $f(x_1,\ldots,x_n) = 0$, et $f$ est alternée.
            \item[\textcolor{myred}{$\implies$}] Réciproquement, si $f$ est alternée, comme $S_n$ est engendré par ses transpositions et la signature d’un produit de $p$ transpositions vaut $(-1)^p$, il nous suffit d’établir le résultat dans un seul cas où $\sigma$ est une transposition. Nous l’avons déjà fait, c’est la propriété d’antisymétrie de $f$.
        \end{itemize}
    \end{demo}

    Finalement, on a donc 
    \[ f(x_1,\ldots,x_n) = \left(\sum_{\sigma \in S_n} \lilbox{myred}{$\varepsilon(\sigma)$} \prod_{i = 1}^n a_{\sigma(i),i}\right) f(e_1,\ldots,e_n) \]

\subsection{Déterminant d’une famille de vecteurs et d’une matrice carrée}

    \subsubsection{Définition}

    \begin{defitheo}{Déterminant d’une famille de vecteurs dans une base et d’une matrice carrée}{}
        Soient $E$ un $\mathbb{K}$-ev de dimension $n$ et $\mathcal{B}$ une base de $E$. Pour toute famille $\mathcal{F}$ de $n$ vecteurs de $E$ de matrice $A$ dans $\mathcal{B}$, on appelle déterminant de $\mathcal{F}$ dans $\mathcal{B}$ le scalaire 
        \[ \det(A) = \det_{\mathcal{B}}(\mathcal{F}) = \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i=1}^n a_{\sigma(i),i} \]
        Ainsi défini sur $E^n$, le déterminant dans la base $\mathcal{B}$ est une forme $n$-linéaire alternée de $E^n$ telle que $\det_{\mathcal{B}}(\mathcal{B}) = 1$.
    \end{defitheo}

    \begin{omed}{Remarque}{mypurple}
        On notera $\det(A) = \begin{vmatrix}
            a_{1,1} & \cdots & a_{1,n} \\
            \vdots & \ddots & \vdots \\
            a_{n,1} & \cdots & a_{n,n}
        \end{vmatrix}$
    \end{omed}

    \begin{demo}{Justification}{mypurple}
        \begin{itemize}
            \item \textbf{Multilinéarité} \quad Se démontre.
            \item \textbf{Caractère alterné} \quad Soient $x_1,\ldots,x_n \in E$ et $\varphi \in S_n$. Dans le calcul suivant, on effectue le changement d’indice $j = \varphi(i)$ associé à la bijection $\varphi(i)$, puis le changement d’indice $\theta = \sigma \varphi^{-1}$.
            \begin{align*}
                \det_{\mathcal{B}}\left(x_{\varphi(1)},.,x_{\varphi(n)}\right) 
                &= \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{\sigma(i), \varphi(i)} \\
                \lilbox{mypurple}{$j = \varphi(i)$} 
                &= \sum_{\sigma \in S_n} \varepsilon(\sigma) \prod_{j = 1}^n a_{\sigma(\varphi^{-1}(j)), j} \\
                \lilbox{mypurple}{$\theta = \sigma \varphi^{-1}$} 
                &= \sum_{\theta \in S_n} \varepsilon(\theta \varphi) \prod_{j = 1}^n a_{\theta(j), j} \\
                &= \sum_{\theta \in S_n} \varepsilon(\theta)\varepsilon(\varphi) \prod_{j = 1}^n a_{\theta(j), j} \\
                &= \varepsilon(\varphi) \sum_{\theta \in S_n} \varepsilon(\theta) \prod_{i = 1}^n a_{\theta(i), i} \\
                &= \varepsilon(\varphi) \det_{\mathcal{B}}(x_1,\ldots,x_n)
            \end{align*}
            \item \textbf{Calcul de $\det_{\mathcal{B}}(\mathcal{B})$} \quad La matrice $B$ de $\mathcal{B}$ dans $\mathcal{B}$ est $I_n$, donc pour tout $\sigma \in S_n$ distinct de l’identité, $\prod_{i=1}^n b_{\sigma(i),i} = 0$. Donc 
            \[ \det_{\mathcal{B}}(\mathcal{B}) = \varepsilon(\id) \prod_{i=1}^n b_{\id(i), i} = 1 \]
        \end{itemize}
    \end{demo}

    \subsubsection{Changement de bases et caractérisation}

    \begin{theo}{Toute forme multilinéaire alternée est un multiple du déterminant dans une base donnée}{}
        Soient $E$ un $\mathbb{K}$-ev de dimension $n$, $\mathcal{B}$ une base de $E$ et $f$ une forme $n$-alternée de $E^n$.

        Alors $f = f(\mathcal{B}) \det_{\mathcal{B}}$.

        Cela signifie que l’ensemble des formes multilinéaires alternées de $E^n$ est un $\mathbb{K}$-ev de dimension $1$.
    \end{theo}

    \begin{demo}{Idée}{myred}
        Le résultat a été montré plus haut, 
        \[ f(x_1,\ldots,x_n) = \left(\sum_{\sigma \in S_n} \lilbox{myred}{$\varepsilon(\sigma)$} \prod_{i = 1}^n a_{\sigma(i),i}\right) f(e_1,\ldots,e_n) \]
    \end{demo}

    \begin{prop}{Formule de changement de base pour le déterminant d’une famille de vecteurs}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \geq 2$
            \item $\mathcal{B} = (e_1,\ldots,e_n)$ et $\mathcal{B}' = (e_1',\ldots,e_n')$ deux bases de $E$
        \end{soient}

        Alors \[ \det_{\mathcal{B}'} = \det_{\mathcal{B}'}(\mathcal{B})\det_{\mathcal{B}} \]
        En particulier, $\det_{\mathcal{B}'}(\mathcal{B}) \det_{\mathcal{B}}(\mathcal{B}') = 1$
    \end{prop}

    \begin{demo}{Preuve}{myred}
        Comme l’ensemble des formes multilinéaires alternées de $E^n$ est une droite vectorielle, donc $\det_{\mathcal{B}'} = \lambda \det_{\mathcal{B}}$. En évaluant en la base $\mathcal{B}$, on obtient l’égalité.
    \end{demo}

    \begin{coro}{Caractérisation des bases par le déterminant}{}
        \begin{soient}
            \item $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \geq 2$
            \item $\mathcal{B} = (e_1,\ldots,e_n)$ une base de $E$
            \item $u_1, \ldots,u_n$ des vecteurs de $E$
        \end{soient}

        Alors \[ (u_1,\ldots,u_n) \text{ est une base de } E \iff \det_{\mathcal{B}}(u_1,\ldots,u_n) \neq 0 \]
    \end{coro}

    \subsubsection{Propriétés}

    \begin{demo}{Preuve}{myorange}
        \begin{itemize}
            \item[$\implies$] Si $\mathcal{B}' = (u_1,\ldots,u_n)$ est une base de $E$, par la formule de changement de base, 
            \[ 1 = \det_{\mathcal{B}'}(\mathcal{B}') = \det_{\mathcal{B}'}(\mathcal{B}) \det_{\mathcal{B}}(\mathcal{B}') \]   
            Or le produit dans $\mathbb{K}$ est intègre, donc $\det_{\mathcal{B}}(\mathcal{B}')$.
            \item[$\impliedby$] Si $(u_1,\ldots,u_n)$ n’est pas une base de $E$, alors la famille $(u_1,\ldots,u_n)$ n’est pas libre. Un des vecteurs est alors combinaison linéaire des autres. Le déterminant étant alterné, on a alors $\det_{\mathcal{B}}(u_1,\ldots,u_n) = 0$.
        \end{itemize}
    \end{demo}

    \begin{prop}{Cas où $n=2$}{}
        \begin{soient}
            \item $E$ un espace vectoriel de dimension $2$
            \item $\mathcal{B} = (e_1,e_2)$ une base de $E$
            \item $x = x_1e_1 + x_2e_2$ et $y = y_1e_1 + y_2e_2$ deux vecteurs de $E$
        \end{soient}
        Alors \begin{align*}
            \det_{\mathcal{B}}(x,y) &= \det_{\mathcal{B}}(x_1e_1 + x_2e_2, y = y_1e_1 + y_2e_2) \\
            &= x_1 y_2 - x_2 y_1
        \end{align*}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Étant donné que $\mathfrak{S}_{2} = \left\{\id, \uptau_{1,2}\right\}$, 
        \begin{align*}
            \det_{\mathcal{B}}(u,v) 
            &= \begin{vmatrix}
                x_1 & y_1 \\
                x_2 & y_2
            \end{vmatrix} \\
            &= \sum_{\sigma \in \mathfrak{S}_2} \varepsilon(\sigma) \prod_{i=1}^2 x_{\sigma(i)} y_{\sigma(i)} \\
            &= x_1 y_2 - x_2 y_1
        \end{align*}
    \end{demo}

    \begin{prop}{Cas où $n=3$}{}
        \begin{soient}
            \item $E$ un espace vectoriel de dimension $3$
            \item $\mathcal{B} = (e_1,e_2,e_3)$ une base de $E$
            \item $u = xe_1 + ye_2 + ze_3$, $v = x'e_1 + y'e_2 + z'e_3$ et $w = x'' e_1 + y''e_2 + z''e_3$ trois vecteurs de $E$
        \end{soient}
        Alors \[ \det_{\mathcal{B}}(u,v,w) = xy'z'' + x'y''z + x''yz' - xy''z - x'yz'' - x''y'z \]
        On appelle généralement ce résultat \textbf{Règle de Sarrus}.
    \end{prop}

    \begin{omed}{Méthode (Règle de Sarrus)}{myolive}
        Écrire la matrice en rajoutant deux premières colonnes $\begin{vmatrix}
            x & x' & x'' & x & x' \\
            y & y' & y'' & y & y'\\
            z & z' & z'' & z & z' 
        \end{vmatrix}$
        et mettre le signe $+$ devant les produits de diagonales de la gauche vers la droite \lilbox{myolive}{$+ xy'z'' + x'y''z + x''yz'$} et un signe $-$ devant les produits de diagonales de la droite vers la gauche \lilbox{myolive}{$- xy''z - x'yz'' - x''y'z$}.
    \end{omed}

    \begin{demo}{Idée}{myolive}
        Procéder de la même façon que pour $n = 2$, on considérant $\mathfrak{S}_3 = \left\{\id, \uptau_{1,2}, \uptau_{1,3}, \uptau_{2,3}, (1,2,3), (1,3,2)\right\}$.
    \end{demo}

    \begin{prop}{}{}
        Soient $A,B \in \mk{n}$.
        \begin{enumerate}
            \item $\det(AB) = \det(A)\det(B)$
            \item $A$ est inversible \textit{ssi} $\det(A) \neq 0$. Dans ce cas, $\det(A^{-1}) = \frac{1}{\det(A)}$.
            \item Le déterminant est un invariant de similitude.
            \item Le déterminant est un invariant de transposition. 
        \end{enumerate}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item Soient $B_1,\ldots,B_n$ les colonnes de $B$ et $\mathcal{B}$ la base canonique de $\mathbb{K}^n$. On sait que 
        \[ \det(B) = \det_{\mathcal{B}}(B_1,\ldots,B_n) \]   
        Or, les colonnes de la matrice $AB$ sont $A B_1,\ldots, A B_n$, donc $\det(AB) = \det_{\mathcal{B}}(AB_1,\ldots,AB_n)$. L’application $\fonction{\Psi}{\left(\mathbb{K}^n\right)^n}{\mathbb{K}}{(X_1,\ldots,X_n)}{\det_{\mathcal{B}}(AX_1,\ldots,AX_n)}$ est multilinéaire alternée, donc est un multiple du déterminant $\det_{\mathcal{B}}$, \textit{i.e.} il existe $\lambda \in \mathbb{K}$ tel que 
        \[ \Psi = \lambda \det_{B} \]   
        Si $(X_1,\ldots,X_n)$ est la base canonique de $\mathbb{K}^n$, alors (en notant $A_1,\ldots,A_n$ les colonnes de $A$.)
        \[ \Psi(X_1,\ldots,X_n) = \det_{\mathcal{B}}(A_1, \ldots, A_n) = \lambda \det_{\mathcal{B}}(\mathcal{B}) = \lambda \]
        Donc $\lambda = \det(A)$. 
            \item $A$ est inversible \textit{ssi} $(A_1,\ldots,A_n)$ forment une base de $\mathbb{K}^n$ \textit{ssi} $\det_{\mathcal{B}}(A_1,\ldots,A_n) = \det(A) \neq 0$. De plus, si $A \in \GL_n(\mathbb{K})$, alors $\det(A)\det(A^{-1}) = \det(AA^{-1}) = \det(I_n) = 1$ d’où le résultat.
            \item Si $A = P^{-1} B P$, alors $\det(A) = \det(P^{-1}) \det(B) \det(P) = \det(B) \det(P^{-1} P) = \det(B)$.
            \item En posant $A = \left(a_{i,j}\right)$, on a 
            \begin{align*}
                \det(A^{\top}) 
                &= \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \sigma(i)} \\
                &\quad \downarrow \quad \text{Réarrangement des termes} \\
                &= \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{\sigma^{-1}, i} \\
                \lilbox{myolive}{$\sigma \leftarrow \sigma^{-1}$ (involution)} &= \sum_{\sigma \in \mathfrak{S}_n} \varepsilon(\sigma) \prod_{i = 1}^n a_{\sigma(i), i} \\
                &= \det(A)
            \end{align*}
        \end{enumerate}
    \end{demo}

    \subsubsection{Méthodes de calcul}

    \begin{theo}{Propriétés du déterminant}{}
        Lorsqu’on calcule le déterminant d’une matrice carrée $A \in \mk{n}$,
        \begin{enumerate}
            \item Lorsqu’on échange deux colonnes de $A$, on multiplie le déterminant par $(-1)$.
            \item Si deux colonnes de $A$ sont égales, le déterminant est nul.
            \item Si une colonne de la matrice est nulle, le déterminant est nul.
            \item Multiplier une colonne de la matrice par $\lambda$ multiplie le déterminant par $\lambda$.
            \item $\det(\lambda A) = \lambda^n \det(A)$.
            \item Rajouter à une colonne de $A$ une combinaison linéaire des autres colonnes de $A$ ne modifie pas la valeur du déterminant.
            \item Si les colonnes sont liées, le déterminant est nul.
        \end{enumerate}
        Ces propriétés restent vraies pour des opérations sur les lignes.
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        \begin{enumerate}
            \item Par définition d’une forme multilinéaire alternée
            \item Par définition d’une forme multilinéaire alternée.
            \item Par définition d’une forme multilinéaire alternée
            \item Par définition d’une forme multilinéaire alternée
            \item Immédiat par la propriété précédente.
            \item Par définition d’une forme multilinéaire alternée.
            \item Par définition d’une forme multilinéaire alternée.
        \end{enumerate}
    \end{demo}

    \begin{omed}{Exemple}{myred}
        Calcul du déterminant de $A = \begin{bmatrix}
            a & b & \cdots & b \\
            b & \ddots & \ddots & \cdots \\
            \vdots & \ddots & \ddots & b \\
            b & \cdots & b & b
        \end{bmatrix}$ pour $a,b \in \mathbb{K}$.

        \begin{align*}
            \det(A) = \begin{vmatrix}
                a & b & \cdots & b \\
                b & \ddots & \ddots & \cdots \\
                \vdots & \ddots & \ddots & b \\
                b & \cdots & b & b
            \end{vmatrix} 
            &= \begin{vmatrix}
                a + (n-1)b & b & & & b \\
                 & a & & & \\
                 & b & & & \\
                 & & & & b \\
                 a + (n-1)b & b & & b & a 
            \end{vmatrix} \\
            &= (a + (n-1)b)  \begin{vmatrix}
                1 & b & & & b \\
                 & a & & & \\
                 & b & & & \\
                 & & & & b \\
                1 & b & & b & a 
            \end{vmatrix} \\
            &= (a + (n-1)b) \begin{vmatrix}
                1  & 0 & & & 0 \\
                 & a - b & & & \\
                 & 0 & & & \\
                 & & & & 0 \\
                1 & 0 & & 0 & a-b 
            \end{vmatrix}\\
            &= (a + (n-1)b)(a-b)^{n-1}
        \end{align*}
    \end{omed}

    \begin{defi}{Mineur relatif et cofacteur}{}
        Soit $M \in \mk{n}$ et $(i,j) \in \intervalleEntier{1}{n}^2$.
        \begin{itemize}
            \item Le \textbf{mineur relatif} à $m_{i,j}$ est le déterminant $\Delta_{i,j}$ de la matrice de $\mk{n-1}$ obtenue en enlevant la $i$-ème ligne et la $j$-ème colonne de $M$.
            \item Le \textbf{cofacteur} de $m_{i,j}$ est le nombre $(-1)^{i+j} \Delta_{i,j}$.
        \end{itemize}
    \end{defi}

    \begin{lem}{}{}
        Soit $A = \begin{bmatrix}
            0 & \cdots & 0 & 1 & 0 & \cdots & 0 \\
            a_{2,1} & & & \cdots & & & a_{2,n} \\
            \vdots & & & \ddots & & & \vdots \\
            a_{n,1} & & & \cdots & & & a_{n,n}
        \end{bmatrix}$, où le $1$ est en $j$-ème colonne.

        Alors $\det(A) = (-1)^{j + 1} \det(\Delta_{1,j})$. Ce résultat est aussi vrai pour les colonnes, puisque le déterminant est un invariant de transposition.
    \end{lem}

    \begin{demo}{Preuve}{mybrown}
        Posons $C_1 = \begin{bmatrix}
            a_{2,1} \\
            \vdots \\
            a_{n,1}
        \end{bmatrix}, \ldots, C_n = \begin{bmatrix}
            a_{2,n} \\
            \vdots \\
            a_{n,n}
        \end{bmatrix}$.
        L’application $(C_1,\ldots,C_{j-1}, C_{j+1}, C_n) \mapsto \det(A)$ est multilinéaire alternée (se vérifie simplement), donc $\det(A) = \lambda \det_{\mathcal{B}_c}(C_1,\ldots, C_{j-1}, C_{j}, \ldots, C_n)$. En choisissant $(C_1,\ldots,C_{n-1})$ la base canonique de $\mathbb{K}^{n-1}$, on obtient \[  \varphi(C_1,\ldots,C_{n-1}) = \det\left(\begin{bmatrix}
            0 & & 0 & 1 & 0 & & 0 \\
            1 & & & a_1 & & & 1 \\
            0 & 1 & & & & & 0 \\ 
            & & & & & & \\
            0 & & & a_{n-1} & & & 1 
        \end{bmatrix}\right) = \lambda \det(I_{n-1}) = \lambda \]   . Or, les opérations $L_i \leftarrow L_i - a_i L_1$ pour $i \in \intervalleEntier{2}{n}$ donnent $\lambda = \begin{vmatrix}
            0 & & 0 & 1 & 0 & & 0 \\
            1 & & & 0 & & & 1 \\
            0 & 1 & & & & & 0 \\ 
            & & & & & & \\
            0 & & & 0 & & & 1 
        \end{vmatrix}$. En appliquant le $j$-cycle $\sigma = (1,2,\ldots,j)$, on obtient $\lambda = \varepsilon(\sigma) \det(I_n) = (-1)^{j+1}$.
    \end{demo}

    \begin{theo}{Développement par rapport à une ligne ou une colonne}{}
        \begin{soient}
            \item $A = \left((a_{i,j})\right)_{1 \leq i,j \leq n} \in \mk{n}$
        \end{soient}
        \begin{alors}
            \item $\forall j \in \intervalleEntier{1}{n}, \, \det(A) = \sum\limits_{i=1}^n (-1)^{i+j} a_{i,j} \Delta_{i,j}$ (Développement par rapport à la $j$-ème colonne)
            \item $\forall i \in \intervalleEntier{1}{n}, \, \det(A) = \sum\limits_{j=1}^n (-1)^{i+j} a_{i,j} \Delta_{i,j}$ (Développement par rapport à la $i$-ème ligne)
        \end{alors}
    \end{theo}

    \begin{demo}{Preuve}{myred}
        Prouvons le résultat pour le développement par rapport à la première colonne. Posons $C_1,\ldots,C_n$ les colonnes de $A$ et $\mathcal{B} = (E_1,\ldots,E_n)$ la base canonique de $\mathbb{K}^n$. On sait que 
        \begin{align*}
            \det(A) 
            &= \det_{\mathcal{B}}(C_1,\ldots,C_n) \esp{où} C_1 = \sum_{i=1}^n a_{i,1} E_i \\
            &= \sum_{i=1}^n a_{i,1} \det_{\mathcal{B}}(E_{i}, C_2,\ldots,C_n) \\
            & \quad \downarrow \quad \text{D’après le lemme} \\
            &= \sum_{i=1}^n (-1)^{i+1} a_{i,1} \det(\Delta_{i,1})
        \end{align*}
        Où les colonnes $C'_{2,i}, \ldots, C'_{n,i}$ sont obtenues en retirant le $i$-ème terme.
    \end{demo}

    \begin{omed}{Méthode}{myred}
        Pour calculer un déterminant par développement,
        \begin{enumerate}
            \item On choisit une colonne (ou ligne) comportant le plus de 0 possible (éventuellement après un nettoyage préliminaire), et on l’encadre (pour le correcteur).
            \item Pour trouver le coefficient $(-1)^{i+j}$, on compte le nombre de déplacements pour aller de la case en haut à gauche à $m_{i,j}$, en inversant le signe à chaque fois.
            \item Pour trouver le mineur relatif à $m_{i,j}$, on raye la ligne et la colonne correspondante.
        \end{enumerate}

        En utilisant les développements par rapport à une ligne ou une colonne, on peut mener des raisonnements par récurrence.
    \end{omed}

    \begin{omed}{Application}{myred}
        Posons $\delta_n = \begin{vmatrix}
            a & b & & 0 \\
            c & \ddots & \ddots & \\
            & \ddots & & b\\
            0 & & c & a
        \end{vmatrix}$ et calculons le.
        \begin{align*}
            \delta_n &= \begin{vmatrix}
                a & b & & 0 \\
                c & \ddots & \ddots & \\
                & \ddots & & b\\
                0 & & c & a
            \end{vmatrix} \\
            & \quad \downarrow \quad \text{Dvp p/r à la première ligne} \\
            &= a \begin{vmatrix}
                a & b & & 0 \\
                c & \ddots & \ddots & \\
                & \ddots & & b\\
                0 & & c & a
            \end{vmatrix} - b \begin{vmatrix}
                c & b & & 0 \\
                0 & a & \ddots & \\
                 & c & \ddots & b\\
                0 & & c & a
            \end{vmatrix} \\
            & \quad \downarrow \quad \text{Dvp p/r à la première colonne} \\
            &= a \Delta_{n-1} - b \begin{vmatrix}
                a & b & & 0 \\
                c & \ddots & \ddots & \\
                & \ddots & & b\\
                0 & & c & a
            \end{vmatrix} \\
            &= a \Delta_{n-1} - b c \Delta_{n-2}
        \end{align*} 
        Ainsi, en posant l’équation caractéristique $(E_c) : X^2 - aX + bc = 0$ \begin{itemize}
            \item \textbf{Si $(E_c)$ possède deux racines réelles $r_1$ et $r_2$} \quad Il existe $\alpha$ et $\beta$ $\in \mathbb{K}$ tels que $\forall n \in \mathbb{N}^*, \Delta_{n} = \alpha r_1^n + \beta r_2^n$. On calcule ensuite à l’aide des deux premiers termes les valeurs de $\alpha$ et $\beta$ (on peut se simplifier la tache en posant $\Delta_0 = 1$). 
            \item \textbf{Si $(E_c)$ possède une racine double $r$} \quad Il existe $\alpha$ et $\beta$ $\in \mathbb{K}$ tels que $\forall n \in \mathbb{N}^*, \Delta_n = (\alpha + n \beta) r^n$.
            \item \textbf{Si $(E_c)$ n’a pas de racines réelles} \quad Alors $(E_c)$ possède deux racines complexes conjugueés $r e^{\pm i \theta}$, et il existe $\alpha, \beta \in \mathbb{K}$ tels que $\forall n \in \mathbb{N}^*, \Delta_n = r^n \left(\alpha \cos(n\theta) + \beta \sin(n \theta)\right)$
        \end{itemize}
    \end{omed}

    \begin{coro}{Déterminant d’une matrice triangulaire}{}
        Soit $T = \left((t_{i,j})\right)_{1 \leq i,j \leq n}$ une matrice triangulaire.

        Alors \[ \det(T)=\prod_{i=1}^n t_{i,i} \]
    \end{coro}

    \begin{demo}{Preuve}{myorange}
        \begin{align*}
            \begin{vmatrix}
                \lambda_1 & & * \\
                & \ddots & \\
                0 & & \lambda_n
            \end{vmatrix} &= \lambda_1 \begin{vmatrix}
                \lambda_2 & & * \\
                & \ddots & \\
                0 & & \lambda_n
            \end{vmatrix} \\
            &= \prod_{i=1}^n \lambda_i
        \end{align*}
    \end{demo}

\subsection{Déterminant d’un endomorphisme}

    \begin{theo}{Déterminant d’un endomorphisme}{}
        \begin{soient}
            \item $E$ un espace vectoriel de dimension finie $n \geq 2$
            \item $\varphi \in \mathcal{L}(E)$
            \item $\mathcal{B} = (e_1, \ldots, e_n)$ et $\mathcal{B}' = (e_1',\ldots,e_n')$ deux bases de $E$
        \end{soient}
        Alors \[ \det(\mat{\mathcal{B}}{\varphi}) =  \det_{\mathcal{B}'}(\varphi(e_1'),\ldots,\varphi(e_n')) = \det_{\mathcal{B}}(\varphi(e_1),\ldots,\varphi(e_n)) = \det(\mat{\mathcal{B}'}{\varphi}) \]
        On l’appelle donc déterminant de $\varphi$, que l’on note $\det(\varphi)$.
    \end{theo}

    \begin{demo}{Démonstration}{mypurple}
        Soit $P$ la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$, qui est inversible. Alors 
        \begin{align*}
            \det(\mat{\mathcal{B}'}{\varphi}) 
            &= \det\left(P^{-1} \mat{\mathcal{B}}{\varphi} P\right) \\
            &= \det(P^{-1}) \det\left(\mat{\mathcal{B}}{\varphi}\right) \det(P) \\
            &= \det\left(\mat{\mathcal{B}}{\varphi}\right)
        \end{align*}
    \end{demo}

    \begin{prop}{Propriétés du déterminant d’un endomorphisme}{}
        \begin{soient}
            \item $E$ un espace vectoriel de dimension finie $n \geq 2$
            \item $\varphi$, $f$ et $\psi$ deux endomorphismes de $E$
        \end{soient}
        \begin{alors}
            \item $\det(\varphi \circ \psi) = \det(\varphi)\det(\psi)$
            \item $f \text{ est bijective} \iff \det(f) \neq 0$. Dans ce cas, $\det(f^{-1}) = \frac{1}{\det(f)}$.
            \item Soit $\mathcal{B}$ une base de $E$, et $\mathcal{F} = (u_1,\ldots,u_n)$ une famille de $E^n$, alors 
            \[ \det_{\mathcal{B}}\left(f(u_1), \ldots, f(u_n)\right) = \det(f) \det_{\mathcal{B}}(u_1,\ldots,u_n) \]
        \end{alors}
    \end{prop}

    \begin{demo}{Idée}{myorange}
        \begin{enumerate}
            \item On sait que, dans une base $\mathcal{B}$, $\mat{\mathcal{B}}{\varphi \circ \psi} = \mat{\mathcal{B}}{\varphi} \mat{\mathcal{B}}{\psi}$, puis on passe au déterminant.
            \item On sait que $f \in \GL(E) \iff \mat{\mathcal{B}}{f} \in \GL_n(\mathbb{K}) \iff det\left(\mat{\mathcal{B}}{f}\right) \neq 0 \iff \det(f) \neq 0$.
            \item L’application $\fonction{\varphi}{E^n}{\mathbb{K}}{(u_1,\ldots,u_n)}{\det_{\mathcal{B}}\left(f(u_1), \ldots, f(u_n)\right)}$ est multilinéaire alternée, donc il existe $\alpha$ tel que $\varphi = \alpha \det_{\mathcal{B}}$. En évaluant en $\mathcal{B}$, on a donc $\alpha = \varphi(\mathcal{B})$ et $\det_{\mathcal{B}}(f(u_1), \ldots, f(u_n)) = \det_{\mathcal{B}}(u_1,\ldots,u_n) \det(f)$.
        \end{enumerate}
    \end{demo}

\subsection{Applications}

    \subsubsection{Matrice de Vandermonde}

    \begin{defi}{Matrice de Vandermonde}{}
        Soient $a_1,\ldots,a_n \in \mathbb{K}$.
        
        La \textbf{matrice de Vandermonde} associée à ces nombres est la matrice 
        \[ \begin{pmatrix}
            1 & a_1 & a_1^2 & \ldots & a_1^{n-1} \\
            1 & a_2 & a_2^2 & \ldots & a_2^{n-1} \\
            \vdots & \vdots &  &  & \vdots \\
            1 & a_n & a_n^2 & \ldots & a_n^{n-1} 
        \end{pmatrix} \]
    \end{defi}

    \begin{theo}{Déterminant de Vandermonde}{}
        Soient $a_1,\ldots,a_n \in \mathbb{K}$.
        
        Alors \[ V(a_1,\ldots,a_n) = \begin{vmatrix}
            1 & a_1 & a_1^2 & \ldots & a_1^{n-1} \\
            1 & a_2 & a_2^2 & \ldots & a_2^{n-1} \\
            \vdots & \vdots &  &  & \vdots \\
            1 & a_n & a_n^2 & \ldots & a_n^{n-1} 
        \end{vmatrix} = \prod\limits_{1 \leq i < j \leq n} (a_j - a_i) \]
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        Pour $n \geq 2$, on note $\mathcal{P}(n)$ la propriété suivante : pour tous $a_1,\ldots,a_n \in \mathbb{K}$, on a
        \[ V(a_1,\ldots,a_n) = \prod\limits_{1 \leq i < j \leq n} (a_j - a_i) \] 
    \begin{itemize}
    \item Pour $n = 2$, on a directement $\begin{vmatrix}
        1 & a_1 \\
        1 & a_2
    \end{vmatrix} = a_2 - a_1$
    \item Soit $n \in \mathbb{N} \backslash \{ 0,1 \}$ tel que $\mathcal{P}(n)$ est vraie.  Soient $a_1,\ldots,a_{n+1} \in \mathbb{K}$
    
    On étudie le polynôme $P$ tel que $P(X) = V(a_1,\ldots,a_n,X)$ : 
    \begin{align*}
        P(X) &= \begin{vmatrix}
            1 & a_1 & a_1^2 & \ldots & a_1^{n} \\
            \vdots & \vdots &  &  & \vdots \\
            1 & a_n & a_n^2 & \ldots & a_n^{n} \\
            1 & X  & X^2 & \ldots & X^n
        \end{vmatrix} \\
        & \downarrow \text{Développement p/r à la dernière ligne} \\
        &= (-1)^{n-1} \left( \Delta_{n,1} - X \Delta_{n,2} + \ldots + (-1)^{n-1} \lilbox{myred}{$X^n \Delta_{n,n}$} \right)
    \end{align*}
    $P$ est donc un polynôme de degré $n$, qui a pour coefficient dominant 
    \begin{align*}
        \Delta_{n,n} &= \begin{vmatrix}
            1 & a_1 & a_1^2 & \ldots & a_1^{n-1} \\
            \vdots & \vdots &  &  & \vdots \\
            1 & a_n & a_n^2 & \ldots & a_n^{n-1}
        \end{vmatrix} \\
        &= V(a_1,\ldots,a_n) \\
        & \downarrow \mathcal{P}(n) \\
        &= \prod\limits_{1 \leq i < j \leq n} (a_j - a_i)
    \end{align*}
    Or, on remarque que si $\exists (i,j) \in \intervalleEntier{1}{n+1}^2, i \neq j \text{ et } a_i = a_j$, alors $V(a_1,\ldots,a_{n+1}) = 0$. Donc $a_1,\ldots,a_n$ sont des racines de $P$, et celles-ci sont les seules car $P$ est de degré $n$. Ainsi, on peut écrire $P$ comme : 
    \[ \lilbox{myred}{$P(X) = \Delta_{n,n} \prod\limits_{i=1}^n (X - a_i)$} \]
    Si on évalue $P$ en $a_{n+1}$, on obtient alors 
    \begin{align*}
        P(a_{n+1}) &= \lilbox{myred}{$V(a_1,\ldots,a_{n+1})$} \\
        P(a_{n+1}) &= \Delta_{n,n} \prod\limits_{i=1}^n (a_{n+1} - a_i) \\
        &= \prod\limits_{1 \leq i < j \leq n} (a_j - a_i) \prod\limits_{i=1}^n (a_{n+1} - a_i) \\
        &= \lilbox{myred}{$\prod\limits_{1 \leq i < j \leq n+1} (a_j - a_i)$}
    \end{align*}
    On a donc le résultat voulu.
    \end{itemize}
    \end{demo}

    \subsubsection{Déterminant d’une matrice triangulaire par blocs}

    \begin{prop}{Déterminant d’une matrice triangulaire par blocs}{}
        Soit $M = \begin{bmatrix}
            A & B \\
            0 & C
        \end{bmatrix} \in \mk{n}$ et $A \in \mp(\mathbb{K})$. Alors $\det(M) = \det(A)\det(C)$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{itemize}
            \item Si $A$ n’est pas inversible, $\det(A) = 0$. Les colonnes de $A$ ne forment pas une famille libre, donc une de ses colonnes est CL des autres. Cette CL se retrouve dans les $p$ premières colonnes de $M$, donc $\det(M) = 0$, et la formule est vérifiée.
            \item Sinon, $M = \begin{bmatrix}
                A & B \\
                0 & C
            \end{bmatrix} = \begin{bmatrix}
                A & 0 \\
                0 & I_{n-p}
            \end{bmatrix} \begin{bmatrix}
                I_p & A^{-1} B \\
                0 & C 
            \end{bmatrix}$ et $\begin{vmatrix}
                P & Q \\
                0 & I
            \end{vmatrix} = \det(P)$ pour $P,Q$ quelconques, par développement, d’où $\det(M) = \det(A) \det(C)$.
        \end{itemize}
    \end{demo}

    \begin{prop}{Généralisation}{}
        Soit $M = \begin{bmatrix}
            A_1 & * & * \\
            0 & \ddots & * \\
            0 & 0 & A_p
        \end{bmatrix} \in \mk{n}$ où $A_1, \ldots,A_p$ sont carrées. Alors $\det(M) = \prod_{i = 1}^p \det(A_i)$.
    \end{prop}

    \begin{demo}{Idée}{myolive}
        Par récurrence sur le nombre de blocs $p$.
        \begin{itemize}
            \item[\textbf{I}] \quad Si $p = 2$, se ramener à la proposition précédente.
            \item[\textbf{H}] \quad Supposons l’ordre $p-1$. On pose $M = \begin{bmatrix}
                A_1 & * \\
                0 & B
            \end{bmatrix}$ où $B = \begin{bmatrix}
                A_2 & * & * \\
                0 & \ddots & * \\
                0 & 0 & A_p
            \end{bmatrix}$ à laquelle on applique l’hypothèse de récurrence.
        \end{itemize}
    \end{demo}

    \subsubsection{Déterminant de Cauchy}

    Soient $(a_1,\ldots,a_n)$ et $(b_1,\ldots,b_n)$ des éléments de $\mathbb{K}^n$ tels que $\forall i,j \in \intervalleEntier{1}{n}$, $a_i + b_j \neq 0$. On appelle \textbf{Matrice de Cauchy} la matrice $M_n = \left(\frac{1}{a_i + b_j}\right)_{i,j \in \intervalleEntier{1}{n}}$.

    \begin{prop}{}{}
        $\det(M_n) = \frac{\prod_{1 \leq i < j \leq n} (a_j - a_i)(b_j - b_i)}{\prod_{i,j \in \intervalleEntier{1}{n}} a_i + b_j}$ 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item \textbf{Formule de récurrence} \quad Notons $C_1,\ldots,C_n$ les colonnes de $M_n$. On applique à $M$ les opérations $C_j \leftarrow C_j - C_n$ pour $j \in \intervalleEntier{1}{n-1}$. Les coefficients $i,j$ avec $j \neq n$ obtenus sont alors de la forme $\frac{1}{a_i + b_j} - \frac{1}{a_i + b_n} = \frac{\lilbox{myolive}{$b_n - b_j$}}{(a_i + b_j)\lilbox{myorange}{$(a_i + b_n)$}}$ dont le numérateur ne dépend par de $i$ \textit{i.e.} de la ligne, et $a_i + b_n$ ne dépend par de $j$, \textit{i.e.} de la colonne. D’où la factorisation suivante : 
            \[ \det(M_n) = \frac{\prod_{j=1}^{n-1} (b_n - b_j)}{\prod_{i=1}^n (a_i + b_n)} \begin{vmatrix}
                \frac{1}{a_1 + b_1} & & \frac{1}{a_{1} + b_{n-1}} & 1 \\
                 & & & & \\
                 & & & & \\
                \frac{1}{a_{n} + b_1} & & \frac{1}{a_{n} + b_{n-1}} & 1
            \end{vmatrix} \]    
            En réalisant cette fois-ci les opérations $L_i \leftarrow L_i - L_n$ pour $i \in \intervalleEntier{1}{n-1}$, on obtient comme coefficients $(i,j)$ pour $i,j \in \intervalleEntier{1}{n-1}$, $ \frac{1}{a_i + b_j} - \frac{1}{a_n + b_j} = \frac{\lilbox{myolive}{$a_n - a_i$}}{(a_i + b_j)\lilbox{myorange}{$a_n + b_j$}}$. De la même façon que précédemment, par factorisation du numérateur sur la $j$-ème colonne, et de $(a_n - b_j)$ sur la $i$-ème ligne, on a donc 
            \[ \det(M_n) = \frac{\prod_{j=1}^{n-1} (b_n - b_j)}{\prod_{i=1}^n (a_i + b_n)} \frac{\prod_{i=1}^{n-1} (a_n - a_i)}{\prod_{j = 1}^{n-1} (a_n + b_j)} \begin{vmatrix}
                \frac{1}{a_1 + b_1} & & \frac{1}{a_{1} + b_{n-1}} & 0 \\
                 & & & & \\
                \frac{1}{a_{n-1} + b_1} & & \frac{1}{a_{n-1} + b_{n-1}} & 1 \\
                1 & & & 1 
            \end{vmatrix} = \frac{\prod_{j=1}^{n-1} (b_n - b_j)}{\prod_{i=1}^n (a_i + b_n)} \frac{\prod_{i=1}^{n-1} (a_n - a_i)}{\prod_{j = 1}^{n-1} (a_n + b_j)} \det(M_{n-1}) \]
            D’où la relation de récurrence $\det(M_{n}) = \frac{\prod_{j=1}^{n-1} (b_n - b_j)}{\prod_{i=1}^n (a_i + b_n)} \frac{\prod_{i=1}^{n-1} (a_n - a_i)}{\prod_{j = 1}^{n-1} (a_n + b_j)} \det(M_{n-1})$.
            \item \textbf{I} \quad Pour $n = 2$, 
            \begin{align*}
                \det(M_2) 
                &= \begin{vmatrix}
                    \frac{1}{a_1 + b_1} & \frac{1}{a_1 + b_2} \\
                    \frac{1}{a_2 + b_1} & \frac{1}{a_2 + b_2}
                \end{vmatrix} \\
                &= \frac{1}{(a_1 + b_1)(a_2 + b_2)} - \frac{1}{(a_2 + b_1)(a_1 + b_2)} \\
                &= \frac{a_1 b_1 + a_2 b_2 - a_1 b_2 - a_2 b_1}{\prod_{1 \leq i,j \leq 2} (a_i + b_j)} \\
                &= \frac{\prod_{(i,j) = (1,2)} (a_j - a_i)(b_j - b_i)}{\prod_{1 \leq i,j \leq 2} (a_i + b_j)}
            \end{align*}
            \item \textbf{H} \quad Supposons l’ordre $n - 1$. D’après la formule de récurrence, on a
            \begin{align*}
                \det(M_n) 
                &= \frac{\prod_{j=1}^{n-1} (b_n - b_j)}{\prod_{i=1}^n (a_i + b_n)} \frac{\prod_{i=1}^{n-1} (a_n - a_i)}{\prod_{j = 1}^{n-1} (a_n + b_j)} \det(M_{n-1}) \\
                &= \frac{\prod_{j=1}^{n-1} (b_n - b_j)}{\prod_{i=1}^n (a_i + b_n)} \frac{\prod_{i=1}^{n-1} (a_n - a_i)}{\prod_{j = 1}^{n-1} (a_n + b_j)} \frac{\prod_{1 \leq i < j \leq n-1} (a_j - a_i)(b_j - b_i)}{\prod_{i,j \in \intervalleEntier{1}{n-1}} a_i + b_j} \\
                &= \frac{\prod_{1 \leq i < j \leq n} (a_j - a_i)(b_j - b_i)}{\prod_{i,j \in \intervalleEntier{1}{n}} a_i + b_j}
            \end{align*}
        \end{itemize}
    \end{demo}

    \subsubsection{Formules de Cramer}

    Soit $(\mathcal{S})$ un système linéaire de $n$ équations à $n$ inconnues, que l’on écrit 
    \[ (\mathcal{S}) : \left\{ \begin{array}{l}
        a_{1,1} x_1 + \cdots + a_{1,n} x_n = b_1 \\
        \quad \vdots \\
        a_{n,1} x_1 + \cdots + a_{n,n} x_n = b_n
    \end{array} \right. \]    
    On peut écrire ce système sous \textbf{forme matricielle} $A X = B$. On suppose que $(\mathcal{S})$ est un \textbf{système de Cramer}. 

    \begin{prop}{Formules de Cramer}{}
        L’unique solution du système est donnée par $\forall i \in \intervalleEntier{1}{n}, x_i = \frac{\det(A_i)}{\det(A)}$, où $A_i$ est la matrice obtenue en remplaçant la $i$-ème colonne de $A$ par $B$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Notons $C_1, \ldots, C_n$ les colonnes de $A$ et $\mathcal{B}$ la base canonique de $\mk{n,1}$. Alors et \[ \det(A) = \det_{\mathcal{B}}(C_1,\ldots,C_n) \esp{et} \det(A_i) = \det_{\mathcal{B}}(C_1, \ldots, B, \ldots, C_n) \]
        Or $AX = B \iff x_1 C_1 + \cdots + x_n C_n = B$, donc 
        \begin{align*}
            \det(A_i) 
            &= \det_{\mathcal{B}}\left(C_1, \ldots, \sum_{j=1}^n x_j C_j, \ldots, C_n\right) \\
            &= x_i \det_{\mathcal{B}}\left(C_1, \ldots, C_i, \ldots, C_n\right) \\
            &= x_i \det(A)
        \end{align*}
        D’où $x_i = \frac{\det(A_i)}{\det(A)}$.
    \end{demo}

    Ces formules sont d’un intérêt bien plus théorique que pratique, car nécessite des calculs bien trop conséquents.

    \subsubsection{Comatrice d’une matrice carrée}

    \begin{defi}{Comatrice}{}
        Soit $A \in \mk{n}$. La \textbf{comatrice} de $A$ est $\com(A) = \left((-1)^{i + j} \det(\Delta_{i,j})\right)_{i,j \in \intervalleEntier{1}{n}}$ \textit{i.e.} la matrice des cofacteurs de $a_{i,j}$.
    \end{defi}

    \begin{prop}{}{}
        Pour tout $A \in \mk{n}$, on a 
        \[ \com(A)^{\top} A = \det(A) I_n \esp{et} A \com(A)^{\top} = \det(A) I_n \]
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Calculons le coefficient $(i,j) =: x_{i,j}$ de $\com(A)^{\top} A$.
        \begin{itemize}
            \item Si $i= j$, 
            \begin{align*}
                x_{i,j} 
                &= \sum_{k=1}^{n} (-1)^{i + k} \det(\Delta_{k,i}) a_{k,i} \\
                &\quad \downarrow \quad \text{dvp p/r à la } i\text{-ième colonne} \\
                &= \det(A) 
            \end{align*}
            \item Si $i \neq j$, 
            \begin{align*}
                x_{i,j} 
                &= \sum_{k=1}^{n} (-1)^{i + k} a_{k,j} \det(\Delta_{k,i}) \\
                &\quad \uparrow \quad \text{dvp p/r à la } i\text{-ème colonne} \\
                &= \det(B) \\
                &= 0 \\
            \end{align*}
            où $B$ est obtenue en remplaçant la $i$-ème par la $j$-ème colonne de $A$.
        \end{itemize}
    \end{demo}

    On retrouve la formule bien connue des terminales $A^{-1} = \frac{1}{\det(A)} \com(A)^{\top}$.

    \subsubsection{Matrice compagnon}

    \begin{defi}{Matrice compagnon}{}
        Soit $P \in \mathbb{K}[X]$ unitaire tel que $P = \sum_{k=0}^{n} a_k X^k$. On appelle \textbf{matrice compagnon} du polynôme $P$ la matrice 
        \[ C_P = \begin{bmatrix}
            0 & & & 0 & -a_0 \\
            1 & 0 & & & \\
            0 & 1 & & & \\
            & & & 0 & \\
            0 & & 0 & 1 & -a_{n-1}
        \end{bmatrix} \] 
    \end{defi}

    \begin{prop}{}{}
        Pour tout $x \mathbb{K}$, on a $P(x) = \det(x I_n - C_P)$
    \end{prop}

    \begin{demo}{Preuve \textcolor{black}{(Récurrence sur $n$)}}{myolive}
        \begin{itemize}
            \item \textbf{I} \quad Si $n = 1$, $P = a_0 + X$ et $C_P = (-a_0)$ d’où $det(x I_1 - C_p) = x + a_0 = P(x)$.
            \item \textbf{H} \quad Supposons l’ordre $n$ vrai. Soit $P = \sum_{k=0}^{n+1} a_k X^k$ unitaire et $x \in \mathbb{K}$. Alors 
            \begin{align*}
                \det(x I_{n+1} - C_p) 
                &= \begin{vmatrix}
                    x & & & 0 & a_0 \\
                    - 1 & x & & & \\
                    0 & - 1 & & & \\
                    & & & 0 & \\
                    0 & & 0 & - 1 & x + a_{n-1}
                \end{vmatrix} \\
                &\quad \downarrow \quad \text{dvp p/r à la 1ère ligne} \\
                &= x \begin{vmatrix}
                    x & & & 0 & a_1 \\
                    - 1 & x & & & \\
                    0 & - 1 & & & \\
                    & & & 0 & \\
                    0 & & 0 & -1 & x + a_{n-1}
                \end{vmatrix} + (-1)^{n+2} a_0 \begin{vmatrix}
                    -1 & & * \\
                    & & \\
                    0 & & -1
                \end{vmatrix} \\
                &\quad \downarrow \quad Q = a_1 + a_2 X + \ldots + X^n \\
                &= x \det(x I_n - C_Q) + a_0 \\
                &= x Q(x) + a_0 \\
                &= P(x)
            \end{align*}
        \end{itemize}
    \end{demo}

\section{Réduction}

\subsection{Éléments propres \& Polynômes caractéristiques}

    \subsubsection{Éléments propres}

    Soit $E$ un $\mathbb{K}$-ev et $f \in \mathcal{L}(E)$ un endomorphisme de $E$.

    \begin{defi}{Valeur propre et vecteur propre}{}
        Soit $\lambda \in \mathbb{K}$. 

        On dit que $\lambda$ est une \textbf{valeur propre} de $f$ s’il existe $v \in E$, non nul, tel que $f(v) = \lambda v$.

        On dit que $v$ est un \textbf{vecteur propre} de $f$ associé à la valeur propre $\lambda$.
    \end{defi}

    \begin{omed}{Notations}{myyellow}
        \begin{enumerate}[label=\textcolor{myyellow}{\arabic*.}]
            \item On note $\sp(f)$ l’ensemble des valeurs propres de $f$, i.e.
            \[ \sp(f) = \big\{ \lambda \in \mathbb{K}, \exists v \in E \backslash \{0\} , f(v) = \lambda v \big\} \]    
            \item Si $\lambda \in \sp(f)$ est une valeur propre de $f$, on note $E_{\lambda}(f)$ l’ensemble des vecteurs propres de $f$ associés à $\lambda$, 
            \[ E_{\lambda} (f) = \big\{ v \in E, f(v) = \lambda v \big\} = \ker(f - \lambda \id_E) \]
        \end{enumerate}
    \end{omed}

    \begin{prop}{}{}
        Pour tout $\lambda \in \sp(f)$, $E_{\lambda}(f)$ est un sous-espace vectoriel $E$ de dimension $\geq 1$.
    \end{prop}

    \begin{demo}{Preuve ?}{myolive}
        Comme $E_{\lambda}(f) = \ker(f - \lambda \id_E)$, c’est un sous-espace vectoriel comme noyau d’un endomorphisme. Et comme il existe $v \neq 0$ dans $E_{\lambda}(f)$, on a $\dim(E_{\lambda}(f)) \geq 1$.
    \end{demo}

    \begin{omed}{Exemples}{myolive}
        \begin{enumerate}[label=\arabic*.]
            \item Soit $f : (x,y) \mapsto (x + 2y, 3x + 2y)$. $\lambda$ est valeur propre de $f$ s’il existe $(x,y) \neq (0,0)$ tel que
            \begin{align*}
                \et{x + 2y = \lambda x}{3x + 2y = \lambda y}
                &\iff \begin{bmatrix}
                    1- \lambda & 2 \\
                    3 & 2 - \lambda
                \end{bmatrix} \begin{bmatrix}
                    x \\
                    y
                \end{bmatrix} = \begin{bmatrix}
                    0 \\
                    0
                \end{bmatrix} 
            \end{align*}
            Si $A = \begin{bmatrix}
                1- \lambda & 2 \\
                3 & 2 - \lambda
            \end{bmatrix}$ est inversible, alors $(0,0)$ est l’unique solution du système, et $\lambda$ n’est pas valeur propre. Or $\det(A) = -4 -3\lambda + \lambda^2 = (\lambda + 1)(\lambda - 4)$. Donc $\sp(f) \subset \left\{-1, 4\right\}$.
            \begin{itemize}
                \item Si $\lambda = -1$, le système donne $x = -y$, d’où $E_{-1}(f) = \vect\left\{(1,-1)\right\}$.
                \item Si $\lambda = 4$, le système donne $3x - 2y = 0$, d’où $E_{4}(f) = \vect\left\{(2,3)\right\}$.
            \end{itemize}
            Finalement, $-1$ et $4$ sont des valeurs propres de $f$ et $\sp(f) = \left\{-1,4\right\}$.
            \item Si $\fonction{\varphi}{\mathcal{C}^{\infty}(\mathbb{R},\mathbb{R})}{(\mathbb{R},\mathbb{R})}{f}{f'}$, $\lambda$ est une valeur propre de $\varphi$ s’il existe $f \neq 0$ tel que $f' = \lambda f$. $f \in \vect(e^{\lambda x})$ convient, donc $Sp(\varphi) = \mathbb{R}$ et $E_{\lambda}(\varphi) = \vect(e^{\lambda x})$.
        \end{enumerate}
    \end{omed}

    \begin{prop}{}{}
        Si $E$ est de dimension finie, alors $\lambda \in \sp(f)$ \textit{ssi} $\det(\lambda \id_E - f) = 0$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{align*}
            \lambda \in \sp(f) 
            &\iff \ker(f - \lambda \id_E) \neq \{0\} \\
            &\iff f - \lambda \id_E \textit{ n’est pas bijective}
        \end{align*}
    \end{demo}

    \begin{prop}{}{}
        \begin{enumerate}
            \item Des vecteurs propres, non nuls, associés à des valeurs propres distinctes sont linéairement indépendants. 
            \item Des sous-espaces propres associés à des valeurs propres distinctes sont en somme directe.
        \end{enumerate}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item Par récurrence sur $p$.
            \begin{itemize}
                \item \textbf{I} \quad Si $p = 1$, le résultat est clair.
                \item \textbf{H} \quad Supposons l’ordre $p$. Soient $p+1$ vecteurs propres non nuls, associés à des valeurs propres $\lambda_1,\ldots, \lambda_{p+1}$ distinctes. On suppose que $\alpha_1 v_1 + \cdots + \alpha_{p+1} v_{p+1} = 0 \eqlabel{1}$. En composant par $f$, on obtient $\alpha_1 \lambda_1 v_1 + \cdots + \alpha_{p+1} \lambda_{p+1} v_{p+1} = 0 \eqlabel{2}$. En réalisant $\lambda_{p+1} (1) - (2)$, on obtient 
                \[ \alpha_1 (\lambda_{p+1} - \lambda_1) v_1 + \cdots + \alpha_p (\lambda_{p+1} - \lambda_p) v_p = 0 \]    
                D’après $\mathcal{H}_p$, $\forall i \in \intervalleEntier{1}{p}, \alpha_i \left(\lambda_{p+1} - \lambda_i\right) = 0$ donc $\alpha_i = 0$. Finalement, l’équation $(1)$ donne $\alpha_{p+1} = 0$. Donc la famille est libre.
            \end{itemize}
            On peut aussi passer par une preuve directe, en itérant $p-1$ fois la fonction $f$, ce qui donne un système à $p$ équations et $p$ inconnues, de matrice caractéristique de type Vandermonde.
            \item Soient $\lambda_1, \ldots, \lambda_p \in \sp(f)$ distincts. Montrons que $E_{\lambda_1}(f) + \cdots + E_{\lambda_p}(f)$ sont en somme directe. Soit $x$ tel que $x = u_1 + \cdots + u_p = v_1 + \cdots + v_p$, où $u_i$ et $v_i$ sont des éléments $E_{\lambda_i}(f)$. On a alors 
            \[ (u_1 - v_1) + \cdots + (u_p - v_p) = 0 \]   
            Posons $I = \enstq{i \in \intervalleEntier{1}{p}}{u_i - v_i \neq 0}$. La famille $(u_i - v_i)_{i \in I}$ est une famille de vecteurs propres non nuls tels que $\sum_{i \in I} u_i - v_i = 0$. C’est absurde, donc $I = \emptyset$, et la somme est directe.
        \end{enumerate}
    \end{demo}

    \begin{coro}{}{}
        Si $\dim(E) = n$, alors $\card(\sp(f)) \leq n$.
    \end{coro}

    \begin{demo}{Preuve}{myorange}
        On sait qu’une famille de vecteurs propres, non nuls, associés à des valeurs propres distinctes est libre. D’autre part, une famille libre est de cardinal au plus $n$.
    \end{demo}

    \begin{prop}{}{}
        Soient $f,g \in \mathcal{L}(E)$ qui commutent.

        Alors les sous-espaces propres de $f$ sont stables par $g$ et inversement.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Si $x \in E_{\lambda}(f)$. $g(f(x)) = \lambda g(x) = f(g(x))$ donc $E_{\lambda}(f)$ est stable par $g$.
    \end{demo}

    \begin{defi}{}{}
        Soit $M \in \mk{n}$.

        On dit que $\lambda \in \mathbb{K}$ est une \textbf{valeur propre} de $M$ s’il existe une matrice colonne non nulle $X \in \mk{n,1} \backslash \{ 0 \}$ telle que $MX = \lambda X$. On dit que $X$ est un vecteur propre de $M$ associé à $\lambda$ si $MX = \lambda X$.
    \end{defi}

    \begin{omed}{Notation}{myyellow}
        \begin{enumerate}[label=\textcolor{myyellow}{\arabic*.}]
            \item On note $\sp(M)$ l’ensemble des valeurs propres de $M$, \textit{i.e.} 
            \[ \sp(M) = \big\{ \lambda \in \mathbb{K}, \exists X \in \mk{n,1} \backslash \{ 0 \}, MX = \lambda X \big\} \]
            \item Soit $\lambda \in \sp(M)$, on appelle sous-espace propre associé à $\lambda$ l’ensemble noté $E_{\lambda}(M)$ défini par 
            \[ E_{\lambda}(M) = \big\{ X \in \mk{n,1}, MX = \lambda X \big\} \]
        \end{enumerate}
    \end{omed}

    \begin{prop}{Transition}{}
        Soient $E$ un $\mathbb{K}$-ev de dimension finie et $\mathcal{B}$ une base de $E$. Si $f \in \mathcal{L}(E)$ et $M = \mat{\mathcal{B}}{f}$, alors $\sp(f) = \sp(M)$. En particulier, $\sp(M)$ ne dépend pas de la base choisie.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On sait que $\lambda \in \sp(f) \iff \det(f - \lambda \id) = 0$, \textit{i.e.} \textit{ssi} $\det(M - \lambda I_n) = 0$, \textit{cad} $\lambda \in \sp(M)$.
    \end{demo}

    Le spectre est ainsi un invariant de similitude.

    \begin{omed}{Exemples}{myolive}
        \begin{enumerate}[label=\arabic*.]
            \item Soit $M \in \mk{n}$ une matrice nilpotente. Une telle matrice n’est pas inversible, d’où $\ker(M) \neq 0$. Il existe $X \neq 0$ tel que $M X = 0$, donc $0$ est une valeur propre de $M$. Si $\lambda \in \sp(M) \neq 0$, alors il existe $Y \neq 0$ telle que $MY = \lambda Y$. On considère $n = \min(k \in \mathbb{N}, M^k X = 0)$ Donc $M^n Y = \lambda^n Y = 0$, d’où $\lambda^n = 0$ puis $\lambda = 0$.
            \item Si $M = \begin{bmatrix}
                0 & -I_2 \\
                I_2 & 0
            \end{bmatrix} \in \mk{4}$. Soit $X = \begin{bmatrix}
                x \\
                y \\
                z \\
                t
            \end{bmatrix}$, $\lambda$ est valeur propre \textit{ssi} $MX = \lambda X$, donc \textit{ssi} 
            \[et{t = -\lambda y}{y = -\lambda^2 y} \esp{et} \et{z = -\lambda x}{x = - \lambda^2 x}\]
            \begin{itemize}
                \item Si $x = 0$, alors $z = 0$ et $y \neq 0$ d’où $1 = - \lambda^2$.
                \item Si $x \neq 0$, on peut simplifier par $x$, d’où $1 = - \lambda^2$ de même.
            \end{itemize}
            Ainsi, $\sp_{\mathbb{R}}(M) = \emptyset$, et $\sp_{\mathbb{C}}(M) \subset \left\{-i, i\right\}$.
            \begin{itemize}
                \item Si $\lambda = i$, alors $\et{z = -i x}{t = -iy}$, donc $E_{i}(M) = \vect\left(\begin{bmatrix}
                    1 \\
                    0 \\
                    -i \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    1 \\
                    0 \\
                    -i
                \end{bmatrix}\right)$
                \item Si $\lambda = -i$, alors $\et{z = i x}{t = iy}$ donc $E_{-i}(M) = \vect\left(\begin{bmatrix}
                    1 \\
                    0 \\
                    i \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    0 \\ 
                    1 \\
                    0 \\
                    -i
                \end{bmatrix}\right)$ 
            \end{itemize}
            \item Posons $M = \begin{bmatrix}
                1 & 1 & & 0 \\
                 & \ddots & \ddots & \\
                 & & & 1 \\
                0 & & & 1
            \end{bmatrix} \in \mk{n}$. $\lambda$ est valeur propre de $M$ \textit{ssi} il existe $X \neq 0$ tel que $MX = \lambda X$. En posant $X = \begin{bmatrix}
                x_1 \\
                \vdots \\
                x_n
            \end{bmatrix}$, on a 
            \[ \left\{ \begin{array}{l}
                x_1 + x_2 = \lambda x_1 \\
                \quad \vdots \\
                x_{n-1} + x_n = \lambda x_{n-1} \\
                x_n = \lambda x_n
            \end{array}\right. \]
            \begin{itemize}
                \item Si $x_n \neq 0$, alors $\lambda = 1$, puis en remplaçant dans le système, on obtient $x_n = 0$, absurde.
                \item Donc $x_n = 0$, et dans ce cas on repète le procédé pour $x_{n-1}$. 
                \item Il reste finalement $x_1 = \lambda x_1$. Si $x_1 = 0$, $X = 0$ donc $\lambda$ n’est pas valeur propre. Si $x_1 \neq 0$, $\lambda = 1$.
            \end{itemize}
            Donc $\sp(M) = \left\{1\right\}$ et $E_{1}(M) = \vect\left(\begin{bmatrix}
                1 \\
                \vdots \\
                0
            \end{bmatrix}\right)$
        \end{enumerate}
    \end{omed}

    \subsubsection{Polynômes caractéristiques}

    Soit $E$ un $\mathbb{K}$-ev de dimension finie avec $\dim(E) = n$.

    \begin{defitheo}{Polynôme caractéristique}{}
        Soit $f \in \mathcal{L}(E)$ ou $M \in \mk{n}$.

        On appelle polynôme caractéristique de $f$ et de $M$ le polynôme noté $\chi_f$ ou $\chi_M$ défini par 
        \[ \forall \lambda \in \mathbb{K}, \ou{\chi_f(\lambda) = \det(\lambda \id_e - f)}{\chi_M(\lambda) = \det(\lambda \id_e - f)} \]
        Ce polynôme est de la forme 
        \[ \lambda^n - \tr(M)\lambda^{n-1} + \cdots + (-1) \det(M) \] 
    \end{defitheo}

    \begin{demo}{Justification}{mypurple}
        Plaçons nous dans le cas matriciel, avec $M = (m_{i,j})$.
        \begin{align*}
            \chi_M
            &=\det(\lambda I_n - M) \\
            &= \sum_{\sigma \in \mathfrak{S}} \varepsilon(\sigma) \prod_{i=1}^n \left(\lambda \delta_{\sigma(i),i} - m_{\sigma(i),i}\right) \\
        \end{align*}
        Le coefficient de plus haut degré (au maximum $n$) est obtenu seulement pour $\sigma = \id$, donc ce polynôme est unitaire. De plus, on ne peut obtenir de coefficient de degré $n-1$ autrement qu’avec $\sigma = \id$, car il ne peut y avoir un unique entier $i \in \intervalleEntier{1}{n}$ tel que $\sigma(i) \neq i$ pour $\sigma \in \mathfrak{S}$. Ainsi, le coefficient d’ordre $X^{n-1}$ est $-\tr(M)$. Finalement, pour obtenir un terme constant, on calcule la valeur en $0$, et on obtient directement que ce coefficient est $\sum_{\sigma \in \mathfrak{S}} \varepsilon(\sigma) \prod_{i=1}^n (-1)^n m_{\sigma(i),i} = (-1)^n \det(M)$. 
    \end{demo}

    \begin{omed}{Exemple}{mypurple}
        Si $M = \begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}$, alors $\chi_M(\lambda) = X^2 - (a+d)X + (ad - bc)$
    \end{omed}

    \begin{prop}{}{}
        Les valeurs propres sont les racines du polynôme caractéristique. De plus, $\chi_M$ est un invariant de similitude, \textit{i.e.} si $M = \mat{\mathcal{B}}{f}$, alors $\chi_M = \chi_f$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{align*}
            \lambda \in \sp(M) 
            &\iff \ker(\lambda I_n - M) \neq \left\{0\right\} \\
            &\iff \lambda I_n - M \text{n’est pas inversible} \\
            &\iff \det(\lambda I_n - M) = 0 \\
            &\iff \chi_M(\lambda) = 0
        \end{align*}
        De plus, $\chi_f(\lambda) = \det(\lambda \id -f) = \det(\mat{\mathcal{B}}{\lambda \id - f}) = \det(\lambda I_n - M) = \chi_M$.
    \end{demo}

    \begin{omed}{Exemples}{myolive}
        \begin{enumerate}[label=\arabic*]
            \item  Soit $M = \begin{bmatrix}
                1 & 1 & & 0 \\
                 & \ddots & \ddots & \\
                 & & & 1 \\
                0 & & & 1
            \end{bmatrix} \in \mk{n}$. Pour $\lambda \in \mathbb{K}$, on a $\chi_M(\lambda) = \det(\lambda I_n - M) = (\lambda - 1)^n$.
            \item Si $M = \begin{bmatrix}
                0 & -I_2 \\
                I_2 & 0
            \end{bmatrix}$, alors $\chi_M = \det(\lambda I_4 - M) = \begin{vmatrix}
                \lambda I_2 & I_2 \\
                - I_2 & \lambda I_2
            \end{vmatrix}$. Or $I_2$ et $\lambda I_2$ commutent, donc $\begin{vmatrix}
                \lambda I_2 & I_2 \\
                - I_2 & \lambda I_2
            \end{vmatrix} = \det(\lambda^2 I_2 + I_2) = \left(\lambda^2 + 1\right)^2$
        \end{enumerate}
    \end{omed}

    \begin{defi}{Multiplicité d’une valeur propre}{}
        Soit $\lambda \in \sp(f)$ (ou $\lambda \in \sp(M))$. On appelle \textbf{multiplicité} de $\lambda$ sa multiplicité comme racine du polynôme caractéristique. On la note $m_{\lambda}$.
    \end{defi}

    \begin{prop}{}{}
        Si $M \in \mathcal{M}_n(\mathbb{C})$, alors $\chi_M(X) = \prod_{\lambda \in \sp(M)} (X - \lambda)^{m_{\lambda}}$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        $\chi_M$ est scindé dans $\mathbb{C}$ d’après le théorème de d’Alembert et unitaire, de racines $\lambda$ qui sont de multiplicité $m_{\lambda}$.
    \end{demo}

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$ (ou $M \in \mk{n})$.

        Pour tout $\lambda \in \sp(f)$ (ou $\lambda \in \sp(M)$), on a 
        \[ 1 \leq \dim(E_{\lambda}(f)) \leq m_{\lambda} \quad \text{ou} \quad 1 \leq \dim(E_{\lambda}(M)) \leq m_{\lambda} \]
    \end{prop}

    \begin{lem}{}{}
        Soit $f \in \mathcal{L}(E)$ et $F$ un sev de $E$ stable par $f$. Alors $\chi_{\restr{f}{F}} \vbar \chi_{f}$.
    \end{lem}
    
    \begin{demo}{Démonstration du lemme}{mybrown}
        Soit $\mathcal{B}_F = (e_1,\ldots,e_p)$ une base de $F$, que l’on complète en $\mathcal{B}_E = (e_1,\ldots, e_n)$ une base de $E$. On peut écrire $\mat{\mathcal{B}_E}{f} = \begin{bmatrix}
            \mat{\mathcal{B}_F}{f} & B \\
            0 & C
        \end{bmatrix}$. Le polynôme caractéristique de $f$ est 
        \begin{align*}
            \chi_f(\lambda)
            &= \det(\lambda \id - f) \\
            &= \det(\lambda I_n - \mat{\mathcal{B}_E}{f}) \\
            &= \begin{vmatrix}
                \lambda I_p - \mat{\mathcal{B}_F}{f} & - B \\
                0 & \lambda I_{n-p} C 
            \end{vmatrix} \\
            &\quad \downarrow \quad \text{Matrice triangulaire par blocs} \\
            &= \det(\lambda I_p - \mat{\mathcal{B}_F}{f}) \det(\lambda I_{n-p} - C) \\
            &= \chi_{\restr{f}{F}}(\lambda) Q(\lambda)
        \end{align*}
    \end{demo}

    \begin{demo}{Preuve}{myolive}
        Pour $\lambda \in \sp(f)$, $E_{\lambda}(f)$ est stable par $f$, donc $\chi_{E_{\lambda}(f)}$. Or $f(x) = \lambda x$ pour $x \in E_{\lambda}(f)$, donc $\restr{f}{E_{\lambda}(f)} = \lambda \id$. Donc $\chi_{\restr{f}{E_{\lambda}(f)} = \lambda \id} = (X - \lambda)^{\dim(E_{\lambda}(f))}$. Or d’après le lemme, $(X - \lambda)^{\dim(E_{\lambda}(f))} \vbar \chi_{f}$, d’où $\dim(E_{\lambda}(f)) \leq m_{\lambda}$.
    \end{demo}

    \begin{omed}{Exemple}{myolive}
        Pour $M = \begin{bmatrix}
                1 & 1 & & 0 \\
                 & \ddots & \ddots & \\
                 & & & 1 \\
                0 & & & 1
            \end{bmatrix} \in \mk{n}$, on a vu que $\sp(M) = \left\{1\right\}$ et $E_1(M) = \vect\begin{bmatrix}
                1 \\
                0 \\
                \vdots \\
                0
            \end{bmatrix}$ et $\chi_M = (x-1)^n$, d’où $1 = \dim(E_1(M)) \leq n = m_1$.
    \end{omed}

    \begin{coro}{}{}
        Si $\lambda \in \sp(f)$ est une valeur propre de multiplicité $1$, \textit{i.e.} $m_{\lambda} = 1$, alors le sous-espace propre associé est de dimension $1$. Ainsi, $E_{\lambda}(f)$ est une droite vectorielle.
    \end{coro}

    \begin{omed}{Application \textcolor{black}{(Lien avec le pôlynôme caractéristique de l’inverse)}}{myorange}
        Soit $M \in \GL_n(\mathbb{K})$. 
        \begin{align*}
            \chi_M(X) 
            &= \det(X I_n - M) \\
            &= \det(M)\det(X M^{-1} - I_n) \\ 
            &\quad \downarrow \quad X \neq 0 \\
            &= (- X)^n \det(M) \det(\frac{1}{X} I_n - M^{-1}) \\
            &= (- X)^n \det(M) \chi_{M^{-1}}\left(\frac{1}{X}\right)
        \end{align*}
    \end{omed}

    \begin{prop}{}{}
        Soient $A,B \in \mk{n}$. $\chi_{AB} = \chi_{BA}$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On remarque que 
        \begin{align*}
            \begin{bmatrix}
                X I_n - AB & A \\
                0 & X I_n 
            \end{bmatrix} \begin{bmatrix}
                I_n & 0 \\
                B & I_n
            \end{bmatrix} &= \begin{bmatrix}
                X I_n & A \\
                X B & X I_n
            \end{bmatrix} \eqlabel{1} \\
            \begin{bmatrix}
                I_n & 0 \\
                B & I_n
            \end{bmatrix} \begin{bmatrix}
                X I_n & A \\
                0 & X I_n - BA 
            \end{bmatrix} &= \begin{bmatrix}
                X I_n & A \\
                X B & X I_n
            \end{bmatrix} \eqlabel{2}
        \end{align*}
        En égalisant (1) et (2), on obtient $\chi_{AB}(X) X^n = X^n \chi_{BA}(X)$. Ces deux polynômes coïncident sur un infinité de racines, donc sont égaux.
    \end{demo}

\subsection{Diagonalisation}

    \subsubsection{Définition}

    Soit $E$ un $\mathbb{K}$-ev de dimension finie, avec $\dim(E) = n$.

    \begin{defi}{}{}
        \begin{enumerate}[label= \textcolor{myyellow}{\arabic*.}]
            \item On dit que $f \in \mathcal{L}(E)$ est \textbf{diagonalisable} s’il existe une base $\mathcal{B}$ de $E$ telle que $M_{\mathcal{B}}(f)$ est diagonale.
            \item Soit $M \in \mk{n}$, on dit que $M$ est \textbf{diagonalisable} s’il existe une matrice diagonale semblable à $M$, \textit{i.e.} s’il existe $P$ inversible et $D$ diagonale telles que $M = P D P^{-1}$.
        \end{enumerate}
    \end{defi}

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$.

        Alors $f$ est diagonalisable \textit{ssi} il existe une base de $E$ formée de vecteurs propres de $f$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item[$\implies$] Si $f$ est diagonalisable, il existe $\mathcal{B} = (e_1,\ldots,e_n)$ une base de $E$ telle que $\mat{\mathcal{B}}{f}$ est diagonale. Ainsi, pour tout $i \in \intervalleEntier{1}{n}, f(e_i) = \lambda_i e_i$ où $e_i \neq 0$. Les $\lambda_i$ sont donc des valeurs propres associées aux vecteurs $e_i$ d’où $\mathcal{B}$ est une base formée de vecteurs propres.
            \item[$\impliedby$] Si $\mathcal{B} = (e_1,\ldots,e_n)$ est une base de vecteurs propres, alors $\mat{\mathcal{B}}{f}$ est une matrice diagonale.
        \end{itemize}
    \end{demo}

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$ et $\mathcal{B}$ une base de $E$ telle que $M_{\mathcal{B}}(f) = \diag(\lambda_1, \ldots, \lambda_n) = D$.

        Alors \[ \sp(f) = \big\{ \lambda_1,\ldots,\lambda_n \big\} \]
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On sait que $\chi_f(X) = \chi_D(X) = \prod_{i = 1}^n (X - \lambda_i)$. Les valeurs propres sont racines de $\chi_f$, d’où $\sp(f) = \left\{\lambda_1, \ldots, \lambda_n\right\}$.
    \end{demo}

    Il est essentiel ici de remarquer que les $\lambda$ ne sont pas nécessairement distincts. Cela ne signifie par ici que $\card(\sp(f)) = n$, ce qui est (on le verra plus tard) un critère de diagonalisabilité.

    \begin{omed}{Exemple}{myolive}
        Soit $M = \begin{bmatrix}
            0 & -I_2 \\
            I_2 & 0
        \end{bmatrix}$. $\chi_M(X) = (X^2 + 1)^2$.
        \begin{itemize}
            \item $M$ n’a pas de valeurs propres dans $\mathbb{R}$, donc n’est pas diagonalisable dans $\mathbb{R}$.
            \item $\chi_M(X) = (X-i)^2 (X+i)^2$ donc $\sp_{\mathbb{C}}(M) = \left\{-i, i\right\}$. De plus, on a vu que $E_{-i}(M) = \vect\left(\begin{bmatrix}
                    1 \\
                    0 \\
                    i \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    0 \\ 
                    1 \\
                    0 \\
                    -i
                \end{bmatrix}\right) =: \vect(e_1, e_2)$ et $E_{i}(M) = \vect\left(\begin{bmatrix}
                    1 \\
                    0 \\
                    -i \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    1 \\
                    0 \\
                    -i
                \end{bmatrix}\right) =: \vect(e_3, e_4)$. Soit $f$ cannoniquement associée à $M$. Alors $\mathcal{B} = (e_1, e_2, e_3, e_4)$ est une base de vecteurs propres de $f$, donc $f$ et $M$ sont diagonalisables dans $\mathbb{C}$.
                
                Pour diagonaliser $M$, on pose $P = \begin{bmatrix}
                    1 & 0 & 1 & 0 \\
                    0 & 1 & 0 & 1 \\
                    -i & 0 & i & 0 \\
                    0 & -i & 0 & i
                \end{bmatrix}$ et $D = (i, i, -i, -i)$, ce qui donne $M = P D P^{-1}$.
        \end{itemize}
    \end{omed}

    \begin{omed}{Exemple}{myolive}
        Soit $M = \begin{bmatrix}
            1 & 1 & 0 & \cdots & 0 \\
            0 & \ddots & \ddots & & \vdots \\
            \vdots & & & & 0 \\
            & & & & 1 \\
            0 & & & 0 & 1
        \end{bmatrix}$. On sait que $\chi_M(X) = (X-1)^n$, donc $\sp(M) = \left\{1\right\}$. Si $M$ est diagonalisable, $M$ est semblable à une matrice diagonale formée de ses valeurs propres. Comme l’unique valeur propre de $M$ est $1$, on aurait donc $M$ semblable à $I_n$ \textit{i.e.} $M = I_n$. C’est absurde, donc $M$ n’est pas diagonalisable.
    \end{omed}

    \subsubsection{Critères de diagonalisabilité}

    On souhaite dans cette sous-section donner diﬀérents critères permettant d’établir le caractère diagonalisable d’une matrice ou d’un endomorphisme. Les résultats établis dans cette partie, sont énoncés pour des endomorphismes de E, on laisse au lecteur le soin de les traduire en termes matriciels.

    \begin{prop}{Condition suffisante de diagonalisabilité}{}
        Soit $f \in \mathcal{L}(E)$.

        Si $f$ possède $n$ valeurs propres de multiplicité $1$, alors $f$ est diagonalisable.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Posons $\sp(f) = \left\{\lambda_1,\ldots,\lambda_n\right\}$ et pour $i \in \intervalleEntier{1}{n}$, $e_i \neq 0$ un vecteur propre associé à $\lambda_i$. Alors $(e_1,\ldots,e_n)$ est libre car formée de vecteurs propres associés à des valeurs propres distinctes. Or elle est de cardinal $n$, donc est une base de $E$, et donc $f$ est diagonalisable. 
    \end{demo}

    \begin{prop}{}{}
        \begin{soient}
            \item $f \in \mathcal{L}(E)$
            \item $\sp(f) = \big\{ \lambda_1,\ldots,\lambda_p \big\}$ où les $\lambda_i$ sont distincts
        \end{soient}
        Alors les affirmations suivantes sont équivalentes :
        \begin{enumerate}
            \item $f$ est diagonalisable.
            \item $\sum_{i=1}^{p} \dim(E_{\lambda_i}(f)) = n$
            \item $E = \sum_{i=1}^{p} E_{\lambda_i}(f)$
        \end{enumerate}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{itemize}[leftmargin=2cm]
            \item[\textbf{(iii)} $\iff$ \textbf{(ii)}] On sait que $E_{\lambda_1}(f), \ldots, E_{\lambda_p}(f)$ sont en somme directe, donc $\dim\left(\sum_{i=1}^{p} E_{\lambda_i}(f)\right) = \sum_{i=1}^{p} \dim(E_{\lambda_i}(f))$. Or $\sum_{i=1}^{p} E_{\lambda_i}(f) \subset E$, donc $\sum_{i=1}^{p} E_{\lambda_i}(f) = E$.
            \item[\textbf{(i)} $\implies$ \textbf{(iii)}] Si $f$ est diagonalisable, il existe une base $\mathcal{B}$ formée des vecteurs propres de $f$. Donc $\forall k \in \intervalleEntier{1}{n}$, $e_k \in \sum_{i=1}^{p} E_{\lambda_{\lambda_i}(f)}$. Donc $\sum_{i=1}^{p} E_{\lambda_i}(f)$ contient une base de $E$ donc $\sum_{i=1}^{p} E_{\lambda_i}(f) = E$.
            \item[\textbf{(iii)} $\implies$ \textbf{(i)}] Si $\sum_{i=1}^{p} E_{\lambda_i}(f) = E$, alors les $E_{\lambda_i}(f)$ sont supplémentaires dans $E$. Soit $\mathcal{B}$ une base adaptée à cette décomposition, alors $\mathcal{B}$ est formée de vecteurs propres de $E$. Donc $f$ est diagonalisable. 
        \end{itemize}
    \end{demo}

    \begin{theo}{Critère de diagonalisabilité}{}
        Soit $f \in \mathcal{L}(E)$.

        $f$ est diagonalisable \textit{ssi} 
        \[ \et{\chi_f \text{ est scindé sur } \mathbb{K}}{\text{Pour tout } \lambda \in \sp(f), \dim(E_{\lambda}(f)) = m_{\lambda}} \]
    \end{theo}

    \begin{demo}{Preuve}{myred}
        \begin{itemize}
            \item[$\impliedby$] Si $\chi_f$ est scindé, alors $\chi_f(X) = \prod_{\lambda \in \sp(f)} (X-\lambda)^{m_{\lambda}(f)}$ est un polynôme de degré $n$, donc $\sum_{\lambda \in \sp(\lambda)} m_{\lambda}(f) = n$. Donc $\sum_{\lambda \in \sp(f)} \det(E_{\lambda}(f)) = n$, et $f$ est diagonalisable par la proposition précédente.
            \item[$\implies$] Supposons $f$ diagonalisable. Alors $f$ est semblable à $D = \diag(\lambda_1,\ldots,\lambda_p)$ où les $\lambda_i$ apparaissent autant de fois que leur multiplicité $m_{\lambda_i}(f)$. Donc $\chi_f(X) = \prod_{\lambda \in \sp(f)} (X - \lambda)^{m_{\lambda}(f)}$. Donc le polynôme caractéristique est scindé. Or $\sum_{\lambda \in \sp(f)} \dim(E_{\lambda}(f)) = n$ car $f$ est diagonalisable, et $\sum_{\lambda \in \sp(f)} m_{\lambda}(f) = n$ d’après la forme du polynôme caractéristique, et $\dim(E_{\lambda}(f)) \leq m_{\lambda}(f)$ pour $\lambda \in \sp(f)$, donc $m_{\lambda}(f) = \dim(E_{\lambda}(f))$ pour $\lambda \in \sp(f)$.
        \end{itemize}
    \end{demo}

    \begin{coro}{Interprétation matricielle}{}
        Soit $M \in \mk{n}$.

        $M$ est diagonalisable \textit{ssi}
        \[ \et{\chi_M \text{ est scindé sur } \mathbb{K}}{\text{Pour tout } \lambda \in \sp(f), \rg(\lambda I_n - M) = n - m_{\lambda}} \]
    \end{coro}

    \begin{omed}{Exemple}{myorange}
        Soit $M = \begin{bmatrix}
            2 & 5 & & & \\
            & 2 & & & \\
            & & 4 & 2 & \\
            & & 3 & 5 & \\
            & & & & 7 
        \end{bmatrix}$
        \begin{itemize}
            \item La matrice est diagonale par blocs, donc 
            \begin{align*}
                \det(X I_5 - M) 
                &= (X - 7) \begin{vmatrix}
                    X - 2 & 5 \\
                    0 & X - 2 
                \end{vmatrix} \begin{vmatrix}
                    X- 4 & 2 \\
                    3 & X - 5
                \end{vmatrix} \\
                &= (X - 7) (X - 2)^2 (X^2 - 9 X - 6) \\
                &= (X-7)^2 (X-2)^3
            \end{align*}
            Le polynôme caractéristique est scindé, et $\sp(M) = \left\{2,7\right\}$, avec $m_2 = 3$ et $m_7 = 2$.
            \item Si $\lambda = 7$,
            \begin{align*}
                \rg(7 I_5 - M )
                &= \rg\begin{bmatrix}
                    5 & -5 & & & \\
                     & 5 & & & \\
                    & & 3 & -2 & \\
                    & & -3 & 2 & \\
                    & & & & 
                \end{bmatrix} \\
                &= \rg\begin{bmatrix}
                    1 &  & & & \\
                     & 1 & & & \\
                    & &  & 1 & \\
                    & &  &  & \\
                    & & & & 
                \end{bmatrix} \\
                &= 3 = 5 - 2
            \end{align*}
            
            Si $\lambda = 2$, 
            \begin{align*}
                \rg(2 I_5 - M) 
                &= \rg \begin{bmatrix}
                    & -5 & & & \\
                    &  & & & \\
                    & & -2 & -2 & \\
                    & & -3 & -3 & \\
                    & & & & 
                \end{bmatrix} \\
                &= \rg \begin{bmatrix}
                    & 1 & & & \\
                    &  & & & \\
                    & & 1 &  & \\
                    & & 1 & & \\
                    & & & & 
                \end{bmatrix} \\
                &= 2 = 5 - 3
            \end{align*}
            \item Finalement, $M$ est diagonalisable.
        \end{itemize}
    \end{omed}

    \begin{omed}{Exemple}{myorange}
        Soit $M = \begin{bmatrix}
            a & b & & b \\
            b & \ddots & &  \\
            & & & b \\
            b & & b & a
        \end{bmatrix} \in \mk{n}$ telle que $b \neq 0$.
        \begin{itemize}
            \item \begin{align*}
                \det(X I_n - M) 
                &\quad \downarrow \quad C_1 \leftarrow C_1 + \cdots + C_n \\
                &= \begin{vmatrix}
                    X - a - (n-1)b & -b & & & -b \\
                    \vdots & X - a & & & \\
                    & -b & & &  \\
                    &  & & & - b \\
                    X- a - (n-1)b & -b & & -b & X - a \\
                \end{vmatrix} \\
                &= (X - a - (n-1)b) \begin{vmatrix}
                    1 & -b & & & -b \\
                    \vdots & X - a & & & \\
                    & -b & & &  \\
                    &  & & & - b \\
                    1 & -b & & -b & X - a \\
                \end{vmatrix} \\
                & \quad \downarrow \quad C_j \leftarrow C_j + b C_1, j \neq 1 \\
                & (X -a - (n-1)b) \begin{vmatrix}
                    1 & 0 & & & 0 \\
                    \vdots & X - a + b & & & \\
                    & 0 & & &  \\
                    &  & & & 0 \\
                    1 & 0 & & 0 & X - a  + b\\
                \end{vmatrix} \\ 
                &= (X - a - (n-1)b) (X - a + b)^{n-1}
            \end{align*}
            $\chi_M(X)$ est scindé, donc $\sp(M) = \left\{a + (n-1)b, a - b\right\}$ où $m_{a + (n-1)b} = 1$ et $m_{a-b} = n-1$. 
            \item Si $\lambda = a + (n-1)b$, on a directement $\dim(E_{\lambda}(M)) = 1$.
            
            Si $\lambda = a-b$, on a $\lambda I_n - M = - b J$ donc $\rg(\lambda I_n - M) = 1$ et $\dim(E_{\lambda}(M)) = n - 1 = m_{\lambda}(M)$.
            \item La matrice $M$ est diagonalisable, et nous allons donc la diagonaliser.
            
            Si $\lambda = a + (n-1) b$, $E_{\lambda}(M) = \ker((a + (n-1)b) I_n - M) = \vect\begin{bmatrix}
                1 \\
                \vdots \\
                1
            \end{bmatrix}$.

            Si $\lambda = a-b$, $\lambda I_n - M = -b J$ donc $\ker(\lambda I_n - M ) = \vect\begin{bmatrix}
                1 & 
                -1 &
                0 &
                \vdots &
                0
            \end{bmatrix}, \begin{bmatrix}
                0 & 
                1 & 
                -1 &
                0 &
                \vdots
            \end{bmatrix}, \ldots, \begin{bmatrix}
                0 & 
                \vdots & 
                0 & 
                1 &
                -1
            \end{bmatrix}$. On peut donc former une base $mathcal{B}'$ constituée de vecteurs propres de $M$. Soit $P$ la matrice de passage de la base canonique à $\mathcal{B}'$, alors $P = \begin{bmatrix}
                1 & 1 & 0 & & 0 \\
                & -1 & 1 & & \\
                \vdots & 0 & -1 &  & 0 \\
                & & & & 1 \\
                1 & 0 & & 0 & -1
            \end{bmatrix}$, et donc $M = P \diag(a + (n-1)b, a - b, \ldots, a-b) P^{-1}$.
        \end{itemize}
    \end{omed}

\begin{exo}{Diagonalisation avec un paramètre}{}
    Soit $f \in \mathcal{L}(\mathbb{R}^3)$ de matrice dans la base canonique $A = \begin{bmatrix}
        1 & 0 & 1 \\
        -1 & 2 & 1 \\
        2-m & m-2 & m
    \end{bmatrix}$ où $m \in \mathbb{R}$.
    \begin{enumerate}
        \item Quelles sont les valeurs propres de $f$ ?
        \item Pour quelles valeurs de $m$ $f$ est-il diagonalisable ?
        \item Pour $m = 2$, calculer $A^k$ pour tout $k \in \mathbb{N}$.
    \end{enumerate}
\end{exo}

\subsection{Polynôme de matrice ou d’endomorphisme} 

    \subsubsection{Définition et propriétés}

    \begin{defi}{Polynômes d’endomorphisme}{}
        Soit $P = a_0 + a_1 X + \ldots + a_k X^k \in \mathbb{K}[X]$ et $f \in \mathcal{L}(E)$.

        On pose 
        \[ P(f) = a_0 \id_E + a_1 f + \ldots + a_k f^k = \sum_{i=0}^k a_i f^i \]
    \end{defi}

    \begin{defi}{Polynôme de matrice carrée}{}
        Si $M \in \mk{n}$, on pose 
        \[ P(M) = \sum_{i=0}^{k} a_i M^i \]
    \end{defi}

    \begin{prop}{}{}
        Soient $f \in \mathcal{L}(E)$, $P,Q \in \mathbb{K}[X]$ et $\lambda \in \mathbb{K}$.
        \begin{alors}
            \item $(P+Q)(f) = P(f) + Q(f)$
            \item $(\lambda P)(f) = \lambda P(f)$
            \item $PQ(f) = P(f) \circ Q(f)$
        \end{alors}
    \end{prop}

    \begin{demo}{Preuves}{myolive}
        Posons $P(X) = a_0 + a_1 X + \cdots + a_k X^k$ et $Q(X) = b_0 + b_1 X + \cdots + b_{k'} X^{k'}$, où on peut avoir des coefficients nuls. Soit $f \in \mathcal{L}(E)$. Montrons le \textbf{(iii)} :
        \begin{align*}
            PQ 
            &= \sum_{i=0}^{k + k'} \left( \sum_{j = \max(0, i-k')}^{\min(k, i-k')} a_j b_{i - j} \right)X^i \\
            &= \sum_{i=0}^{k + k'} \left( \sum_{j_1 + j_2 = i} a_{j_1} b_{j_2} \right)X^i \\
            PQ(f) 
            &= \sum_{i=0}^{k + k'} \left(\sum_{j_1 + j_2 = i} a_{j_1} b_{j_2}\right) f^i \\
            &= \sum_{i=0}^{k + k'} \left(\sum_{j_1 + j_2 = i} a_{j_1} b_{j_2} f^{j_1 + j_2}\right) \\
            &= \sum_{i=0}^{k + k'} \left(\sum_{j_1 + j_2 = i} a_{j_1} f^{j_1} \circ b_{j_2} f^{j_2}\right) \\
            &= \sum_{j_1 = 0}^{k} a_{j_1} f^{j_1} \circ \sum_{j_2 = 0}^{k'} b_{j_2} f^{j_2} \\
            &= P(f) \circ Q(f) \\
        \end{align*}
    \end{demo}

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$ et $\mathcal{B}$ une base de $E$.

        Alors \[ M_{\mathcal{B}}(P(f)) = P(M_{\mathcal{B}}(f)) \]
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On sait que $f \mapsto \mat{\mathcal{B}}{f}$ est un isomorphisme. Posons $P = a_0 + \cdots + a_k X^k$. Alors 
        \begin{align*}
            \mat{\mathcal{B}}{P(f)} 
            &= \mat{\mathcal{B}}{a_0 \id + \cdots + a_k f^k} \\
            &= a_0 \mat{\mathcal{B}}{\id} + \cdots + a_k \mat{\mathcal{B}}{f^k} \\
            &= a_0 I_n + \cdots + a_k (\mat{\mathcal{B}}{f})^k \\
            &= P(\mat{\mathcal{B}}{f})
        \end{align*}
    \end{demo}

    \begin{coro}{}{}
        Soient $f \in \mathcal{L}(E)$ et $P,Q \in \mathbb{K}[X]$. Alors $P(f)$ et $Q(f)$ commutent.
    \end{coro}

    \begin{demo}{Idée}{myorange}
        $P(f) \circ Q(f) = (PQ)(f) = (QP)(f) = Q(f) \circ P(f)$.
    \end{demo}

    \begin{prop}{}{}
        Soient $f \in \mathcal{L}(E)$, $P \in \mathbb{K}[X]$ et $v \in E, \lambda \in \mathbb{K}$ tels ques $f(v) = \lambda v$. Alors $P(f)(v) P(\lambda)v$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $P = a_0 + \cdots + a_k X^k$. 
        \begin{align*}
            P(f)(v) 
            &= \left(a_0 + \cdots + a_k X^k\right)(v) \\
            &= a_0 v + \ldots + a_k f^k(v) \\
            &= a_0 v + \cdots + a_k \lambda^k v \\
            &= P(\lambda)v
        \end{align*}
    \end{demo}

    \subsubsection{Cas d’une matrice diagonalisable}

    \begin{prop}{}{}
        Si $M = P D P^{-1}$ et $k \in \mathbb{N}^*$.

        Alors \[ M^k = P D^k P^{-1} \]
    \end{prop}

    \begin{prop}{}{}
        Soit $D = \diag(\lambda_1,\ldots,\lambda_n)$ une matrice diagonale et $k \in \mathbb{N}^*$.

        Alors \[ D^k = \diag(\lambda_1^k,\ldots,\lambda_n^k) \]
    \end{prop}

    \begin{prop}{}{}
        Soit $P = a_0 + a_1 X + \ldots + a_k X^k \in \mathbb{K}[X]$ et $M = Q D Q^{-1}$ une matrice diagonalisée où $D = \diag(\lambda_1,\ldots,\lambda_n)$ et $Q$ est inversible.

        Alors 
        \[ P(M) = Q \diag(P(\lambda_1), \ldots, P(\lambda_n)) Q^{-1} \]
    \end{prop}

    Ainsi, si $M$ est diagonalisable, alors $P(M)$ est diagonalisable, dans la même base que $M$.

    \begin{demo}{Preuve}{myolive}
        \begin{align*}
            P(M)
            &= a_0 I_n + a_1 M + \cdots + a_k M^k \\
            &= \sum_{i=0}^{k} a_i \left(Q \diag(\lambda_1, \ldots, \lambda_n) Q^{-1}\right)^i \\
            &= Q \left(\sum_{i=0}^{k} a_i \diag(\lambda_1^i, \ldots, \lambda_n^i)\right) Q^{-1} \\
            &= Q \diag(P(\lambda_1), \ldots, P(\lambda_n)) Q^{-1}
        \end{align*}
    \end{demo}

    \begin{omed}{Exemple}{myolive}
        Posons $M = \begin{bmatrix}
            2 & 1 & & 1 \\
            1 & 2 & & \\
            & & & 1 \\
            1 & & 1 &2
        \end{bmatrix}$ et $P = (n+2) - (n+2)X + X^2$. Calculons $P(M)$.

        On sait que $M$ est diagonale et $\sp(M) = \left\{n+1,1\right\}$, où $m_1(M) = n-1$ et $m_{n+1}(M) = 1$. Donc il existe $Q \in \GL_n(\mathbb{K})$ tel que $M = Q \diag(n+1, 1, \ldots, 1) Q^{-1}$. Par la proposition précédente, $P(M) = Q \diag(P(n+1), P(1), \ldots, P(1)) Q^{-1}$. Or $P(n+1) = 1$ et $P(1) = 1$, donc $P(M) = Q I_N Q^{-1}$. Donc $P(M) = I_n$.

        On a donc obtenu que $(n+2) I_n - (n + 2)M + M^2 = I_n$. Donc $M$ est inversible et $M^{-1} = \frac{n+2}{n+1} I_n - \frac{1}{n+1} M$.
    \end{omed}

    \subsubsection{Polynôme annulateur}

    \begin{defi}{Polynôme annulateur}{}
        Soit $f \in \mathcal{L}(E)$ et $P \in \mathbb{K}[X]$.

        On dit que $P$ est annulateur si $P(f) = 0$.
    \end{defi}

    \begin{omed}{Exemples}{myyellow}
        \begin{enumerate}[label=\arabic*.]
            \item $f$ est un projecteur \textit{ssi} $f^2 = f$, \textit{i.e.} \textit{ssi} $P = X^2 - X$ est un polynôme annulateur de $f$.
            \item Si $M = \begin{bmatrix}
                a & b \\
                c & d
            \end{bmatrix}$, alors $P = X^2 - \tr(M) X + \det(M)I_2$ est un polynôme annulateur de $M$. En effet, $M^2 = \begin{bmatrix}
                a^2 + bc & ab + bd \\
                ac + dc & bc + d^2
            \end{bmatrix} = \begin{bmatrix}
                a^2 + bc & b(a+d) \\
                c(a+d) & d^2 + bc
            \end{bmatrix}$, $\tr(M) = a+d$ et $\det(M) = ad - bc$. Donc $M^2 - \tr(M)M + \det(M)I_2 = 0_2$.
        \end{enumerate}
    \end{omed}

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$ et $P \in \mathbb{K}[X]$ un polynôme annulateur de $f$.

        Alors les valeurs propres de $f$ sont racines de $P$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Soit $\lambda \in \sp(f)$. Il existe $v \in E$, $v \neq 0$ tel que $f(v) = \lambda v$. Donc $P(f)(v) = P(\lambda)v = 0$. Or $v \neq 0$, donc $P(\lambda) = 0$.
    \end{demo}

    \begin{theo}{Caractérisation des endomorphismes diagonalisables}{}
        Soit $f \in \mathcal{L}(E)$.

        Alors $f$ est diagonalisable \textit{ssi} il existe un polynôme $P$ annulateur de $f$ scindé à racines simples.
    \end{theo}

    \begin{demo}{Preuve}{myred}
        \begin{itemize}
            \item[$\implies$] Si $f$ est diagonalisable. Son polynôme caractéristique $\chi_f$ est scindé, donc possède au moins une racine, \textit{i.e.} $\sp(f) \neq 0$. Posons $\sp(f) = \left\{\lambda_1,\ldots,\lambda_k\right\}$, où $\lambda_1,\ldots,\lambda_k$ sont distincts, et $P = \prod_{i=1}^k (X-\lambda_i)$ scindé à racines simples. Comme $f$ est diagonalisable, il existe $\mathcal{B} = (e_1,\ldots, e_n)$ formée de vecteurs propres de $f$. Alors pour $i \in \intervalleEntier{1}{n}$, il existe $\lambda \in \sp(f)$ tel que 
            \[ P(f)(e_i) = P(\lambda)e_i = 0 \]
            $P(f)$ est nul sur une base, donc est nul, et $P$ est annulateur de $f$.
            \item[$\impliedby$] On peut supposer que le polynôme annulateur est unitaire, et s’écrit sous la forme $P = \prod_{i=1}^{k}(X - \lambda_i)$. Pour $i \in \intervalleEntier{1}{k}$, on pose $L_i(X) = \prod_{\substack{j = 1 \\ j \neq i}}^{k} (X - \lambda_j)$ le $i$-ème polynôme de Lagrange. 
            
            On sait que $(L_1,\ldots,L_k)$ est une base de $\mathbb{K}_{k-1}[X]$. Donc il existe $(a_1,\ldots,a_k) \in \mathbb{K}^k$ tels que $1 = \sum_{i=1}^{k} a_i L_i(X)$, d’où $\id_E = \sum_{i=1}^{k} a_i L_i(f)$, \textit{i.e.} $\forall x \in E, x = \sum_{i=1}^{k} a_i L_i(f)(x)$.

            Pour tout $i \in \intervalleEntier{1}{k}$, on a $P(X) = (X - \lambda_i) L_i(X)$ d’où $P(f) = \left(f - \lambda_i \id_E\right) \circ L_i(f) = 0$. Ainsi, pour tout $i \in \intervalleEntier{1}{k}$, $\Im(L_i(f)) \subset \ker(f - \lambda_i \id_E)$. Donc pour $x \in E$, $x = \sum_{i=1}^{k} a_i L_i(f)(x)$. 

            Donc pour tout $x \in E$, il existe $(v_1, \ldots v_k) \in \ker(f - \lambda_1 \id) \times \cdots \times \ker(f - \lambda_k \id)$ tels que $x = v_1 + \cdots v_k$. Donc 
            \[ E = \sum_{i=1}^{k} \ker(f - \lambda_i \id) \]
            Or $P$ est annulateur, donc pour tout valeur propre $\lambda$ de $f$, $P(\lambda) = 0$. Soit $j \in \intervalleEntier{1}{k}$ tel que $\lambda_j \notin \sp(f)$ alors $\ker(f - \lambda_j \id) = \left\{0\right\}$. Donc $E = \sum_{\substack{i = 1 \\ i \neq j}}^{k} \ker(f - \lambda_i \id)$. Donc $E = \sum_{\lambda \in \sp(f)} E_{\lambda}(f)$.
        \end{itemize}
    \end{demo}

    \begin{omed}{Exemple}{myred}
        Soit $M = \begin{bmatrix}
            2 & 1 & & 1 \\
            1 & \ddots & & \\
            & & & 1 \\
            1 & & 1 & 2
        \end{bmatrix}$. Le polynôme $P = (X - (n+1))(X-1) = X^2 - (n+2) X + n+1$ est annulateur, donc la matrice est diagonalisable.
    \end{omed}

    \begin{coro}{}{}
        Soit $f \in \mathcal{L}(E)$ diagonalisable et $F$ un sev de $E$ stable par $f$.

        Alors l’endomorphisme $\tilde{f}$ induit par $f$ sur $F$ est diagonalisable.
    \end{coro}

    \begin{demo}{Preuve}{myorange}
        D’après le théorème précédent, il existe $P \in \mathbb{K}[X]$ scindé à racines simples tel que $P(f) = 0$. Alors $P(\tilde{f}) = 0$, donc $\tilde{f}$ est diagonalisable.
    \end{demo}

    \subsubsection{Théorème de Cayley-Hamilton}

    \begin{theo}{Cayley-Hamilton}{CH}
        Soit $f \in \mathcal{L}(E)$.

        Le polynôme caractéristique de $f$ est annulateur, \textit{i.e.} 
        \[ \chi_f(f) = 0 \]
    \end{theo}

    \begin{demo}{Démonstration dans le cas diagonalisable}{myred}
        Supposons $M \in \mk{n}$ diagonalisable. Il existe $Q \in \GL_n(\mathbb{K})$ et $D = \diag(\lambda_1,\ldots,\lambda_n)$ tels que $M = Q D Q^{-1}$ où $\sp(M) = \left\{\lambda_1,\ldots,\lambda_n\right\}$ où les $\lambda_i$ ne sont pas nécessairement distincts deux à deux. Alors $\chi_M(M) = Q \diag(\chi_M(\lambda_1), \ldots, \chi_M(\lambda_n)) Q^{-1} = 0$ car les $\lambda_i$ sont racines du polynôme caractéristique.

        La densité des matrices diagonalisables dans $\mathcal{M}_n(\mathbb{C})$ permet de généraliser le résultat : Si $A \in \mathcal{M}_n(\mathbb{C})$, on peut l’approcher par une suite de matrices diagonalisables $A = \lim D_k$ où chaque $D_k$ vérifie par ce qui précède $\chi_{D_k}(D_k) = 0$. Or, 
        \[ \et{P_k \limi{k}{+\infty} P \in \mathbb{C}_n[X]}{A_k \limi{k}{+\infty} A \in \mathcal{M}_n(\mathbb{C})} \implies P_k(A_k) \limi{k}{+\infty} P(A) \in \mathcal{M}_n(\mathbb{C}) \]    
        On peut donc remarquer que $\chi_{D_k}(D_k) \limi{k}{+\infty} \chi_A(A)$ d’où $\chi_A(A) = 0$ par unicité de la limite.
    \end{demo}

    \begin{demo}{Démonstration dans le cas général}{myred}
        Soit $u \in \mathcal{L}(E)$, où $E$ est un espace vectoriel de dimension finie. Montrer que $\chi_u(u) = 0$ revient à montrer que $\forall x \in E, \chi_u(u)(x) = 0$. La propriété étant immédiate pour $x = 0$, nous considérerons désormais un vecteur $x \in E$ non nul.
        \begin{itemize}
            \item $E$ étant de dimension finie, l’ensemble non vide $\mathcal{N}_x = \enspr{k \in \mathbb{N}^*}{(x,u(x), \ldots, u^{k-1}(x)) \text{ est libre}}$ est majoré. Notons $p$ son plus grand élément, et $F = \vect((x,u(x), \ldots, u^{p-1}(x)))$. Par maximalité de $p$, 
            \[ \exists \lambda_0,\ldots, \lambda_{p-1} \in \mathbb{K}, \quad u^p(x) = \lambda_0 x + \cdots \lambda_{p-1} u^{p-1}(x) \]   
            \item $F$ est stable par $u$. En effet, si $y$ est combinaison linéaire de $x,u(x), \ldots, u^{p-1}(x)$, alors $u(y)$ l’est de même d’après la valeur de $u^p(x)$. Considérons la matrice de l’endomorphisme induit $\restr{u}{F}$ dans la base $(x, u(x), \ldots, u^{p-1}(x))$ et son polynôme caractéristique :
            \[ \mat{}{\restr{u}{F}} = \begin{bmatrix}
                0 & 0 & \cdots & 0 & \lambda_0 \\
                1 & \ddots & \ddots & \vdots & \lambda_1 \\
                0 & \ddots & \ddots & 0 & \vdots \\
                \vdots & \ddots & \ddots & 0 & \vdots \\
                0 & \cdots & 0 & 1 & \lambda_{p-1}
            \end{bmatrix} \esp{et} \chi_{\restr{u}{F}} = \begin{vmatrix}
                X & 0 & \cdots & 0 & -\lambda_0 \\
                - 1 & \ddots & \ddots & \vdots & -\lambda_1 \\
                0 & \ddots & \ddots & 0 & \vdots \\
                \vdots & \ddots & \ddots & X & \vdots \\
                0 & \cdots & 0 & - 1 & X - \lambda_{p-1}
            \end{vmatrix} = X^p - \sum_{k=0}^{p-1} \lambda_k X^k \]   
            par exemple en développant par rapport à la dernière colonne.
            
            Comme $u^p(x) = \lambda_0 x + \lambda_1 u(x) + \cdots + \lambda_{p-1} u^{p-1}(x)$, alors $\chi_{\restr{u}{F}}(u)(x) = 0$.
            \item Comme $\chi_{\restr{u}{F}}$ divise $\chi_u$, il existe $P \in \mathbb{K}[X]$ tel que $\chi_{u} = P \times \chi_{\restr{u}{F}}$. Ainsi, $\chi_u(u) = P(u) \circ \chi_{\restr{u}{F}}(u)$ donc est nul sur $E$.
        \end{itemize}
    \end{demo}

    \begin{omed}{Exemple}{myred}
        Pour une matrice carrée d’ordre $2$ 
        \[ M = \begin{pmatrix}
            a & b \\
            c & d
        \end{pmatrix} \]  
        on a $\chi_M = X^2 - (a+d)X + ad - bc$. Par conséquent, 
        \[ M^2 - \tr(M) M + \det(M) I_2 = 0 \]
    \end{omed}

    \begin{omed}{Application}{myred}
        Si $M \in \mk{n}$ est nilpotente, alors $M^n = 0$.
    \end{omed}

    \begin{demo}{Justification}{myred}
        Si $M$ est nilpotente, il existe $k \in \mathbb{N}$ tel que $M^k = 0$ \textit{i.e.} $X^k$ est annulateur donc $\sp(M) \subset \left\{0\right\}$. Si $\mathbb{K} = \mathbb{C}$, alors $\chi_M$ est scindé, donc $\chi_M = X^n$. Ainsi, $\chi_M(M) = 0 = M^n$.
    \end{demo}

\subsection{Applications de la réduction}

    \subsubsection{Terme général d’une suite vérifiant une relation de récurrence linéaire}

    Soit $(u_n) \in \mathbb{K}^n$ une suite telle que $p \in \mathbb{N}^*$ et $(a_0, \ldots, a_{p-1}) \in \mathbb{K}^p$ vérifiant, pour tout $n \in \mathbb{N}$, $u_{n+p} = a_0 u_n + \cdots + a_{p-1} u_{n + p-1}$. On cherche le \textsc{terme général} de $(u_n)$. 
    
    Posons $X_n = \begin{bmatrix}
        u_n \\
        \vdots \\
        u_{n + p-1}
    \end{bmatrix}$. Alors $(u_n)$ vérifie la relation de récurrence \textit{ssi} $(X_n)$ vérifie 
    \[ X_{n + 1} = \underbrace{\begin{bmatrix}
        0 & 1 & 0 & \ldots & 0 \\
        \vdots & \ddots & \ddots & \ddots & \vdots \\
        \vdots & & \ddots & \ddots & 0 \\
        0 & \cdots & \cdots & 0 &  1 \\
        a_0 & a_1 & \cdots & \cdots & a_{p-1}
    \end{bmatrix}}_{M} X_n \]  
    Ainsi, pour tout  $n \in \mathbb{N}$, $X_n = M^n X_0$. Supposons que $M$ possède $p$ valeurs propres. Alors ces valeurs propres sont de multiplicité $1$ car $M \in \mk{p}$, donc $M$ est diagonalisable, et il existe $Q \in \GL_p(\mathbb{K})$ telle que 
    \[ M = Q \diag(\lambda_1,\ldots,\lambda_p)Q^-1 \esp{d’où} M^n = Q \diag(\lambda_1^n, \ldots, \lambda_p^n) Q^{-1} \] 
    Ainsi, pour tout $n \in \mathbb{N}$, $X_n = Q \begin{bmatrix}
        \lambda_1^n & & 0 \\
        & \ddots & \\
        0 & & \lambda_p^n
    \end{bmatrix} Q^{-1} X_0$, \textit{i.e.} il existe $A_1,\ldots,A_p \in \mathbb{K}$ tel que 
    \[ \forall n \in \mathbb{N}, u_n = A_1 \lambda_1^n + \cdots + A_p \lambda_p^n \]   
    D’où $(u_n) \in \vect((\lambda_1)^n, \ldots, (\lambda_p)^n)$.

    \begin{omed}{Exemple \textcolor{black}{(Suite de Fibonacci)}}{myolive}
        On appelle \textbf{suite de Fibonacci} la suite $(f_n)$ définie par $f_0 = f_1 = 1$ et la relation $f_{n+2} = f_{n+1} + f_n$. 

        Posons $X_n = \begin{bmatrix}
            f_n \\
            f_{n+1}
        \end{bmatrix}$. Alors $X_{n+1} = \underbrace{\begin{bmatrix}
            0 & 1 \\
            1 & 1
        \end{bmatrix}}_M X_n$ d’où $X_n = M^n X_0$.

        Le polynôme $\chi_M(X) = X^2 - X - 1$ possède deux racines réelles distinctes $\frac{1 \pm \sqrt{5}}{2}$, qui sont valeurs propres de $M$, donc il existe $\alpha$ et $\beta$ tels que 
        \[ \forall n \in \mathbb{N}, f_n = \alpha \left( \frac{1 + \sqrt{5}}{2} \right)^n + \beta \left( \frac{1-\sqrt{5}}{2} \right)^n \]
        
        Or, sachant que $f_0 = f_1 = 1$, on trouve que 
        \[ \forall n \in \mathbb{N}, f_n = \frac{1}{2} \left( \left( \frac{1 + \sqrt{5}}{2} \right)^{n+1} - \left( \frac{1-\sqrt{5}}{2} \right)^{n+1}  \right) \]
    \end{omed}

    \subsubsection{Calcul d’un déterminant circulant}

    Soient $(a_0, \ldots, a_{n-1}) \in \mathbb{K}^n$ et $M = \begin{bmatrix}
        a_0 & a_1 & \cdots & a_{n-1} \\
        a_{n-1} & a_0 & \cdots & a_{n-2} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_1 & a_2 & \cdots & a_0
    \end{bmatrix}$. On cherche à calculer le déterminant, dit \textbf{circulant}, de $M$.

    Posons $J = \begin{bmatrix}
        0 & 1 & & 0 \\
        \vdots & \ddots & \ddots & \\
        0 & & \ddots & 1 \\
        1 & 0 & \cdots & 0
    \end{bmatrix}$. Soit $f$ canoniquement associée à $J$ et $\mathcal{B} = (e_1,\ldots, e_n)$ la base canonique de $\mathbb{K}^n$. Alors $\forall i \in \intervalleEntier{1}{n}$, $f(e_i) = \ovl{e_{i-1}}$. Ainsi, $f^p(e_i) = \ovl{e_{i - p}}$. 

    On peut donc écrire $M = a_0 I_n + a_1 J + \cdots + a_{n-1} J^{n-1}$. Si $P = a_0 + a_1 X + \cdots a_{n-1} X^{n-1}$, alors $M = P(J)$. Ainsi, si on peut diagonaliser $J$, cela revient à pouvoir diagonaliser $M$.

    $J$ est-elle diagonalisable ?
    \begin{align*}
        \chi_J(X) 
        &= \det(X I_n - J) \\
        &= \begin{vmatrix}
            X & - 1 & 0 & \cdots & 0 \\
            0 & \ddots & \ddots & \ddots & \vdots \\
            \vdots & & & \ddots & 0 \\
            0 & & & \ddots & -1 \\
            -1 & 0 & \cdots & 0 & X
        \end{vmatrix} \\
        &\quad \downarrow \quad \text{dvp p/r à la prem. colonne} \\
        &= X^n + (-1)^n \times (-1)^{n-1} \\
        &= X^n - 1
    \end{align*}
    Donc les valeurs propres de $J$ sont les racines $n$-ième de l’unité, \textit{i.e.} $\sp_{\mathbb{C}}(J) = \left\{e^{\frac{2ik\pi}{n}}, k \in \intervalleEntier{0}{n-1}\right\}$. On pose $\omega = e^{\frac{2i\pi}{n}}$. $J$ possède $n$ valeurs propres distinctes, donc est diagonalisable, et il existe $Q \in \GL_n(\mathbb{C})$ telle que $J = Q \diag(1,\omega, \ldots, \omega^{n-1}) Q^{-1}$. Ainsi, $\det(M) = \prod_{k=0}^{n-1} P(\omega^k)$.

    \begin{omed}{Exemple}{myblue}
        Calcul de $\Delta_n = \begin{vmatrix}
            1 & 2 & \cdots & n \\
            n & 1 & \cdots & \vdots \\
            \vdots & \vdots & \ddots & 2 \\
            2 & 3 & \cdots & 1
        \end{vmatrix}$. On pose $P = 1 + 2X + \cdots + n X^{n-1}$, et on sait que $\Delta_n = \prod_{k=0}^{n-1} P(\omega^k)$.

        On remarque que $P = (1 + X + \cdots + X^n)'$, d’où
        \begin{align*}
            P(x) 
            &= (1 + x + \cdots + x^n)'\\
            &= \left(\frac{1-x^{n+1}}{1-x}\right)' \\
            &= \frac{-(n+1)x^n (1-x) + 1 - x^{n+1}}{(1-x)^2}
        \end{align*}
        \begin{itemize}
            \item Si $k \in \intervalleEntier{1}{n-1}$,
            \begin{align*}
                P(\omega^k) 
                &= \frac{-(n+1)(\omega^k)^n (1 - \omega^k) + 1 - (\omega^k)^{n+1}}{(1 - \omega^k)^2} \\
                &= \frac{-(n+1)(1 - \omega^k) + 1 - \omega^k}{(1 - \omega^k)^2} \\
                & = \frac{-n}{1 - \omega^k}
            \end{align*}
            \item Si $k = 0$, $P(\omega^k) = P(1) = \frac{n(n+1)}{2}$
        \end{itemize}
        Finalement, 
        \begin{align*}
            \Delta_n
            &= \frac{n(n+1)}{2} \times \prod_{k=1}^{n-1} \frac{-n}{1 - \omega^k} \\
            &= \frac{(-1)^{n-1}(n+1)n^n}{2 \prod_{k=1}^{n-1}(1 - \omega^k)} 
        \end{align*}

        Calculons séparément le produit $\prod_{k=1}^{n-1} (1 - \omega^k)$. $1 - \omega^k$ est racine de $(1 - X)^n - 1$, d'où $(1 - X)^n - 1 = (-1)^n \prod_{k=0}^{n-1} (X - (1- \omega^k))$. On cherche le produit des racines \textsc{\textbf{non nulles}} de ce polynôme, \textit{i.e.} le coefficient de degré $1$ de $(1 - X)^n -1$, qui est $-n$ par dénombrement. Ainsi, on obtient $-n = (-1)^{2n-1} \prod_{k=1}^{n-1} (1 - \omega^k)$ \textit{soit} $n = \prod_{k=1}^{n-1} (1 - \omega^k)$.

        Donc 
        \begin{align*}
            \Delta_n 
            &= \frac{(-1)^{n-1} (n+1) n^{n-1}}{2}
        \end{align*}

        On aurait aussi pu directement remarquer que $1 + X + \cdots + X^{n-1} = \frac{1 - X^n}{1 - X} = \prod_{k=1}^{n-1} (X - \omega^k)$ d’où $\prod_{k=1}^{n-1} (1 - \omega^k) = n$ en évaluant en $1$.
    \end{omed}

    \subsubsection{Traces successives nulles}

    \begin{omed}{Traces successives nulles}{myjade}
        Soit $M \in \mathcal{M}_n(\mathbb{C})$ telle que $\forall k \in \intervalleEntier{1}{n}, \tr(M^k) = 0$. Montrons que $M$ est nilpotente.
    \end{omed}

    \begin{demo}{Résolution}{myjade}
        On sait que $\chi_M$ est scindé, donc $M$ est trigonalisable. Il existe $P \in \GL_n(\mathbb{C})$ et $T = \begin{bmatrix}
            \lambda_1 & & \star \\
             & \ddots & \\
            0 & & \lambda_n
        \end{bmatrix}$ telles que $M = P T P^{-1}$. Donc $\tr(M) = \sum_{i=1}^{n} \lambda_i = 0$.

        De plus, si $k \in \intervalleEntier{1}{n}$, $M^k = P T^k P^{-1}$. Or, pour tout $k \in \intervalleEntier{1}{n}$, $T^k = \begin{bmatrix}
            \lambda_1^k & & \star \\
             & \ddots & \\
            0 & & \lambda_n^k
        \end{bmatrix}$ et ainsi $\sum_{i=1}^{n} \lambda_i^k = 0$.

        Montrons que cela implique que ces $\lambda_k$ sont tous nuls en raisonnant par l’absurde. On suppose qu’il existe une valeur propre non nulle. Posons $P(X) = \prod_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} (X - \lambda_i)$ qui est de degré non nul $k$. On peut écrire $P(X) = a_0 + a_1 X + \cdots + a_k X^k$. Et donc 
        \begin{align*}
            \prod_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} P(\lambda_i) 
            &= 0 \\
            &= \sum_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} (a_0 + a_1 \lambda_i + \cdots + a_n \lambda_i^k) \\
            &= a_0 \sum_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} 1 + a_1 \sum_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} \lambda_i + \cdots + a_k \sum_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} \lambda_i^k 
            &= a_0 \card\left\{i \in \intervalleEntier{1}{n}, \lambda_i \neq 0\right\} + a_1 \underbrace{\sum_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} \lambda_i}_{= 0} + \cdots + a_n \underbrace{\sum_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} \lambda_i^k}_{= 0}
        \end{align*}
        On en déduit que $a_0 = 0$ puisque l’on a supposé $\card\left\{i \in \intervalleEntier{1}{n}, \lambda_i \neq 0\right\} \neq 0$. Or $P(0) = \prod_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} (- \lambda_i) = (-1)^k \prod_{\substack{i = 1 \\ \lambda_i \neq 0}}^{n} \lambda_i$, ce qui est absurde. Donc $M$ est semblable à $T = \begin{bmatrix}
            0 & & \star \\
             & \ddots & \\
            0 & & 0
        \end{bmatrix}$. Ainsi, $\chi_M = \chi_T = X^n$ est annulateur par le théorème de Cayley-Hamilton, d’où $T^n = 0$ puis $M^n = 0$.
    \end{demo}

    \begin{omed}{Une matrice nilpotente}{myred}
        Soient $A,B \in \mk{n}$ et $C = AB - BA$. On suppose que $C$ commute avec $A$ où $B$. Montrons que $C$ est nilpotente.
    \end{omed}

    \begin{demo}{Solution}{myred}
        On a tout d’abord $\tr(C) = \tr(AB) - \tr(BA) = 0$. Est-ce le cas pour tout $C^k$ où $k \in \intervalleEntier{1}{n}$ ? 

        Supposons arbitrairement que $C$ commute avec $A$. Alors $CA = AC$, et
        \begin{align*}
            \tr(C^k) 
            &= \tr(C^{k-1}(AB - BA)) \\
            &= \tr(C^{k-1} AB) - \tr(C^{k-1} BA) \\
            &\quad \downarrow \quad A C^k = C^k A \\
            &= \tr(A C^{k-1} B) - \tr(C^{k-1} BA) \\ 
            &\quad \downarrow \quad \tr(XY) = \tr(YX) \\
            &= 0
        \end{align*} 
    \end{demo}

\subsection{Généralisation des résultats de réduction}

    \subsubsection{Lemme des noyaux}

    \begin{lem}{dit des noyaux}{}
        On considère un espace vectoriel $E$ sur un corps commutatif $K$ et $u \in \mathcal{L}(E)$. Soit $s \in \mathbb{N}^*$.

        Si $Q_1,\ldots,Q_s \in K[X]$ sont des polynômes premiers entre eux deux à deux, alors 
        \[ \ker\left[(Q_1\cdots Q_s)(u)\right] = \bigoplus_{i=1}^s \ker(Q_i(u)) \]
        De plus, chacun des projecteurs associés à cette décomposition en somme directe est la restriction à $\ker\left[(Q_1\cdots Q_s)(u)\right]$ d’un polnôme en $u$.
    \end{lem}

    \begin{demo}{Preuve}{mybrown}
        \begin{enumerate}[label=\textcolor{myjade}{(\arabic*)}]
            \item Les polynômes $R_1,\ldots,R_s \in K[X]$ définis par 
            \[ \forall k \in \intervalleEntier{1}{s}, \quad R_k = \prod_{\substack{1 \leq i \leq s \\ i \neq k}} Q_i \]    
            sont premiers entre eux dans leur ensemble, donc d’après le théorème de Bézout, il existe des polynômes $V_1, \ldots, V_s \in K[X]$ tels que 
            \[ V_1 R_1 + \ldots + V_s R_s = 1 \]    
            \item Si $x \in \ker\left[(Q_1\cdots Q_s)(u)\right]$, on déduit de la relation précédente que 
            \[ x = x_1 + \ldots + x_s \quad \text{où } \forall i \in \intervalleEntier{1}{s}, x_i = \left[(V_i R_i)(u)\right](x) \]
            De plus, pour chaque $i \in \intervalleEntier{1}{s}$, on a 
            \[ \left[Q_i(u)\right](x_i) = \left[(Q_i V_i R_i)(u)\right](x) = V_i(u) \left(\left[(Q_1 \cdots Q_s)(u)\right](x)\right) = 0 \]
            Donc $x_i \in \ker((Q_i(u)))$. On a donc montré que 
            \[ \ker\left[(Q_1 \cdots Q_s)(u)\right] = \ker\left((Q_1(u))\right) + \ldots + \ker\left(Q_s(u)\right) \]     
            \item Considérons un élément 
            \[ (x_1,\ldots,x_s) \in \ker(Q_1(u)) \times \cdots \times \ker(Q_s(u)) \]   
            vérifiant $x_1 + \ldots + x_s = 0$. En remarquant que pour tout couple $(i,j) \in \intervalleEntier{1}{s}^2$ avec $i \neq j$, on a $\left[R_i(u)\right](x_j) = 0$ et en utilisant la relation de Bézout, on a 
            \[ x_i = \sum_{j=1}^{s} \left[V_j R_j(u)\right](x_i) = \left[V_i R_i (u)\right](x_i) = \left[V_i R_i(u)\right]\left(- \sum_{\substack{j = 1 \\ j \neq i}}^{s} x_j\right) = 0 \]
            donc la somme est directe.
        \end{enumerate}
    \end{demo}

    \subsubsection{Sous-espaces caractéristiques}

    Nous avons vu que, lorsque $f$ est diagonalisable, on a $E = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_r}$ avec $E_{\lambda_i} = \ker(f - \lambda_i \id_E)$ le sous-espace propre associé à la valeur propre $\lambda_i$. Nous allons montrer que si $f$ n’est pas diagonalisable, mais si son polynôme caractéristique est scindé sur $\mathbb{K}$, on a 
    \[ E = \ker(f - \lambda_1 \id)^{m_1} \oplus \cdots \oplus \ker(f - \lambda_r \id)^{m_r} \]   
    où $m_i$ est la multiplicité de la valeur propre $\lambda_i$ comme racine du polynôme caractéristique de $f$.

    \begin{defi}{}{}
        Soit $f$ un endomorphisme de $E$. Soit $\lambda$ une valeur propre de $f$ et $m$ sa multiplicité en tant que racine de $\chi_f$. Le \textbf{sous-espace caractéristique} de $f$ pour la valeur propre $\lambda$ est 
        \[ N_{\lambda} = \ker\left((f - \lambda \id)^m\right) \]     
    \end{defi}

    Pour $\lambda$ valeur propre de $f$, on a $E_{\lambda} \subset N_{\lambda}$ car $\ker(f - \lambda \id) \subset \ker(f - \lambda \id)^m$ quel que soit $k \geq 1$.

    \begin{theo}{}{}
        Soit $f$ un endomorphisme de $E$ tel que $\chi_f$ est scindé sur $\mathbb{K}$. Notons $\chi_f = \pm (X - \lambda_1)^{m_1} \cdots (X - \lambda_r)^{m_r}$. Alors 
        \begin{enumerate}
            \item Tout $N_{\lambda_i}$ est stable par $f$.
            \item $E = N_{\lambda_1} \oplus \cdots \oplus N_{\lambda_r}$.
            \item $\dim N_{\lambda_i} = m_i$.
        \end{enumerate}
    \end{theo}

    Autrement dit, l’espace vectoriel $E$ est la somme directe des sous-espaces caractéristiques. En plus, la dimension du sous-espace caractéristique associé à la valeur de $\lambda$ est sa multiplicité comme racine de $\chi_f$.

    \begin{demo}{Preuve}{myred}
        \begin{itemize}
            \item Soit $x \in N_{\lambda} = \ker(f - \lambda \id)^m$. On a $(f - \lambda \id)^m(x) = 0$. Or 
            \[ (f - \lambda \id)^m \circ f(x) = f \circ (f - \lambda \id)^m (x) = 0 \]   
            D’où $f(x) \in N_{\lambda}$. Cela vient donc du fait que $f - \lambda \id$ et $f$ commutent, qui est un résultat que nous avons déjà rencontré.
            \item C’est une application directe du lemme des noyaux : Les polynômes $(X - \lambda_i)^{m_i}$ sont premiers entre eux dans leur ensemble puisque les valeurs propres sont distinctes. Par le lemme des noyaux, on obtient : 
            \[ \ker \chi_f(f) = \ker(f - \lambda_1 \id)^{m_1} \oplus \cdots \oplus \ker(f - \lambda_r \id)^{m_r} = N_{\lambda_1} \oplus \cdots \oplus N_{\lambda_r} \]    
            Or d’après le théorème de Cayley-Hamilton, on a $\chi_f(f) = 0$, donc $\ker \chi_f(f) = E$, d’où le résultat.
            \item Notons $g_i = \restr{f}{N_{\lambda_i}}$ pour $1 \leq i \leq r$. Pour $i \neq j$, $N_{\lambda_i} \cap N_{\lambda_j} = \left\{0\right\}$. Le polynôme caractéristique de $g_i$ est scindé (car il divise celui de $f$) et sa seule racine est la valeur propre de $g_i$, \textit{i.e.} $\lambda_i$. Ainsi, $\chi_{g_i} (X) = \pm (X - \lambda_i)^{n_i}$ où $n = \dim N_{\lambda_i}$. De plus, 
            \[ \pm (X - \lambda_1)^{m_1} \cdots (X - \lambda_r)^{m_r} = \chi_f(X) = \chi_{g_1}(X) \cdots \chi_{g_r}(X) \]    
            En identifiant les exposants des facteurs irréductibles, on a directement $n_i = \dim N_{\lambda_i} = m_i$ pour $i \in \intervalleEntier{1}{r}$.
        \end{itemize}
    \end{demo}

\subsection{Trigonalisation}    

    \subsubsection{Endomorphismes et matrices trigonalisables}

    \begin{defi}{Trigonalisable}{}
        \begin{itemize}
            \item Soit $f \in \mathcal{L}(E)$. On dit que $f$ est \textbf{trigonalisable} s’il existe une base $\mathcal{B}$ de $E$ telle que $M_{\mathcal{B}}(f)$ est triangulaire supérieure.
            \item Soit $M \in \mk{n}$. On dit que $M$ est \textbf{trigonalisable} si $M$ est semblable à une matrice triangulaire supérieure.
        \end{itemize}
    \end{defi}

    Si une matrice $\mat{\mathcal{B}}{f}$, dans une base $\mathcal{B} = (e_1, \ldots, e_n)$, est triangulaire supérieure, alors dans la base $\mathcal{B}' = (e_n, \ldots, e_1)$, la matrice $\mat{\mathcal{B}'}{f}$ est triangulaire inférieure. C’est une convention que l’on choisit ici pour les démonstrations mais qui n’a pas d’influence sur la réduction.

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$. Les affirmations suivantes sont équivalentes :
        \begin{enumerate}
            \item $f$ est trigonalisable ;
            \item il existe $B$ une base de $E$ telle que $M_{\mathcal{B}}(f)$ est trigonalisable ;
            \item pour toute base $\mathcal{B}$ de $E$, $M_{\mathcal{B}}(f)$ est trigonalisable.
        \end{enumerate}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}[leftmargin=2cm]
            \item[\textbf{(i)} $\implies$ \textbf{(ii)}] Si $f$ est trigonalisable, il existe une base $\mathcal{B}$ de $E$ telle que $\mat{B}{f}$ est triangulaire supérieure donc trigonalisable.
            \item[\textbf{(ii)} $\implies$ \textbf{(iii)}] $\mat{\mathcal{B}}{f}$ est trigonalisable et $\mat{\mathcal{B}'}{f}$ y est semblable donc est trigonalisable.
            \item[\textbf{(iii)} $\implies$ \textbf{(i)}] Soit $\mathcal{B}$ telle que $\mat{\mathcal{B}}{f}$ est trigonalisable. Il existe alors $P \in \GL_n(\mathbb{K})$ et $T$ une matrice triangulaire supérieure telles que $\mat{\mathcal{B}}{f} = P T P^{-1}$. Il existe $\mathcal{B}'$ une base de $E$ telle que $P$ est la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$, et par la formule de changement de base,
            \[ \mat{\mathcal{B}'}{f} = P^{-1} \mat{\mathcal{B}}{f}  P = P^{-1} P T P^{-1} P = T \]   
            \textit{i.e.} $f$ est trigonalisable.
        \end{itemize}
    \end{demo}

    \begin{theo}{}{}
        Soit $f \in \mathcal{L}(E)$ (ou $A \in \mk{n}$). 

        Alors $f$ est trigonalisable \textit{ssi} son polynôme caractéristique $\chi_f$ (ou $\chi_A$) est scindé dans $\mathbb{K}$.
    \end{theo}

    \begin{demo}{Démonstration \textcolor{black}{(Cas matriciel)}}{myred}
        \begin{itemize}
            \item[$\implies$] Si $M$ est trigonalisable, $M = P^-1 \underbrace{\begin{bmatrix}
                \lambda_1 & & \star \\
                & \ddots & \\
                0 & & \lambda_n
            \end{bmatrix}}_{T} P$, d’où $\chi_M(X) = \chi_T(X) = \prod_{i=1}^{n} (X - \lambda_i)$ est scindé. 
            \item[$\impliedby$] Par récurrence sur $n$.
            \begin{itemize}
                \item[\textbf{I}] Si $n = 2$, $\chi_M$ est scindé donc $\sp(M) \neq \emptyset$. Soit $\lambda \in \sp(M)$. Il existe $X_1 \neq 0 \in \mk{2,1}$ telle que $MX_1 = \lambda X_1$. Il existe $X_2$ telle que $\mathcal{B} = (X_1, X_2)$ est une base de $\mk{2,1}$. Alors 
                \[ \mat{\mathcal{B}}{f} = \begin{bmatrix}
                    \lambda & \star \\
                    0 & \star
                \end{bmatrix}\] 
                \textit{i.e.} $M$ est semblable à une matrice triangulaire supérieure.
                \item[\textbf{H}] Soit $M \in \mk{n+1}$ telle que $\chi_M$ est scindé. Ainsi, $\sp(M) \neq 0$, et il existe $\lambda \in \sp(M)$ et $X_1 \neq 0 \in \mk{n+1,1}$ tels que $MX_1 = \lambda X_1$. On complète $(X_1)$ en $\mathcal{B}= (X_1, \ldots X_{n+1})$ une base de $\mk{n+1, 1}$, d’où  
                \[ \mat{\mathcal{B}}{f} = \begin{bmatrix}
                    \lambda & \star & \ldots & \star \\
                    0 & & & \\
                    \vdots & & M' & \\
                    0 & & &
                \end{bmatrix} \]   
                Or $\chi_M = (X - \lambda) \chi_{M'}$, donc $\chi_{M'}$ est scindé et par hypothèse de récurrence, $M' = P T P^{-1}$. Donc $M$ est semblable à  $\mat{\mathcal{B}}{f} 
                 = \begin{bmatrix}
                    \lambda & \star & \ldots & \star \\
                    0 & & & \\
                    \vdots & & P T P^{-1} & \\
                    0 & & &
                \end{bmatrix}$ qui est telle que $\begin{bmatrix}
                    1 & 0 \\
                    0 & P^-1
                \end{bmatrix} \begin{bmatrix}
                    \lambda & \star & \ldots & \star \\
                    0 & & & \\
                    \vdots & & P T P^{-1} & \\
                    0 & & &
                \end{bmatrix} \begin{bmatrix}
                    1 & 0 \\
                    0 & P
                \end{bmatrix} = \begin{bmatrix}
                    \lambda & \star & \ldots & \star \\
                    0 & & & \\
                    \vdots & & T  & \\
                    0 & & &
                \end{bmatrix}$ i.e. semblable à une matrice triangulaire supérieure. Finalement, $M$ est semblable à une matrice triangulaire supérieure.
            \end{itemize}
        \end{itemize}
    \end{demo}

    Ainsi, dans le cas particulier où $n = 2$, $M = \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}$ est trigonalisable \textit{ssi} $X^2 - (a + d)X + ad - bc$ est scindé, \textit{i.e.} $\Delta = (a-d)^2 + 4bc < 0$.

    \begin{coro}{}{}
        Si le corps est $\mathbb{K} = \mathbb{C}$, alors tout endomorphisme est trigonalisable et toute matrice est trigonalisable.
    \end{coro}

    \begin{demo}{Ideé}{myorange}
        Tout polynôme dans $\mathbb{C}[X]$ est scindé d’après le théorème de Gauss.
    \end{demo}

    \begin{prop}{}{}
        Soit $f \in \mathcal{L}(E)$ un endomorphisme trigonalisable. On pose $Sp(f) = \big\{ \lambda_1 , \ldots, \lambda_p  \big\}$ où les $\lambda_i$ sont distincts et de multiplicités $m_1,\ldots,m_p$.

        Alors \[ \tr(f) = \sum_{i=1}^{p} m_i \lambda_i \quad \text{et} \quad \det(f) = \prod_{i=1}^{p} (\lambda_i)^{m_i} \]
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        $f$ est trigonalisable donc il existe une base de $E$ telle que $\mat{\mathcal{B}}{f} = \begin{bmatrix}
            \lambda_1 & & \star \\
            & \ddots & \\
            0 & & \lambda_p
        \end{bmatrix}$ d’où le résultat.
    \end{demo}

    \subsubsection{Méthode de trigonalisation d’une matrice}

    \begin{omed}{Exemple}{mybrown}
        Soit $A = \begin{bmatrix}
            1 & 0 & 0 & 0 \\
            4 & -2 & -3 & 3 \\
            0 & 4 & 4 & -3 \\
            -2 & 1 & 0 & 1
        \end{bmatrix}$. Réduisons $A$.
        \begin{itemize}
            \item \textbf{Calcul du polynôme caractéristique} \quad 
            \begin{align*}
                \chi_A(X)
                &= \det(X I_n - A) \\
                &= \begin{vmatrix}
                    X - 1 & 0 & 0 & 0 \\
                    - 4 & X + 2 & 3 & -3 \\
                    0 & 4 & X - 4 & + 3 \\
                    +2 & -1 & 0 & X - 1
                \end{vmatrix} \\
                &\quad \downarrow \quad \text{dvp p/r à la première ligne} \\
                &= (X-1) \begin{vmatrix}
                    X + 2 & 3 & -3 \\
                    4 & X - 4 & + 3 \\
                    -1 & 0 & X - 1
                \end{vmatrix} \\
                &\quad \downarrow \quad \text{dvp p/r à la dernière ligne}
                &= (X-1)^2 \begin{vmatrix}
                    X + 2 & 3 \\
                    4 & X - 4
                \end{vmatrix} - \begin{vmatrix}
                    3 & - 3 \\
                    X - 4 & 3
                \end{vmatrix} \\
                &= (X - 1)^4
            \end{align*}
            Donc $\sp(A) = \left\{1\right\}$. Si la matrice $A$ est diagonalisable, alors elle est semblable à la matrice identité, ce qui est absurde, donc $A$ n’est pas diagonalisable. Toutefois, son polynôme caractéristique est scindé, donc elle est trigonalisable.
                \item \textbf{Calcul de $E_1(A)$} \quad On a $I_4 - A = \begin{bmatrix}
                    0 & 0 & 0 & 0 \\
                    -4 & 3 & 3 & 3 \\
                    0 & -4 & -3 & 3 \\
                    2 & -1 & 0 & 0
                \end{bmatrix}$, qui est de rang $3$ car les colonnes première, deuxième et troisième sont indépendantes. Ainsi, $\dim\left(\ker(I_4 - A)\right) = 1$, et on a directement $E_1(A) = \ker(I_4 - A) = \vect\left\{\begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix}\right\}$.
                \item \textbf{Calcul de $\ker(I_4 - A)^2$} \quad On a $(I_4 - A)^2 = \begin{bmatrix}
                    0 & 0 & 0 & 0 \\
                    -18 & 0 & 0 & 0 \\
                    22 & -3 & - 3 & 3 \\
                    4 & -3 & - 3 & 3 
                \end{bmatrix}$ qui est de rang $2$, étant donné que la ligne deuxième correspond à la soustraction de la quatrième par la troisième. Ainsi, $\dim\left(\ker(I_4 - A)^2\right) = 2$ -- Ce résultat se déduisait du fait que $\underbrace{\ker(I_4 - A)}_{\text{de dim 1}} \subsetneq  \ker(I_4 - A)^2 \subset \cdots \subset \ker\underbrace{(I_4 - A)^4}_{= 0 \text{par C.H.}} = \mathbb{K}^4$, et que si deux noyaux consécutifs sont égaux, alors il y a égalité des noyaux suivants --. Ainsi, $\ker(I_4 - A) \subset \ker(I_4 - A)^2 = \vect\left(\begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix}\right)$. Comme $(I_4 - A) \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix} \in \ker(I_4 - A) = E_1(A)$, on remarque que $(I_4 - A) \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix} = - \begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix}$. Ainsi, cela se réécrit $A \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix} = \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix} + \begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix}$. Cette expression sera utile à la trigonalisation, et est une étape nécessaire du raisonnement.
                \item \textbf{Calcul de $\ker(I_4- A)^3$ } \quad On a $(I_4 - A)^3 = \begin{bmatrix}
                    0 & 0 & 0 & 0 \\
                    0 & 0 & 0 & 0 \\
                    18 & 0 & 0 & 0 \\
                    18 & 0 & 0 & 0 \\
                \end{bmatrix}$ qui est de rang $1$, et donc de noyau de dimension $3$, tel que $\ker(I_4 - A)^2 \subset \ker(I_4 - A)^3 = \vect\left(\begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    1 \\
                    0 \\
                    0
                \end{bmatrix}\right)$. On remarque de plus que $(I_4 - A) \begin{bmatrix}
                    0 \\
                    1 \\
                    0 \\
                    0
                \end{bmatrix} = - \begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix} + 3 \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix}$.
                \item \textbf{Détermination de la base} \quad On sait que $(I_4 - A)^4 = 0$ par le théorème de Cayley-Hamilton, donc $\ker(I_4 - A) = \mathbb{K}^4 = \vect\left(\begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    0 \\
                    1 \\
                    0 \\
                    0
                \end{bmatrix}, \begin{bmatrix}
                    1 \\ 
                    0 \\
                    0 \\
                    0
                \end{bmatrix}\right)$. Enfin, on remarque que $(I_4 - A) \begin{bmatrix}
                    1 \\
                    0 \\
                    0 \\
                    0
                \end{bmatrix} = 2 \begin{bmatrix}
                    0 \\
                    0 \\
                    1 \\
                    1
                \end{bmatrix} + 2 \begin{bmatrix}
                    0 \\
                    -1 \\
                    1 \\
                    0
                \end{bmatrix} - 6 \begin{bmatrix}
                    0 \\
                    1 \\
                    0 \\
                    0
                \end{bmatrix}$.
            \item \textbf{Trigonalisation de $A$} \quad Soient $f_1 = \begin{bmatrix}
                0 \\
                0 \\
                1 \\
                1
            \end{bmatrix}$, $f_2 = \begin{bmatrix}
                0 \\
                -1 \\
                1 \\
                0
            \end{bmatrix}$, $f_3 = \begin{bmatrix}
                0 \\
                1 \\
                0 \\
                0
            \end{bmatrix}$ et $f_4 = \begin{bmatrix}
                1 \\
                0 \\
                0 \\
                0
            \end{bmatrix}$. En posant $\mathcal{B} = (f_1, f_2, f_3, f_4)$, on trouve que la matrice de l’endomorphisme $f$ canoniquement associé à $A$ dans $\mathcal{B}$ est -- en reprenant les expressions à chaque fois trouvées $f_2 = f_2 + f_1$, $f_3 = f_3 + f_1 - 3 f_2$ puis $f_4 = f_4 - 2 f_1 - 2 f_2 + 6 f_3$ --,
            \[ A' = \mat{\mathcal{B}}{f} = \begin{bmatrix}
                1 & 1 & 1 & -2 \\
                0 & 1 & -3 & -2 \\
                0 & 0 & 1 & 6 \\
                0 & 0 & 0 & 1
            \end{bmatrix} \]    d’où $A = P A' P^{-1}$ où $P = \begin{bmatrix}
                0 & 0 & 0 & 1 \\
                0 & 1 & 1 & 0 \\
                1 & -1 & 0 & 0 \\
                1 & 0 & 0 & 0
            \end{bmatrix}$.
        \end{itemize}
    \end{omed}

    \begin{omed}{Principe de trigonalisation}{myred}
        \begin{itemize}
            \item Soit $A \in \mk{n}$ trigonalisable -- \textit{i.e.} $\chi_A$ est scindé --. Posons $\chi_A = \prod_{i=1}^{p} (X - \lambda_i)^{m_i}$. D’après le théorème de Cayley-Hamilton, $\chi_A$ est annulateur. Posons $P_i(X) = (X - i)^{m_i}$, tel que $\chi_A = \prod_{i=1}^{p} P_i$ et donc $\prod_{i=1}^{p} P_i(A) = 0$. Ainsi, $(P_1,\ldots,P_p)$ sont premiers entre eux dans leur ensemble, et d’après le lemme des noyaux, 
        \[ \mk{n,1} = \bigoplus_{i=1}^p \ker(A - \lambda_i I_n)^{m_i} \]   
            \item On cherche donc une base adaptée à cette décomposition. 
            \begin{itemize}
                \item Soit $i \in \intervalleEntier{1}{p}$. On veut une base de $\ker(A - \lambda_i I_n)^{m_i}$. On sait que 
                \[ \ker(A - \lambda_i I_n) \subset \ker(A - \lambda_i I_n)^2 \subset \cdots \subset \ker(A -\lambda_i I_n)^{m_i} \]
                Pour cela, on cherche tout d’abord une base de $E_{\lambda_i}$, formée de vecteurs propres.
                \item Si $\ker(A - \lambda_i I_n)^2 = \ker(A - \lambda_i I_n)$, alors tous les noyaux suivants seront égaux, et la base de $E_{\lambda_i}$ est une base de $\ker(A - \lambda_i I_n)^{m_i}$.
                \item Sinon, $\ker(A - \lambda_i I_n) \subsetneq \ker(A - \lambda_i I_n)^2$, donc on peut compléter la base de $E_{\lambda_i}$ en base de $\ker(A - \lambda_i I_n)^2$.
                \item On répète ce processus pour chaque $i \in intervalleEntier{1}{p}$, pour obtenir une base de $\mk{n,1}$ dans laquelle la matrice de l’endomorphisme canoniquement associé à $A$ est triangulaire supérieure.
            \end{itemize}
        \end{itemize}
    \end{omed}

    \begin{omed}{Exemple}{myred}
        Soit $A = \begin{bmatrix}
            1 & -1 & 1 & 0 & 1 \\
            0 & 0 & 1 & 1 & 0 \\
            0 & -1 & 2  0 & 1 \\
            0 & 0 & 0 & 1 & -2 \\
            0 & 0 & 0 & 1 & -3
        \end{bmatrix}$.
        \begin{itemize}
            \item On calcule $\chi_A = \det(X I_5 - A)$ dans un premier temps.
            \begin{align*}
                \chi_A(X) 
                &\quad \downarrow \quad \text{dvp p/r à la première ligne} \\
                &= (X - 1) \begin{vmatrix}
                    X & - 1 & -1 & 0 \\
                    1 & X - 2 & 0 & -1 \\
                    0 & 0 & X- 1 & 2 \\
                    0 & 0 & -2 & X+3
                \end{vmatrix} \\
                &\quad \downarrow \quad \text{déterminant triangulaire par bloc} \\
                &= (X-1)^3 (X + 1)^2
            \end{align*}
            Donc $A$ est trigonalisable car $\chi_A$ est scindée.
            \item On a donc, d’après le principe de trigonalisation, $\mathbb{K}^5 = \ker(I_5 + A)^2 \oplus \ker(I_5 - A)^3$. 
            \item \textbf{Détermination d’une base de $\ker(I_5 + A)^2$} \quad $(I_5 + A) = \begin{bmatrix}
                -2 & 1 & -1 & 0 & -1 \\
                0 & -1 & -1 & -1 & 0 \\
                0 & 1 & -3 & 0 & -1 \\
                0 & 0 & 0 & -2 & 2 \\
                0 & 0 & 0 & -2 & 2
            \end{bmatrix}$ est de rang $4$ car les quatre premières colonnes sont libres, donc $\dim(\ker(I_5 + A)) = 1$. Pour trouver un vecteur de $\ker(I_5 + A)$, on résout le système $(I_5 + A) \begin{bmatrix}
                x \\
                y \\
                z \\
                t \\
                u
            \end{bmatrix} = \begin{bmatrix}
                0 \\
                0 \\
                0 \\
                0 \\
                0
            \end{bmatrix}$, pour trouver le vecteur $f_1 = \begin{bmatrix}
                1 \\
                1 \\
                1 \\
                -2 \\
                -2
            \end{bmatrix}$. $(I_5 + A)^2 = \begin{bmatrix}
                4 & -4 & 4 & 1 & 1 \\
                0 & 0 & 4 & 3 & -1 \\
                0 & -4 & 8 & 1 & 1 \\
                0 & 0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0 & 0  
            \end{bmatrix}$ est de rang $3$ donc $\dim(\ker(I_5 + A)^2) = 2$, et $f_1$ en est un élément car $\ker(I_5 + A) \subset \ker(I_5 + A)^2$. $f_2 = \begin{bmatrix}
                1 \\
                2 \\
                1 \\
                -1 \\
                1
            \end{bmatrix} \in \ker(I_5 + A)^2$ convient, donc $\ker(I_5 +A)^2 = \vect(f_1, f_2)$.
            \item \textbf{Détermination d’une base de $\ker(I_5 - A)^3$} \quad $(I_3 - A) = \begin{bmatrix}
                0 & 1 & -1 & 0 & -1 \\
                0 & 1 & -1 & -1 & 0 \\
                0 & 1 & -1 & 0 & -1 \\
                0 & 0 & 0 & 0 & 2 \\
                0 & 0 & 0 & -2 & 4 \\
            \end{bmatrix}$ est de rang $3$ d’après la configuration des colonnes -- une est nulle et deux sont proportionnelles, le reste indépendantes --. donc $\dim(\ker(I_5 - A)) = 2$. On en cherche une base, par exemple constituée des vecteurs $f_3 = \begin{bmatrix}
                0 \\
                1 \\
                1 \\
                0 \\
                0
            \end{bmatrix}$ et $f_4 = \begin{bmatrix}
                1 \\
                1 \\ 
                0 \\
                0 \\
                0
            \end{bmatrix}$. Ensuite $(I_5 - A)^2 = \begin{bmatrix}
                0 & 0 & 0 & 1 & -3 \\
                0 & 0 & 0 & - 1 & -1 \\
                0 & 0 & 0 & 1 & -3 \\
                0 & 0 & 0 & -4 & 8 \\
                0 & 0 & 0 & - 8 & 12 \\
            \end{bmatrix}$ est de rang $2$, donc son noyau est de dimension $3$. On peut donc compléter sa base d’après la famille $(f_3,f_4)$ en lui ajoutant le vecteur $f_5 = \begin{bmatrix}
                0 \\
                1 \\
                0 \\
                0 \\
                0
            \end{bmatrix}$ par exemple. 
            \item Finalement, $(f_1,\ldots,f_5)$ est une base de trigonalisation. Il reste à calculer les images des vecteurs par $A$.
            \begin{align*}
                A f_2 &= \alpha f_1 - f_2 \\
            \end{align*}
        \end{itemize}
    \end{omed}

\subsection{Décomposition de Dunford}

    Nous allons montrer que toute matrice, dont le polynôme caractéristique est scindé, peut s’écrire comme somme d’une matrice diagonalisable et d’une matrice nilpotente. Autrement dit, cette matrice est semblable à la somme d’une matrice diagonale et d’une matrice nilpotente.

    \begin{theo}{Décomposition du Dunford}{}
        Soit $f$ un endomorphisme de $E$ tel que $\chi_f$ soit scindé sur $\mathbb{K}$. Alors il existe un unique couple $(n,d)$ d’endomorphismes tel que :
        \begin{enumerate}
            \item $n$ est nilpotent et $d$ est diagonalisable ;
            \item $f = n + d$ ;
            \item $n \circ d = d \circ n$.
        \end{enumerate}
    \end{theo}

    On pourrait de plus montrer que $d$ et $n$ sont des polynômes en $f$. En particulier, si $\mathbb{K} = \mathbb{C}$, cette décomposition existe toujours. Le théorème peut aussi s’écrire matriciellement : 

    \begin{theo}{Décomposition de Dunford \textcolor{black}{(Version matricielle)}}{}
        Pour toute matrice $A \in \mk{n}$ ayant un polynôme caractéristique scindé sur $\mathbb{K}$, il existe une unique matrice $N$ nilpotente et une unique matrice $\Delta$ diagonalisable telles que 
        \[ A = N + \Delta \esp{et} N \Delta = \Delta N \]     
    \end{theo}

    Il est très important de noter que $\Delta$ n’est qu’une matrice diagonalisable, et non diagonale. Il existe alors $P \in \GL_n(\mathbb{K})$ et $D$ diagonale telles que $D = P^{-1} \Delta P$. Si on note $N' = P^{-1} N P$, alors $N'$ est encore nilpotente et $N' D = D N'$. Une autre façon d’écrire la décomposition de Dunford est donc d’écrire $P^{-1} A P = D + N'$, \textit{i.e.} $A$ est semblable à la somme d’une matrice diagonale et d’une matrice nilpotente.

    \begin{coro}{}{}
        Soit $f$ un endomorphisme avec une décomposition de Dunford $f = d + n$. Alors 
        \begin{itemize}
            \item $f \text{ diagonalisable } \iff f = d \iff n = 0$ ;
            \item $f \text{ nilpotent } \iff  f = n \iff d = 0$
        \end{itemize}
    \end{coro}

    Avant de démontrer ces théorèmes de décomposition de Dunford, introduisons quelques lemmes.

    \begin{lem}{}{}
        Si $f$ est nilpotent, alors $0$ est son unique valeur propre et on a 
        \[ \chi_f(X) = (-1)^n X^n \]  
    \end{lem}

    \begin{demo}{Preuve}{mybrown}
        \begin{itemize}
            \item Notons $A$ la matrice de $f$ dans une base. Comme $f$ est nilpotent, il existe $k \geq 1$ tel que $A^k = 0$. Cela implique $\det(A^k) = 0$, puis $\det(A) = 0$. La matrice $A$ n’est donc pas inversible, $f$ n’est pas bijectif, et donc non injectif -- l’ensemble de départ étant aussi celui d’arrivée --. Ainsi, $\ker(f) \neq \left\{0\right\}$, ce qui est exactement dire que $0$ est une valeur propre de $f$. 
            \item Supposons que $\lambda$ soit une valeur propre de $f$ : il existe alors $x \neq 0$ tel que $f(x) = \lambda x$. Par récurrence immédiate, cela signifie que $f^n(x) = \lambda^n x = 0$ -- provient du fait que si $f$ est nilpotent, son indice de nilpotence est inférieur à la dimension de l’espace --, donc $\lambda = 0$. Ainsi, la seule valeur propre de $f$, et donc la seule racine de son polynôme caractéristique est $0$. 
            \item Si $\mathbb{K} = \mathbb{R}$, on considère l’endomorphisme aussi défini sur $\mathbb{C}$. En termes de matrices, cela revient à dire que la matrice à coefficients réels $A$ peut aussi être vue à coefficient complexes. Or, sur $\mathbb{C}$, un polynôme dont la seule racine est $0$ est de la forme $a X^n$, donc $\chi_f(X) = (-1)^n X^n$, étant donné que le coefficient dominant d’un polynôme caractéristique est $(-1)^n$.
        \end{itemize}
    \end{demo}

    \begin{lem}{}{}
        Soit $f$ un endomorphisme de $E$ diagonalisable. On note $\lambda_1,\ldots, \lambda_r$ ses valeurs propres et $E_{\lambda_1}, \ldots, E_{\lambda_r}$ les sous-espaces propres correspondants. Si $F$ est un sev de $E$ stable par $f$, alors on a 
        \[ F = (F \cap E_{\lambda_1}) \oplus \cdots \oplus (F \cap E_{\lambda_{r}}) \]    
    \end{lem}

    \begin{demo}{Preuve}{mybrown}
        Soit $x \in F$. Comme $x \in E$, il existe $x_1,\ldots,x_r$ uniques tels que $x_i \in E_{\lambda_i}$ et $x = x_1 + \ldots + x_r$. 

        Le sous-espace $F$ est stable par $f$, donc également par $P(f)$ pour tout $P \in \mathbb{K}[X]$. Comme $x_i \in E_{\lambda_i}$, alors $f(x_i) = \lambda_i x_i$ et plus généralement $P(f)(x_i) = P(\lambda_i) x_i$. Pour $1 \leq i \leq r$, on définit 
        \[ P_i(X) = \prod_{\substack{k = 1 \\ k \neq i}}^{r} (X - \lambda_k) \]    
        On a ainsi $P_i(\lambda_j) = 0$ si $i \neq j$ et $P_i(\lambda_i) \neq 0$. On peut donc écrire 
        \[ P_i(f)(x) = P_i(f)(x_1 + \ldots + x_r) = P_i(\lambda_i) x_i \]    
        Or $P_i(f)(x) \in F$ par stabilité de $f$, donc $x_i \in F$. Ainsi, pour tout $1 \leq i \leq r$, $x_i \in F \cap E_{\lambda_i}$, d’où le résultat.
    \end{demo}

    \begin{lem}{}{}
        Si $f$ est diagonalisable et $F$ est un sous-espace vectoriel de $E$, stable par $f$, alors la restriction de $f$ à $F$ est aussi diagonalisable. 
    \end{lem}

    Une preuve de ce lemme a déjà été faite dans la partie sur les polynôme d’endomorphismes, mais en voici une autre : 

    \begin{demo}{Démonstration}{mybrown}
        Notons $g$ la restriction de $f$ au sous-espace $F$. $g = \restr{f}{F}$ est bien définie car $F$ est stable par $f$. Soient $ \lambda_1,\ldots, \lambda_r$ les valeurs propres de $f$. L’endomorphisme $f$ étant supposé diagonalisable, on a 
        \[ E = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_r} \]   
        Ainsi, d’après le lemme précédent, 
        \[ F = (F \cap E_{\lambda_1}) \oplus \cdots \oplus (F \cap E_{\lambda_r}) \]    
        Or, quel que soit $\mu \in \mathbb{K}$, $\ker(g - \mu \id) = F \cap \ker(f - \mu \id)$, donc les valeurs propres de $g$, notées $\mu_1,\ldots,\mu_s$ appartiennent à l’ensemble $\left\{\lambda_1,\ldots,\lambda_r\right\}$. Ces $\mu_i$ forment exactement l’ensemble des valeurs propres de $f$ pour lesquels $F \cap E_{\lambda_i} \neq \left\{0\right\}$, et 
        \[ F = \ker(g - \mu_1 \id) \oplus \cdots \oplus \ker(g - \mu_s \id) \]    
        ce qui prouve que $g$ est diagonalisable.
    \end{demo}

    \begin{lem}{}{}
        Soient $f,g$ deux endomorphismes diagonalisables. On suppose que $f \circ g = g \circ f$. Alors il existe une base commune de vecteurs propres de $f$ et de $g$. 
    \end{lem}

    Autrement dit, si $A,B \in \mk{n}$ sont diagonalisables et commutent, alors on peut les diagonaliser dans une base commune, \textit{i.e.} il existe $P \in \mk{n}$ inversible telle que $P^{-1} A P$ et $P^{-1} B P$ soient toutes les deux diagonales.

    \begin{demo}{Preuve}{mybrown}
        Soient $\lambda_1,\ldots,\lambda_r$ les valeurs propres de $f$. Notons $E_{\lambda_i} = \left\{x \in E, \quad f(x) = \lambda_i x\right\}$. On a alors, pour $x \in E_{\lambda_i}$., 
        \[ f(g(x)) = g(f(x)) = g(\lambda_ix) = \lambda_i g(x) \]   
        donc $g(x_i) \in E_{\lambda_ï}$, ce qui prouve que $E_{\lambda_i}$ est stable par $g$. D’après le lemme 4, la restriction de $g$ à $E_{\lambda_i}$ est donc diagonalisable. On considère, dans $E_{\lambda_i}$, une base $\mathcal{B}_i$ de vecteurs propres de $g$. Ce sont aussi des vecteurs propres de $f$, et comme $f$ est diagonalisable, 
        \[ E = E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_r} \]   
        La base $\mathcal{B}_1 \cup \cdots \cup \mathcal{B}_r$ est donc une base de $E$ formée de vecteurs propres qui le sont à la fois pour $f$ et $g$.
    \end{demo}

    Passons désormais à la preuve du théorème. Voici l’idée générale :
    \begin{itemize}
        \item On décompose l’ev $E$ en somme des SE caractéristiques $N_{\lambda_i}$, où les $\lambda_i$ sont les valeurs propres de $f$ ;
        \item Sur chacun de ces SE, on décompose la restriction de $f$ en $d_i + n_i$, avec $d_i = \lambda_i \id_{N_{\lambda_i}}$ qui est diagonalisable ;
        \item On montre que $n_i$, qui est $f- d_i$ restreit à $N_{\lambda_i}$, est nilpotent ;
        \item Comme $d_i$ est $\lambda_i$ fois l’identité, alors $d_i$ commute en particulier avec $n_i$.
    \end{itemize}

    \begin{demo}{Démonstration}{myred}
        \begin{itemize}[label=\textcolor{myred}{$\to$}]
            \item \textbf{Construction} \quad \begin{itemize}
                \item Soit $\chi_f$ le polynôme caractéristique de $f$ qui est par hypothèse scindé sur $\mathbb{K}$. Notons $\lambda_i$ une valeur propre de $f$, et $m_i$ sa multiplicité : 
                \[ \chi_f(X) = \pm \prod_{i = 1}^r (X - \lambda_i)^{m_i} \]   
                Soient $N_{\lambda_i}$ les SE caractéristiques de $f$. Pour $i \in \intervalleEntier{1}{r}$, on a 
                \[ N_{\lambda_i} = \ker(f - \lambda_i  \id)^{m_i} \esp{et} E = N_{\lambda_1} \oplus \cdots \oplus N_{\lambda_r} \]    
                \item Pour tout $N_{\lambda_i}$, on définit $d$ par \[ d : x \in N_{\lambda_i} \mapsto \lambda_i x \]   
                L’espace vectoriel $E$ étant sommme directe des $N_{\lambda_i}$, $d$ est définie sur $E$ tout entier. En effet, si $x \in E$ est décomposé en $x = x_1 + \cdots + x_r$, avec $x_i \in N_{\lambda_i}$, alors 
                \[ d(x) = \lambda_1 x_1 + \cdots + \lambda_r x_r \]   
                On pose enfin $n(x) = f(x) - d(x)$. Il nous reste à vérifier que $n$ et $d$ conviennent.
            \end{itemize}
            \item \textbf{Propriétés} \begin{enumerate}
                \item Par construction, $d$ est diagonalisable. En effet, fixons une base pour chaque SE $N_{\lambda_i}$. Pour chaque vecteur $x$ de cette base, $d(x) = \lambda_i x$. Comme $E$ est somme directe des $N_{\lambda_i}$, alors dans la base de $E$ formée de l’union des bases des $N_{\lambda_i}$, $d$ est diagonalisable.
                \item On a défini $n = f - d$. $N_{\lambda_i}$ est stable par $n$ (car c’est vrai pour $f$ et $d$). On pose $n_i = \restr{n}{N_{\lambda_i}} = \restr{f}{N_{\lambda_i}} - \id_{N_{\lambda_i}}$. Alors par définition, $N_{\lambda_i} = \ker(n_ï^{m_i})$, et donc $n_i^{m_i} = 0$. Ainsi, en posant $m = \max\left\{m_i\right\}_{i \in \intervalleEntier{1}{m}}$, on obtient que $n^m = 0$.
                \item On va vérifier que $d \circ n = n \circ d$. Si $x \in E$, il se décompose en $x = x_1 + \cdots + x_r$ avec $x_i \in N_{\lambda_i}$. Sur chaque $N_{\lambda_i}$, $\restr{d}{N_{\lambda_i}}$ commute avec tout endomorphisme, en particulier avec $d \circ n(x_i) = n \circ d(x_i)$ puisque $N_{\lambda_i}$ est stable par $n$. On a donc 
                \[ d \circ n(x) = d \circ n(x_1 + \cdots + x_r) = d \circ n(x_1) + \cdots + d \circ n(x_r) = n \circ d(x_1) + \cdots + n \circ d(x_r) = n \circ d(x) \]
                Ainsi, $d$ et $n$ commutent.
                \item Il reste à prouver l’unicité. Supposons que $(n,d)$ soit le couple construit ci-dessus et $(n', d')$ un autre couple vérifiant les propriétés \textbf{(i)}, \textbf{(ii)} et \textbf{(iii)} de la décomposition de Dunford.
                \begin{itemize}
                    \item Montrons que $d$ et $d'$ commutent, et $n$ et $n'$. On a $f = d + n = d' + n'$, donc 
                    \[ d \circ f = d \circ (d + n) = (d + n) \circ d = f \circ d \]    
                    car $n$ et $d$ commutent. Ainsi, $f$ et $d$ commutent, et il en va de même pour $d'$. Soit $x \in N_{\lambda_i} = \ker(f - \lambda_i \id)^{m_i}$. Alors 
                    \[ (f - \lambda_i \id)^{m_i} \circ d'(x) = d' \circ (f - \lambda_i \id)_{m_i}(x) = 0 \]   
                    d’où $N_{\lambda_i}$ est stable par $d'$. Par construction, $\restr{d}{\lambda_{N_i}} = \lambda_i \id_{N_{\lambda_i}}$, donc $d$ et $d'$ commutent sur chaque $N_{\lambda_i}$ et donc sur $E$ tout entier.
                    
                    Or $n = f - d$ et $n' = f - d'$, donc $n$ et $n'$ commutent également.
                    \item Comme $d$ et $d'$ commutent, d’après un des lemmes, il existe une base commune de vecteurs propres, en particulier, $d - d'$ est diagonalisable. 
                    
                    Comme les endomorphismes $n$ et $n'$ sont nilpotents et commutent, $n - n'$ est également nilpotent. 
                    \item Ainsi, $d - d' = n' - n$ est un endomorphisme à la fois diagonalisable et nilpotent. Comme il est nilpotent, sa seule valeur propre est $0$ -- voir un lemme ci-dessus --. Et comme il est diagonalisable, c’est nécessairement l’endomorphisme nul. On a donc $d - d' = n' - n = 0$, ce qui donne l’unicité.
                \end{itemize}
            \end{enumerate}
        \end{itemize}
    \end{demo}

    \begin{omed}{Piège classique}{myred}
        Une matrice triangulaire peut toujours s’écrire comme somme d’une matrice diagonale et d’une matrice nilpotente, mais, en général, celles-ci ne commutent pas. 
        \[ A = \begin{bmatrix}
            1 & 3 \\
            0 & 2
        \end{bmatrix} \qquad D = \begin{bmatrix}
            1 & 0 \\
            0 & 2
        \end{bmatrix} \qquad N = \begin{bmatrix}
            0 & 3 \\
            0 & 0
        \end{bmatrix} \]   
        en est un exemple : on n’a pas $ND = DN$, et la décomposition de Dunford n’est pas ainsi trouvée. La décomposition de Dunford est ici tout simplement $D = A$ et $N$ est la matrice nulle car $A$ est diagonalisable.
    \end{omed}

    \begin{omed}{Pratique de la décomposition}{myred}
        La méthode pour trouver la décomposition de Dunford d’une matrice $A \in \mk{n}$ consiste à suivre les étapes de la preuve : 
        \begin{enumerate}
            \item On calcule le polynôme caractéristique $\chi_A$ de $A$ : il doit être scindé. On calcule ses racines, qui sont les valeurs propres de $A$.
            \item Pour chaque valeur propre $\lambda$, de multiplicité $m$, on note $N_{\lambda} = \ker(A - \lambda \id)^m$. C’est un EV de dimension $m$. On détermine $m$ vecteurs formant une base de $N_{\lambda}$. L’union de toutes les bases $\mathcal{B}_{\lambda}$ des $N_{\lambda}$ forme une base $\mathcal{B} = (v_1,\ldots, v_n) \in \mathbb{K}^n$.
            \item On définit l’endomorphisme $d$ par $d(v_i) = \lambda v_i$ pour chaque $v_i \in N_{\lambda}$. On peut remarquer que dans la base $\mathcal{B}$, la matrice de $d$ est diagonale. On note $\mathcal{B}_0 = (e_1,\ldots,e_n)$ la base canonique de $\mathbb{K}^n$, et $A$ la matrice de $f$ dans la base $\mathcal{B}_0$. Alors $\Delta$ sera la matrice de $d$ dans la base $\mathcal{B}_0$, \textit{i.e.} les colonnes de $\Delta$ sont les coordonnées de $d(e_i)$ exprimées dans la base $(e_1,\ldots, e_n)$. 
            \item On pose $N = A - \Delta$. Par la démonstration du théorème de décomposition de Dunford, $\Delta$ est diagonalisable, $N$ est nilpotente et $\Delta N = N \Delta$. La matrice de passage $P$ de la base $\mathcal{B}$ vers la base canonique $\mathcal{B}_0$ transforme $\Delta$ en une matrice diagonale $D = P^{-1} \Delta P$.
        \end{enumerate}
    \end{omed}

    \begin{omed}{Exemple}{myred}
        Calculons la décomposition de Dunford de la matrice 
        \[ A = \begin{bmatrix}
            1 & 1 & 1 \\
            0 & 1 & 1 \\
            0 & 0 & 2
        \end{bmatrix} \in \mathcal{M}_3(\mathbb{R}) \]   
        \begin{enumerate}
            \item Le polynôme caractéristique $\chi_A$ est égal à 
            \[ \chi_A(X) = \det(A - X I_3) = \begin{vmatrix}
                1 - X & 1 & 1 \\
                0 & 1- X & 1 \\
                0 & 0 & 2- X
            \end{vmatrix} = -(X-1)^2(X-2) \]   
            Nous avons donc deux valeurs propres qui sont $\lambda_1$ et $\lambda_2 = 2$. La valeur propre $1$ est de multiplicité $m_1 = 2$, alors que, pour la valeur propre 2, $m_2 = 1$.
            \item On note $N_1 = \ker(A - I_3)^2$ et $N_2 = \ker(A - 2 I_3)$. L’espace vectoriel $\mathbb{R}^3$ s’écrit comme somme directe 
            \[ \mathbb{R}^3 = \ker(A - I_3)^2 \oplus \ker(A - 2 I_3) \]    
            Déterminons ces SE caractéristiques : 
            \begin{itemize}
                \item On sait que $N_1 = \ker(A - I_3)^2$ est un EV de dimension $2$. 
                \[ A - I_3 = \begin{bmatrix}
                    0 & 1 & 1 \\
                    0 & 0 & 1 \\
                    0 & 0 & 1
                \end{bmatrix} \qquad (A - I_3)^2 = \begin{bmatrix}
                    0 & 0 & 2 \\
                    0 & 0 & 1 \\
                    0 & 0 & 1
                \end{bmatrix} \]   
                Ainsi, $N_1$ est le plan vectoriel engendré par les vecteurs $v_1 = (1,0,0)$ et $v_2 = (0,1,0)$, \textit{i.e.} $N_1 = \mathbb{R} v_1 + \mathbb{R} v_2$. Au passage, notons que la matrice $A$ n’est pas diagonalisable : en effet, la valeur $1$ est de multiplicité $2$, mais $E_1 = \ker(A - I_3) = \left\{v \in \mathbb{R}^3, \quad Av = v\right\}$ est de dimension seulement $1$ car $E_1 = \mathbb{R} v_1$.
                \item On sait que $N_2$ est un EV de dimension $1$. Pour déterminer le noyau $\ker(A - I_3) = \left\{v \in \mathbb{R}^3, \quad Av = 2v\right\}$, si $v = (x,y,z)$, on résout :
                \[ \left\{ \begin{array}{rcl}
                    x + y + z & = & 2x \\
                    y + z & = & 2y \\
                    2z & = & 2z
                \end{array} \right. \iff \et{x = 2z}{y = z} \]  
                Le SE $N_2$ est donc la droite vectorielle engendrée par le vecteur $v_3 = (2,1,1)$ : $N_2 = \mathbb{R}v_3$. 
                \item La famille $\mathcal{B} = (v_1, v_2, v_3)$ est une base de $\mathbb{R}^3$ : 
                \[ \mathbb{R}^3 = \mathbb{R} v_1 \oplus \mathbb{R} v_2 \oplus \mathbb{R} v_3 \]   
            \end{itemize}
            \item On définit l’endomorphisme $d$ par $d(v_1) = v_1$, $d(v_2) = v_2$ et $d(v_3) = 2 v_3$. Dans la base $\mathcal{B}$, la matrice de $d$ est donc $D = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 2 
            \end{bmatrix}$. Or nous voulons la matrice de $d$ dans la base canonique $\mathcal{B}_0 = (e_1, e_2, e_3)$. 
            \begin{align*}
                d(e_1) &= d((1,0,0)) = (1,0,0) = e_1 \\
                d(e_2) &= e_2 \\
                d(e_3) &= d((0,0,1)) \\
                &= d(-2v_1 - v_2 + v_3) \\
                &= -2 d(v_1) - d(v_2) + d(v_3) \\
                &= -2v_1 - v_2 + 2 v_3 \\
                &= 2e_1 + e_2 + 2e_3
            \end{align*}
            Ainsi, 
            \[ \Delta = \mat{\mathcal{B}_0}{d} = \begin{bmatrix}
                1 & 0 & 2 \\
                0 & 1 & 1 \\
                0 & 0 & 2
            \end{bmatrix} \]   
            \item On pose \[ N = A - \Delta = \begin{bmatrix}
                0 & 1 & -1 \\
                0 & 0 & 0 \\
                0 & 0 & 0
            \end{bmatrix}\] La décomposition de Dunford est $A = \Delta + N$. La preuve du théorème de décomposition affirme que $\Delta$ est diagonalisable, $N$ est nilpotente et $\Delta N = N \Delta$. 
            \item On note $P$ la matrice de passage de la base $\mathcal{B}_0$ vers la $\mathcal{B}$. $P$ contient donc, en colonnes, les vecteurs de la nouvelle base $\mathcal{B} = (v_1, v_2, v_3)$ exprimés dans l’ancienne base $\mathcal{B}_0 = (e_1, e_2, e_3)$. Comme $v_1 = e_1$, $v_2 = e_2$ et $v_3 = 2e_1 + e_2 + e_3$, alors 
            \[ P = \begin{bmatrix}
                1 & 0 & 2 \\
                0 & 1 & 1 \\
                0 & 0 & 1
            \end{bmatrix} \esp{et on calcule} P^{-1} = \begin{bmatrix}
                1 & 0 & -2 \\
                0 & 1 & -1 \\
                0 & 0 & 1
            \end{bmatrix} \]   
            Si besoin, on peut diagonaliser $\Delta$ : 
            \[ D = P^{-1} \Delta P = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 2
            \end{bmatrix} \]    
        \end{enumerate}
    \end{omed}

    $D$ et $N$ sont uniques, mais il y a plusieurs choix possibles pour les vecteurs $v_i$ et donc pour la matrice $P$.

    \begin{omed}{Application au calcul de puissance}{myred}
        La décomposition de Dunford est utile pour calculer les puissances d’une matrice. Voyons les étapes pour calculer $A^p$, où $A \in \mk{n}$.
        \begin{enumerate}
            \item Écrire la décomposition de Dunford $A = \Delta + N$.
            \item Diagonaliser $\Delta$ en $D = P^{-1} \Delta P$ où $D$ est diagonale. On peut ainsi calculer $D^k$ facilement pour $k \geq 0$.
            \item On note $N' = P^{-1} N P$. la matrice $N'$ est toujours nilpotente, et on calcule ses puissances successives en sachant qu’APCR, tous les $N'^k$ sont nuls.
            \item Comme $D$ et $N'$ commutent (car $\Delta$ et $N$ commutent), on applique la formule du binôme de Newton : 
            \[ (D + N')^p = \sum_{k=0}^{p} \binom{p}{k} D^{p-k} N'^k \]   
            La somme a peu de termes non nuls car $N'^k$ est nulle APCR.
            \item Finalement, on a $A = \Delta + N = P(D + N') P^{-1}$. 
            \[ A^p = P(D + N')P^{-1} \]   
        \end{enumerate}
    \end{omed}

\subsection{Réduction de Jordan}

    Nous allons montrer que toute matrice, dont le polynôme caractéristique est scindé, est semblable à une matrice diagonale par blocs, avec blocs « presque diagonaux ».

    \begin{defi}{}{}
        Un \textbf{bloc de Jordan} est une matrice de la forme 
        \[ J(\lambda) = \begin{bmatrix}
            \lambda & 1 & 0 & 0 \\
            0 & \lambda & \ddots & 0 \\
            \vdots & \ddots & \ddots & 1 \\
            0 & \cdots & 0 & \lambda
        \end{bmatrix} \in \mk{p} \]   
        avec $\lambda \in \mathbb{K}$ et $p \geq 1$. 
    \end{defi}

    C’est donc une matrice triangulaire supérieure, avec des coeffictiens $\lambda$ sur la diagonale, des 1 juste au-dessus, puis des 0 encore au-dessus. 

    On peut montrer par exemple que $(J(\lambda) - \lambda I_p)^p = 0$, et que le polynôme caractéristique de $J(\lambda)$ est $(-1)^p(X - \lambda)^p$. Son polynôme minimal est $(X - \lambda)^p$.

    \begin{defi}{}{}
        Une \textbf{Matrice de Jordan} est une matrice diagonale par blocs de la forme 
        \[ \begin{bmatrix}
            J_1(\lambda_1) & 0 & \cdots & 0 \\
            0 & J_2(\lambda_2) & \ddots & \vdots \\
            \vdots & \ddots & \ddots & 0 \\
            0 & \cdots & 0 & J_r(\lambda_r) 
        \end{bmatrix} \]  
        où les $J_i(\lambda_i)$ sont des blocs de Jordan.
    \end{defi}

    Les blocs de Jordan peuvent être de tailles différentes, et les valeurs $\lambda_i \in \mathbb{K}$ sont quelconques, et peuvent être égales.

    \begin{theo}{Réduction de Jordan, version matricielle}{}
        Soit $A \in \mk{n}$ une matrice de polynôme caractéristique scindé sur $\mathbb{K}$. Alors $A$ est semblable sur $\mathbb{K}$ à une matrice de Jordan, appelée \textbf{réduite de Jordan} de $A$. Il existe donc $P \in \mk{n}$ inversible telle que 
        \[ P^{-1} A P = \diag(J_1,\ldots,J_r) \]   
        où les $J_i$ sont des blocs de Jordan
    \end{theo}

    \begin{theo}{Réduction de Jordan, version endomorphismes}{}
        Soit $f$ un endomorphisme de $E$ dont le polynôme caractéristique $\chi_f(X)$ est scindé sur $\mathbb{K}$. Il existe une base $\mathcal{B}$ de $E$ où la matrice de $f$ est de Jordan, \textit{i.e.} 
        \[ \mat{\mathcal{B}}{f} = \diag(J_1,\ldots,J_r) \]   
    \end{theo}

    Nous admettons ces théorèmes, mais nous verrons sur des exemples comment obtenir la matrice de Jordan. 

    Les $\lambda$ qui apparaissent dans les blocs de Jordan sont les valeurs propres de $A$, (ou de $f$) et donc les racines du polynôme caractéristique. Une même valeur de $\lambda$ peut apparaître dans plusieurs blocs différents. EP, ce théorème s’applique à toutes les matrices complexes. 

    Cette décomposition est unique dans le sens où le nombre est la taille des blocs de Jordan ne dépendent que de $A$ (ou de $f$). Par contre, on s’autorise à permuter les blocs de Jordan entre eux.

    Le nombre de blocs associés à la valeur propre $\lambda$ est égal à la dimension du sous-espace propre $E_{\lambda}$. La somme des tailles des blocs de Jordan associés à $\lambda$ est la multiplicité de $\lambda$ comme racine du polynôme caractéristique. La taille du plus grand bloc de Jordan associé à $\lambda$ est la multiplicité de $\lambda$ comme racine du polynôme minimal.

    \begin{omed}{Méthode}{myred}
        \begin{enumerate}
            \item Calculer le polynôme caractéristique et les valeurs propres de $A$.
            \item Pour chaque valeur propre $\lambda$, calculer le SEP $E_{\lambda} = \ker(A - \lambda I_n)$ et trouver une base de $E_{\lambda}$. Le nombres de blocs de Jordan associés à $\lambda$ est $\dim E_{\lambda}$. 
            \item Pour chaque vecteur propre de la base de $E_{\lambda}$, on construit le bloc de Jordan associé : 
            \begin{itemize}
                \item Si $v_1 \in E_{\lambda}$ est un vecteur propre de la base de $E_{\lambda}$, on cherche $v_2 \in \mathbb{K}^n$ tel que $(A - \lambda I_n) v_2 = v_1$.
                \item Puis on cherche s’il existe $v_3 \in \mathbb{K}^n$ tel que $(A - \lambda I_n) v_3 = v_2$. 
                \item On arrête le processus lorsqu’il n’y a pas de solution.
                \item On a $Av_1 = \lambda v_1$, $Av_2 = v_1 + \lambda v_2$, \ldots, $A v_p = v_{p-1} + \lambda v_p$.
                \item Donc, dans le SE engendré par ces $(v_1,\ldots,v_p)$, la matrice associée à $A$, dans cette base, est exactement le bloc de Jordan 
                \[ J(\lambda) = \begin{bmatrix}
            \lambda & 1 & 0 & 0 \\
            0 & \lambda & \ddots & 0 \\
            \vdots & \ddots & \ddots & 1 \\
            0 & \cdots & 0 & \lambda
        \end{bmatrix} \in \mk{p} \]
                \item On peut aussi savoir quand s’arrêter en utilisant le fait que le bloc de Jordan est toujours d’une taille $p$ inférieure ou égale à la multiplicité de $\lambda$ comme racine du polynôme caractéristique (et même du polynôme minimal).    
            \end{itemize}
            \item On procède ainsi pour tout SEP de $A$, et on obtient une matrice réduite de Jordan.
        \end{enumerate}
    \end{omed}

    \begin{omed}{Exemple}{myred}
        Soit 
        \[ A = \begin{bmatrix}
            4 & 3 & -2 \\
            -3 & -1 & 3 \\
            2 & 3 & 0 
        \end{bmatrix} \in \mathcal{M}_3(\mathbb{R}) \]   
        Calculons sa réduite de Jordan $J$ et une matrice de passage $P$ telle que $P^{-1} A P = J$.
        \begin{enumerate}
            \item On commence par calculer le polynôme caractéristique de $A$ : 
            \[ \chi_A(X) = \det(A - X I_3) = \begin{vmatrix}
                4 - X & 3 & -2 \\
                -3 & -1 -X & 3 \\
                2 & 3 & -X
            \end{vmatrix} = -(X + 1)(X - 2)^2 \]   
            Il y a donc deux valeurs propres $-1$ et $2$. 
            \item Valeur propre $-1$. \quad La valeur propre -1 est de multiplicité $1$. Le sous-espace propre associé $E_{-1} = \ker(A + I_3) = \left\{v \in \mathbb{R}^3, \quad A v = -v\right\}$ est donc de dimension $1$. Après calculs, on trouve que $E_{-1} = \mathbb{R} v_1$ où $v_1 = (-1, 1 ,-1)$. Comme la multiplicité de $-1$ comme racine de $\chi_{A}(X)$ est $1$, alors la valeur propre $-1$ sera juste associée à un bloc de Jordan de taille $1 \times 1$.
            \item Valeur propre $2$. \quad La valeur propre $2$ est de multiplicité $2$. Il faut déterminer le sous-espace propre associé à $E_2 = \ker(A - 2 I_3) = \left\{v \in \mathbb{R}^3, \quad A v = 2 v \right\}$. Après calculs, on trouve que $E_2 = \mathbb{R} v_2$ où $v_2 = (1,0,1)$. Comme $E_2$ est un EV de dimension $1$, alors que $2$ est de multiplicité $2$, la matrice $A$ n’est pas diagonalisable, et on sait alors que la valeur propre $2$ sera associée à un bloc de Jordan de taille $2 \times 2$. 
            \item Bloc de Jordan. \quad On cherche $v_3 \in \mathbb{R}^3$ tel que $(A - 2I_3) v_3 = v_2$. Si $v_3 = (x,y,z)$, alors 
            \begin{align*}
                (A - 2 I_3) v_3 = v_2 &\iff \begin{bmatrix}
                    2 & 3 & -2 \\
                    -3 & -3 & 3 \\
                    2 & 3 & -2
                \end{bmatrix} \begin{bmatrix}
                    x \\
                    y \\
                    z
                \end{bmatrix} = \begin{bmatrix}
                    1 \\
                    0 \\
                    1
                \end{bmatrix} \\
                &\iff \et{2x + 3y - 2z = 1}{x + y - z = 0} \\
                &\iff \et{x = -1 + z}{y = 1}
            \end{align*}
            En prenant par exemple $z = 0$, on choisit $v_3 = (-1,1,0)$.
            \item Matrice de Jordan. \quad Dans la base $(v_1,v_2,v_3)$, on a $A v_1 = - v_1$, $A v_2 = 2 v_2$ et $(A - 2 I_3) v_3 = v_2$, \textit{i.e.} $A v_3 = v_2 + 2v_3$. La matrice associée à $A$ dans la base $(v_1, v_2,v_3)$ est donc 
            \[ J= \begin{bmatrix}
                -1 & 0 & 0 \\
                0 & 2 & 1 \\
                0 & 0 & 2
            \end{bmatrix}
            \]  
            Autrement dit, $J = P^{-1} A P$ où $P$ est la matrice dont les colonnes sont les vecteurs sont $v_1, v_2,v_3$ exprimés dans la base canonique : 
            \[ P = \begin{bmatrix}
                -1 & 1 & -1 \\
                1 & 0 & 1 \\
                -1 & 1 & 0
            \end{bmatrix} \]   
        \end{enumerate}
    \end{omed}

    \begin{omed}{Applications}{myred}
        \begin{itemize}
            \item Si $A \in \mathcal{M}_n(\mathbb{C})$, alors $A$ est semblable à sa transposée $A^{T}$. En effet, il suffit de le vérifier lorsque $A$ est un bloc de Jordan.
            \item Si $N \in \mk{4}$ est nilpotente, alors $N$ est semblable à une et une seule des $5$ matrices suivantes :
            \[ \begin{bmatrix}
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0
            \end{bmatrix} \quad \begin{bmatrix}
                0 & 1 & 0 & 0 \\
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0
            \end{bmatrix} \quad \begin{bmatrix}
                0 & 1 & 0 & 0 \\
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 1 \\
                0 & 0 & 0 & 0
            \end{bmatrix} \quad \begin{bmatrix}
                0 & 1 & 0 & 0 \\
                0 & 0 & 1 & 0 \\
                0 & 0 & 0 & 0 \\
                0 & 0 & 0 & 0
            \end{bmatrix} \quad \begin{bmatrix}
                0 & 1 & 0 & 0 \\
                0 & 0 & 1 & 0 \\
                0 & 0 & 0 & 1 \\
                0 & 0 & 0 & 0
            \end{bmatrix} \]   
            Il y a une infinité de matrices nilpotentes $4 \times 4$, mais uniquement $5$ à une similitude près.
        \end{itemize}
    \end{omed}

\newpage

\section[Endom. sur un espace euclidien]{Endomorphismes sur un espace euclidien}

Un des objectifs de cette section est d’étudier les endomorphismes remarquables d’un espace euclidien, comme les isométries vectorielles et les endomorphismes autoadjoints. $\left(E, \spr{.}{.}\right)$ désignera par la suite un espace euclidien, de dimension notée $n$.

Soit $u \in \mathcal{L}(E)$. En guise de préambule, déterminons sa matrice dans une base orthonormale $\mathcal{B} = (e_1,\ldots,e_n)$. 
\[ \forall j \in \intervalleEntier{1}{n}, \quad u(e_j) = \sum_{i=1}^{n} \spr{u(e_j)}{e_i} e_i \esp{donc} M = \mat{\mathcal{B}}{u} = \begin{bmatrix}
    \spr{u(e_1)}{e_1} & \cdots & \spr{u(e_n)}{e_1} \\
    \vdots & & \vdots \\
    \spr{u(e_1)}{e_n} & \cdots & \spr{u(e_n)}{e_n} 
\end{bmatrix} \]    
La j-ème colonne de $M$ s’écrit $C_j = \left(\spr{u(e_j)}{e_i}\right)_{1 \leq i \leq n}$, et par conséquent $M^{\top} M = \left(\spr{u(e_j)}{u(e_i)}\right)_{1 \leq i,j \leq n}$.

\subsection{Adjoint d’un endomorphisme}

    Commençons par définir, dans ce contexte euclidien, l’adjoint $u^*$ d’un endomorphisme $u$. Cet endomorphisme est à $u$ ce que la matrice $M^{\top}$ est à $M$ (dans une base orthonormale). On se place donc dans un espace euclidien $\left(E,\spr{.}{.}\right)$.

    \begin{theo}{Représentation des formes linéaires}{}
        Pour toute forme linéaire $\varphi$, il existe un unique vecteur $a \in E$ tel que 
        \[ \forall x \in E, \quad \varphi(x) = \spr{a}{x} \]
    \end{theo}
    
    \begin{demo}{Preuve}{myred}
        Pour tout $a \in E$, $x \mapsto \spr{a}{x}$ est une forme linéaire. Or, l’application 
        \[ \fonction{\xi}{E}{E^*}{a}{\left[x \mapsto \spr{a}{x}\right]} \]   
        est un isomorphisme entre $E$ et son dual $E^* = \mathcal{L}(E,\mathbb{R})$. En effet, sa linéarité est immédiate, et $\dim(E) = \dim(E^*)$. Si $a \in \ker(\xi)$, cela signifie que $x \mapsto \spr{a}{x}$ est l’application nulle de $E$. Ainsi, en évaluant en $a$, on obtient que $\norm{a}^2 = 0$, \textit{i.e.} $a = 0$. Donc $\xi$ est bijective, et donc surjective, ce qui permet de conclure.
    \end{demo}

    En dimension finie, toute forme linéaire est donc issue d’un produit scalaire. Une conséquence directe de ce théorème est que toute forme linéaire sur $\mk{n}$ est de la forme $M \mapsto \tr(AM)$.

    \begin{defitheo}{Adjoint d’un endomorphisme}{}
        Soit $u \in \mathcal{L}(E)$. Il existe un unique endomorphisme $v$ de $E$ vérifiant 
        \[ \forall x, y \in E, \quad \spr{u(x)}{y} = \spr{x}{v(y)} \]   
        Un tel endomorphisme est appelé adjoint de $u$, et est noté $u^*$.
    \end{defitheo}

    \begin{demo}{Preuve}{mypurple}
        \begin{itemize}
            \item \textbf{Existence et unicité} \quad L’application $x \mapsto \spr{u(x)}{y}$ est une forme linéaire. D’après le théorème de représentation, il existe un unique vecteur $a_y \in E$ tel que 
            \[ \forall x \in E, \quad \spr{u(x)}{y} = \spr{x}{a_y} \]   
            On pose alors $u^*(y) = a_y$, et donc $\forall x,y \in E, \spr{u(x)}{y} = \spr{x}{u^*(y)}$.
            \item \textbf{Linéarité} \quad Soient $y_1,y_2 \in E$ et $\lambda \in \mathbb{R}$. Alors, par bilinéairité du produit scalaire, $\forall x \in E$, 
            \begin{align*}
                \spr{u(x)}{\lambda y_1 + y_2} 
                &\overset{i}{=} \spr{x}{u^*(\lambda y_1 + y_2)} \\
                &\overset{ii}{=} \lambda \spr{x}{\lambda_1} + \spr{x}{\lambda_2} \\
                &= \lambda \spr{x}{u^*(\lambda_1)} + \spr{x}{u^*(\lambda_2)} \\
                &= \spr{x}{\lambda u^*(\lambda_1) + u^*(\lambda_2)} 
            \end{align*}
            Ainsi, $\spr{x}{u^*(\lambda y_1 + y_2) - \lambda u^*(\lambda_1) - u^*(\lambda_2)} = 0$. Or le seul vecteur orthogonal à tous les autres est nul, d’où la linéarité.
        \end{itemize}
    \end{demo}

    \begin{prop}{}{}
        \begin{enumerate}
            \item L’application $u \mapsto u^*$ est linéaire.
            \item Pour tous $u,v \in \mathcal{L}(E), (u \circ v)^* = v^* \circ u^*$.
            \item Le passage à l’adjoint est involutif : pour tous $u \in \mathcal{L}(E)$, $(u^*)^* = u$.
        \end{enumerate}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item Soient $u,v \in \mathcal{L}(E)$ et $\lambda \in \mathbb{R}$. Alors pour tous $x,y \in E$, 
            \[ \spr{(\lambda x + y)(x)}{y} = \lambda \spr{u(x)}{y} + \spr{v(x)}{y} = \lambda\spr{x}{u^*(y)} + \spr{x}{v^*(y)} = \spr{x}{(\lambda u^* + v^*)(y)} \]   
            Par unicité de l’adjoint $(\lambda u + v)^* = \lambda u^* + v^*$.
            \item Soient $u,v \in \mathcal{L}(E)$. Alors pour tous $x,y \in E$, 
            \[ \spr{u(v(y))}{y} = \spr{v(x)}{u^*(y)} = \spr{x}{v^*(u^*(y))} \]   
            Toujours par unicité, $(u \circ v)^* = v^* \circ u^*$.
            \item Enfin, pour tous $x,y \in E$, $\spr{u(x)}{y} = \spr{u^*(y)}{x} = \spr{y}{(u^*)^*(x)}$. D’où l’égalité $(u^*)^* = u$.
        \end{enumerate}
    \end{demo}

    Exposons un résultat dont on peut soupçonner l’intérêt majeur en matière de réduction.

    \begin{prop}{}{}
        Soit $F$ un sous-espace vectoriel de $E$ stable par $u$. Alors $F^{\perp}$ est stable par $u^*$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $y \in F^{\perp}$. Montrons que $u^*(y) \in F^{\perp}$. Pour tout $x \in E$, $\spr{u^*(y)}{x} = \spr{y}{u(x)} = 0$ car $x \in F$ donc $u(x) = F$.
    \end{demo}

    \begin{prop}{Matrice de l’adjoint dans une base orthonormale}{}
        Soit $\mathcal{B}$ une base orthonormale de $E$. On pose $M = \mat{\mathcal{B}}{u}$. Alors $M' = \mat{\mathcal{B}}{u^*} = M^{\top}$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Notons $\mathcal{B} = (e_1,\ldots,e_n)$. D’après le préambule, $m_{i,j} = \spr{u(e_j)}{e_i}$ et $m_{i,j}' = \spr{u^*(e_j)}{e_i} = \spr{u(e_i)}{e_j}$ pour tous $i,j \in \intervalleEntier{1}{n}$. Il en découle directement $M' = M^{\perp}$.
    \end{demo}

    On déduit de cette proposition que les endomophismes $u$ et $u^*$ ont même rang, même déterminant, même trace et même polynôme caractéristique.

\subsection{Isométries vectorielles et matrices orthogonales}

    \subsubsection{Matrices orthogonales}

    Dans cette SSSection, on considère une matrice $A \in \mathcal{M}_n(\mathbb{R})$

    \begin{defi}{Matrice orthogonale}{}
        On dit que $A$ est une \textbf{matrice orthoonale} si 
        \[ A^{\top} A = I_n \]   
        On note $\mathcal{O}_{n}(\mathbb{R})$ l’ensemble des matrices orthogonales réelles de taille $n$. 
    \end{defi}

    \begin{omed}{Exemples}{myyellow}
        \begin{enumerate}[label = \textcolor{myyellow}{\arabic*.}]
            \item $A = \begin{bmatrix}
                0 & 0 & 1 \\
                0 & 1 & 0 \\
                1 & 0 & 0
            \end{bmatrix}$ qui est telle que $A = A^{\top}$ et $A^2 = I_3$.
            \item $A = \frac{1}{9} \begin{bmatrix}
                8 & 1 & -4 \\
                -4 & 4 & -7 \\
                1 & 8 & 4
            \end{bmatrix} \in \mathcal{O}_3(\mathbb{R})$.
        \end{enumerate}
    \end{omed}

    \begin{prop}{}{}
        Les affirmations suivantes sont équivalentes :
        \begin{enumerate}
            \item $A \in \mathcal{O}_n(\mathbb{R})$.
            \item Les colonnes de $A$ forment une BON de $\mathcal{M}_{n,1}(\mathbb{R})$ muni du SPR canonique.
            \item $A A^{\top} = I_n$.
            \item Les lignes de $A$ forment une BON pareil.
        \end{enumerate}
    \end{prop}

    \begin{demo}{Démonstration}{myolive} 
        \begin{itemize}[leftmargin=2cm]
            \item[\textbf{(i)} $\iff$ \textbf{(iii)}] \begin{align*}
                A \in \mathcal{O}_n(\mathbb{R})
                &\iff A^{\top} A = I_n \\
                &\iff A \text{ inversible avec } A^{-1} = A^{\top} \\
                &\iff A A^{\top} = I_n
            \end{align*}
            \item[\textbf{(i)} $\iff$ \textbf{(ii)}] Posons $C_1,\ldots,C_n$ les colonnes de $A$ et $A = (a_{i,j})$. 
            \begin{align*}
                A^{\top} A = I_n 
                &\iff \forall (i,j) \in \intervalleEntier{1}{n}^2, \quad  \left[A^{\top} A\right]_{i,j} \\
                &\iff \forall (i,j) \in \intervalleEntier{1}{n}^n, \quad  \sum_{k=1}^{n} a_{k,i} a_{k,j} = \delta_{i,j} \\
                &\iff \forall (i,j) \in \intervalleEntier{1}{n}^n, \quad \spr{C_i}{C_j} = \delta_{i,j} \\
                &\iff (C_1,\ldots,C_n) \text{ est une BON de } \mathcal{M}_{n,1}(\mathbb{R})
            \end{align*}
            \item[\textbf{(i)} $\iff$ \textbf{(iv)}] Se calcule de la même façon. 
        \end{itemize}
    \end{demo}

    \begin{defitheo}{Groupe orthogonal d’ordre $n$}{}
        $\mathcal{O}_n(\mathbb{R})$ est un sous-groupe de $\GL_n(\mathbb{R})$, dit \textbf{groupe orthogonal d’ordre $n$}. 
    \end{defitheo}

    \begin{demo}{Démonstration}{mypurple}
        Soient $A,B \in \mathcal{O}_n(\mathbb{R})$, alors 
        \begin{align*}
            (AB)^{\top} AB 
            &= B^{\top} A^{\top} A B \\
            &= B^{\top} B \\
            &= I_n
        \end{align*}
        De plus, $I_n \in \mathcal{O}_n(\mathbb{R})$. Comme $A \in \GL_n(\mathbb{R})$, $A^{-1}$ existe et 
        \begin{align*}
            (A^{-1})^{\top} A^{-1} 
            &= (A^{\top} A)^{-1} \\
            &= I_n
        \end{align*}
    \end{demo}

    \begin{prop}{}{}
        Soit $A \in \mathcal{O}_n(\mathbb{R})$, alors $\det(A) = \pm 1$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        $A^{\top} A = I_n$, donc 
        \begin{align*}
            \det(A^{\top} A)
            &= \det(A^{\top}) \det(A) \\
            &= \det(A)^2 \\
            &= 1
        \end{align*}
    \end{demo}

    \begin{defitheo}{Groupe spécial orthogonal}{}
        On appelle groupe spécial orthogonal l’ensemble $\SO_n(\mathbb{R})$ défini par les matrices orthogonales de déterminant $1$, qui est un sous-groupe de $\mathcal{O}_n(\mathbb{R})$.
    \end{defitheo}

    On peut directement remarquer que l’ensemble des matrices orthogonales de déterminant $-1$ n’est pas stable par produit, donc ne constitue pas un sous-groupe de $\mathcal{O}_n(\mathbb{R})$.

    \begin{demo}{Preuve}{mypurple}
        Si $A, B \in \SO_n(\mathbb{R})$, alors $AB \in \mathcal{O}_n(\mathbb{R})$ et $\det(AB) = \det(A)\det(B) = 1$ d’où $AB \in \SO_n(\mathbb{R})$. De plus, $\SO_n(\mathbb{R}) \subset \mathcal{O}_n(\mathbb{R})$. Finalement, si $\det(A^{-1}) = \det(A^{\top}) = \det(A) = 1$ donc $A^{-1} \in \SO_n(\mathbb{R})$.
    \end{demo}

    \subsubsection{Isométrie vectorielle ou endomorphisme orthogonal}

    Dans tout ce paragraphe, $E$ désigne un espace vectoriel euclidien, et $f$ un endomorphisme de $E$. 

    \begin{defi}{Endomorphisme orthogonal}{}
        On dit que $f$ est un \textbf{endomorphisme orthogonal} s’il conserve le produit scalaire, \textit{i.e.} 
        \[  \forall x,y \in E, \quad \spr{f(x)}{f(y)} = \spr{x}{y} \]   
        On note $\mathcal{O}(E)$ l’ensemble des endomorphismes symétriques de $E$.
    \end{defi}

    \begin{defi}{Isométrie vectorielle}{}
        On dit que $f$ est une \textbf{isométrie vectorielle} si 
        \[ \forall x \in E, \quad \norm{f(x)} = \norm{x} \]   
    \end{defi}

    \begin{prop}{Lien entre les endomorphismes orthogonaux et les isométries vectorielles}{}
        Il y a équivalence entre $f \in \mathcal{O}(E)$ et $f$ est une isométrie vectorielle.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item[$\implies$] Si $f \in \mathcal{O}(E)$, on a pour tout $x \in E$ que $\norm{f(x)} = \sqrt{\spr{f(x)}{f(x)}} = \sqrt{\spr{x}{x}} = \norm{x}$.
            \item[$\impliedby$] Si $f$ est une isométrie, d’après la formule de \textit{polarisation}, 
            \begin{align*}
                \spr{f(x)}{f(y)} 
                &= \frac{1}{4} \left(\norm{f(x) + f(y)}^2 - \norm{f(x)-f(y)}^2\right) \\
                &= \frac{1}{4} \left(\norm{f(x + y)}^2 - \norm{f(x-y)}^2\right) \\
                &= \frac{1}{4} \left(\norm{x+y}^2 - \norm{x-y}^2\right) \\
                &= \spr{x}{y} 
            \end{align*}
        \end{itemize}
    \end{demo}

    \begin{prop}{}{}
        Les affirmations suivantes sont équivalentes :
        \begin{enumerate}
            \item $f$ est un endomorphisme orthogonal.
            \item Il existe une base orthonormée $B$ de $E$ telle que $f(B)$ est une base orthonormée de $E$.
            \item Pour toute base orthonormée $B$ de $E$, $f(B)$ est une base orthonormée de $E$.
        \end{enumerate}
    \end{prop}

    \begin{demo}{Démonstration tournante}{myolive}
        \begin{itemize}[leftmargin=2cm]
            \item[\textbf{(iii)} $\implies$ \textbf{(ii)}] est clair car $E$ possède toujours une BON.
            \item[\textbf{(i)} $\implies$ \textbf{(iii)}] Soient $f \in \mathcal{O}(E)$ et $B = \left(e_1,\ldots,e_n\right)$ une BON de $E$. Pour tout $(i,j) \in \intervalleEntier{1}{n}^2$, $\spr{f(e_i)}{f(e_j)} = \spr{e_i}{e_j} = \delta_{i,j}$ donc $f(B)$ est une BON de $E$.
            \item[\textbf{(ii)} $\implies$ \textbf{(i)}] Soit $B = (e_1,\ldots,e_n)$ une BON de $E$ telle que $f(B) = \left(f(e_1), \ldots, f(e_n)\right)$ est une BON de $E$ pareil. Soient $x,y \in E$ tels que $x = \sum_{i=1}^{n} x_i e_i$ et $y = \sum_{j=1}^{n} y_j e_j$. Alors on a 
            \begin{align*}
                \spr{f(x)}{f(y)} 
                &= \spr{\sum_{i=1}^{n} x_i f(e_i)}{\sum_{j=1}^{n} y_j f(e_j)} \\
                &= \sum_{i=1}^{n} \sum_{j=1}^{n} x_i y_j \spr{f(e_i)}{f(e_j)} \\
                &= \sum_{k=1}^{n} x_k y_k \\
                &= \spr{x}{y}
            \end{align*}
        \end{itemize}
    \end{demo}

    \begin{prop}{Groupe orthogonal}{}
        L’ensemble $\mathcal{O}(E)$ est un sous-groupe de $\mathcal{L}(E)$ pour la loi $\circ$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item Si $f,g \in \mathcal{O}(E)$, on remarque que $\spr{(f \circ g)(x)}{(f \circ g)(y)} = \spr{g(x)}{g(y)} = \spr{x}{y}$. Donc $\circ$ est interne. 
            \item De plus, elle est associative, l’élément neutre est $\id$.
            \item Enfin, l’image d’une base est une base, donc $f$ est un isomorphisme, et $f^{-1}$ existe. $\spr{x}{y} = \spr{(f \circ f^{-1})(x)}{(f \circ f^{-1})(y)} = \spr{f^{-1}(x)}{f^{-1}(y)}$.
        \end{itemize}
    \end{demo}

    \begin{prop}{}{}
        Soit $f \in \mathcal{O}(E)$ et $F$ un SEV de $E$ stable par $f$. Alors $F^{\perp}$ est stable par $f$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $y \in F$ et $x \in F^{\perp}$. On a $\spr{f(x)}{y} = \spr{x}{f^{-1}(y)} = 0$ car $f^{-1}(y) \in F$ -- Cela vient du fait que la bijectivité de $f$ fait de $F$ un SEV stable par $f^{-1}$--. 
    \end{demo}

    \begin{omed}{Exemple \textcolor{black}{(Symétries orthogonales)}}{myolive}
        Soit $s$ une SO de $E$. On suppose que $s$ est la symétrie par rapport à $F$. Soient $x,y \in E$. On sait que $E = F \oplus F^{\perp}$, donc il existe $(x_1,x_2), (y_1,y_2) \in F \times F^{\perp}$ tels que $x = x_1 + x_2$ et $y = y_1 + y_2$. 
        \begin{align*}
            \spr{s(x)}{s(y)}
            &= \spr{x_1 - x_2}{y_1 - y_2} \\
            &= \spr{x_1}{y_1} - \underbrace{\spr{x_1}{y_2}}_{=0} - \underbrace{\spr{x_2}{y_1}}_{=0} + \spr{x_2}{y_2} \\
            &= \spr{x_1}{y_1} + \spr{x_1}{y_2} + \spr{x_2}{y_1} + \spr{x_2}{y_2} \\
            &= \spr{x}{y}
        \end{align*}
        donc $s \in \mathcal{O}(E)$.
    \end{omed}

    Il faut cependant être vigilant, car cela n’est généralement pas le cas pour une projection orthogonale. En effet, en gardant les notations précédentes, et en introduisant $p$ la PO sur $F$ parallèlement à $F^{\perp}$, on a 
    \begin{align*}
        \spr{p(x)}{p(y)} 
        &= \spr{x_1}{y_1} \\
        &= \spr{x}{y} - \spr{x_2}{y_2} \\
        &\neq \spr{x}{y} \quad \textit{a priori}
    \end{align*}

    \begin{theo}{Transition}{}
        Soit $B$ une BON de $E$. 
        \[ f \in \mathcal{O}(E) \iff \mat{B}{f} \in \mathcal{O}_n(\mathbb{R}) \]
    \end{theo}

    \begin{demo}{Preuve}{myred}
        Posons $B = (e_1,\ldots,e_n)$ et $A = \mat{B}{f} = (a_{i,j})$. 
        \begin{align*}
            f \in \mathcal{O}(E) 
            &\textit{ssi} f(B) \text{ est une BON de } E \\
            &\textit{ssi} \forall i,j \in \intervalleEntier{1}{n}, \quad \spr{f(e_i)}{f(e_j)} = \delta_{i,j} \\
            &\textit{ssi} \forall i,j \in \intervalleEntier{1}{n}, \quad \spr{\sum_{k=1}^n a_{k,i}e_i}{\sum_{k=1}^{n}a_{k,j}e_j} \\
            &\textit{ssi} \textit{ssi} \forall i,j \in \intervalleEntier{1}{n}, \quad \sum_{k=1}^{n} a_{k,i} a_{k,j} = \delta_{i,j} \\
            &\textit{ssi} A \in \mathcal{O}_n(\mathbb{R})
        \end{align*}
    \end{demo}

    \begin{prop}{}{}
        Soient $B$ une BON de $E$ et $B'$ une base de $E$, $P$ la matrice de passage de $B$ à $B'$. Alors $B'$ est une BON de $E$ \textit{ssi} $P \in \mathcal{O}_n(\mathbb{R})$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $f \in \mathcal{L}(E)$ définie par $f(B) = B'$. On sait que $P = \mat{B}{f}$. Comme $f \in \mathcal{O}(E)$ \textit{ssi} $B'$ est une BON de $E$, le résultat est clair.
    \end{demo}

    \subsubsection{Orientation de l’espace, produit mixte et produit vectoriel}

    Soient $\mathcal{B}$ et $\mathcal{B}'$ deux bases orthonormales et $P$ la matrice de passage de $\mathcal{B}$ à $\mathcal{B}'$. $P \in \O_n(\mathbb{R})$ donc $\det(P) = \pm 1$.

    \begin{defitheo}{Orientation de l’espace}{}
        On dit que $\mathcal{B}$ et $\mathcal{B}'$ définissent la \textbf{même orientation} \textit{ssi} $\det(P) = 1$. On définit ainsi une relation d’équivalence sur l’ensemble des BON de $E$.

        Orienter l’espace consiste à choisir arbitrairement une base orthonormale de $E$. Toutes celles qui définissent la même orientation seront dites \textbf{directes}, les autres \textbf{indirectes}.
    \end{defitheo}

    \begin{demo}{Justification}{mypurple}
        \begin{itemize}
            \item \textbf{Réflexivité} \quad Si $\mathcal{B} = \mathcal{B}'$, $P = I_n$ et $\det(P) = 1$.
            \item \textbf{Symétrie} \quad Si $P \in \SO_n(\mathbb{R})$, $P^{-1} \in \SO_n(\mathbb{R})$.
            \item \textbf{Transitivité} \quad Si $P, Q$ sont les matrices de passages de $\mathcal{B}$ à $\mathcal{B}'$ puis à $\mathcal{B}''$ -- où les bases sont toutes de même sens --, alors la matrice de passage de $\mathcal{B}$ à $\mathcal{B}''$ est $PQ$, et $\det(PQ) = \det(P)\det(Q) = 1$.
        \end{itemize}
    \end{demo}

    \begin{omed}{Exemple}{mypurple}
        Par convention, les bases directes de $\mathbb{R}^3$ sont celles qui respectent la règle du tire-bouchon.
    \end{omed}

    Soient $\left(E,\spr{.}{.}\right)$ un espace euclidien \textbf{\textsc{orienté}} et $\mathcal{B}$ une base orthonormale \textbf{\textsc{directe}} de $E$.

    \begin{defitheo}{Produit mixte}{}
        Soient $x_1,\ldots,x_n \in E$. On appelle \textbf{produit mixte} de la famille $(x_1,\ldots,x_n)$ le scalaire 
        \[ \det(x_1,\ldots,x_n) := \det_{\mathcal{B}}(x_1,\ldots,x_n) ::= [x_1,\ldots,x_n] \]
        Il est indépendant de la base orthonormale directe choisie.
    \end{defitheo}

    \begin{demo}{Justification}{mypurple}
        Soit $\mathcal{B}'$ une base orthonormale directe de $E$. Alors 
        \[ \det_{\mathcal{B}'}(x_1,\ldots,x_n) = \det_{\mathcal{B}'}(\mathcal{B})\det_{\mathcal{B}}(x_1,\ldots,x_n) = [x_1,\ldots,x_n] \]   
        car $\det_{\mathcal{B}'}(\mathcal{B}) = \det(P) = 1$.
    \end{demo}

    \begin{omed}{Remarques}{mypurple}
        \begin{itemize}[label=\textcolor{mypurple}{$\star$}]
            \item Dans une base orthonormale \textbf{\textsc{indirecte}} $\mathcal{B}_-$ de $E$, on a $\det_{\mathcal{B}_-}(x_1,\ldots,x_n) = - [x_1,\ldots,x_n]$. En réalité, $\det(P) = \varepsilon(\sigma)$, où $\mathcal{B}' = (x_{\sigma(1)}, \ldots, x_{\sigma(n)})$, et la valeur de la signature permet de confirmer le sens de $\mathcal{B}'$.
            \item Le produit mixte s’interprète géométriquement 
            \begin{itemize}
                \item dans $\mathbb{R}^2$, $\left[\vect{x},\vect{y}\right]$ est l’aire algébrique du parallélogramme engendré par les deux vecteurs ;
                \item dans $\mathbb{R}^3$, $\left[\vect{x},\vect{y}, \vect{z}\right]$ est le volume algébrique du parallélépipède engendré par les trois vecteurs.
            \end{itemize}
        \end{itemize}
    \end{omed}

    \begin{omed}{Exercice}{mypurple}
        Soit $E$ un EVE, $B$ une BON de $E$, $f \in \mathcal{L}(E)$ et $A = \mat{B}{f}$. Montrer que les affirmations suivantes sont équivalentes :
        \begin{enumerate}
            \item $f$ est une symétrie orthogonale ;
            \item $f \in \mathcal{O}(E)$ et $f$ est une symétrie ;
            \item $A \in \O_n(\mathbb{R})$ et $A^{\top} = A$ ;
            \item il existe $B'$ une BON de $E$ telle que $\mat{B'}{f}$ est diagonale, de coefficient diagonaux $\pm 1$.
        \end{enumerate}
    \end{omed}

    \begin{demo}{Résolution}{mypurple}
        \begin{itemize}[leftmargin=3cm]
            \item[\textbf{(i)} $\implies$ \textbf{(ii)}] a déjà été montré.
            \item[\textbf{(ii)} $\implies$ \textbf{(i)}] Soit $f \in \O(E)$ et $f$ une symétrie. C’est la symétrie par rapport à $\ker(f - \id)$ parallèment à $\ker(f + \id)$. Or on sait que $E = \ker(f - \id) \oplus \ker(f + \id)$. Soit $x \in \ker(f - \id)$ et $y \in \ker(f + \id)$, on a 
            \begin{align*}
                \spr{x}{y}
                &= \spr{f(x)}{-f(y)} \\
                &= - \spr{f(x)}{f(y)} \\
                &= - \spr{x}{y}
            \end{align*}
            D’où $\spr{x}{y} = 0$, puis $\ker(f + \id) = \ker(f - \id)^{\perp}$ et $f$ est une symétrie orthogonale.
            \item[\textbf{(i)} $\implies$ \textbf{(iv)}] Si $f$ est la symétrie par rapport à $F$, on a $E = F \oplus F^{\perp}$. Alors si $B'$ une base adaptée à cette décomposition, $\mat{B'}{f} = \begin{bmatrix}
                I_{\dim(F)} & 0 \\
                0 & -I_{\dim(F^{\perp})}
            \end{bmatrix}$.
            \item[\textbf{(iv)} $\implies$ \textbf{(iii)}] Soit $B'$ une BON telle que $\mat{B'}{f} = \diag(\pm 1) := \Delta$ et $P$ la matrice de passage de $B$ à $B'$. Alors $P \in \O_n$ et $A = P \Delta P^{\top}$, donc 
            \begin{align*}
                A^{\top} A 
                &= (P \Delta P^{\top})^{\top}(P \Delta P^{\top}) \\
                &= P \Delta^{\top} P^{\top} P \Delta P^{\top} \\
                &= P \Delta^2 P^{\top} \\
                &= P P^{\top} = I_n
            \end{align*}
            De plus, comme $\Delta^{\top} = \Delta$, on a $A^{\top} = A$.
            \item[\textbf{(iii)} $\implies$ \textbf{(ii)}] Supposons $A \in \O_n(\mathbb{R})$ et $A^{\top} = A$. L’endomorphisme canoniquement associé à $A$ -- $f$ -- est donc orthogonal, et $f$ est une symétrie \textit{ssi} $f^{-1} = f$ \textit{ssi} $A^{-1} = A$ \textit{ssi} $A^{\top} = A$.
        \end{itemize}
    \end{demo}

    On considère désormais un espace $E$ orienté et de dimension $3$.

    \begin{defitheo}{Produit vectoriel}{}
        Soient $x,y \in E$. 

        L’application $z \mapsto [x,y,z]$ étant une forme linéaire sur $E$, il existe un unique vecteur de $E$, noté $x \wedge y$ tel que 
        \[ \forall z \in E, \quad [x,y,z] = \spr{x \wedge y}{z} \]   
        On appelle \textbf{produit vectoriel} de $x$ et $y$ ce vecteur.
    \end{defitheo}

    \begin{omed}{Remarques}{mypurple}
        \begin{enumerate}[label=\textcolor{mypurple}{\arabic*.}]
            \item L’application $(x,y) \mapsto x \wedge y$ est antisymétrique : pour tous $x,y \in E, \quad x \wedge y = - y \wedge x$. Cela vient le l’antisymétrie du déterminant, qui définit le produit mixte.
            \item Si l’on note $(x_1,x_2,x_3)$ et $(y_1,y_2,y_3)$ les coordonnées respectives de $x$ et $y$ dans une base orthonormale directe de $E$, la relation $\det(x,y,z) = \spr{x \wedge y}{z}$ nous permet de vérifier que 
            \[ x \wedge y = \left[\begin{tblr}{c}
                x_2 y_3 - x_3 y_2 \\
                x_3 y_1 - x_1 y_3 \\
                x_1 y_2 - x_2 y_1
            \end{tblr}\right] \]
        \end{enumerate}
    \end{omed}

    \begin{prop}{Propriétés du produit vectoriel}{}
        Soient $x,y,z \in E$.

        \begin{alors}
            \item $x \wedge y \perp x$ et $x \wedge y \perp y$.
            \item $x \wedge y = 0_E$ \textit{ssi} $(x,y)$ est liée.
            \item Si $x$ et $y$ ne sont pas colinéaires, $(x,y, x \wedge y)$ est une base directe de $E$.
            \item Si $(x,y)$ est une famille orthonormale, $x \wedge y$ est l’unique vecteur tel que $(x,y, x \wedge y)$ est une base orthonormale directe de $E$.
            \item \textbf{Identité de Lagrange}
            \[ \spr{x}{y}^2 + \norm{x \wedge y}^2 = \norm{x}^2 \norm{y}^2 \]   
            \item \textbf{Double produit vectoriel} 
            \[ x \wedge (y \wedge z) = \spr{x}{z} y - \spr{x}{y} z \]
        \end{alors}
    \end{prop}

    \subsubsection{Isométries vectoriels en dimension 2}

    \begin{defi}{Rotation et symétrie}{}
        Soient $\theta, \varphi \mathbb{R}$. On pose 
        \[ R_{\theta} = \begin{bmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{bmatrix} \esp{et} S_{\varphi} = \begin{bmatrix}
            \cos(\varphi) & \sin(\varphi) \\
            \sin(\varphi) & -\cos(\varphi)
        \end{bmatrix} \]   
    \end{defi}

    \begin{prop}{Groupes orthogonaux d’ordre $2$}{}
        $\SO_2(\mathbb{R}) = \enstq{R_{\theta}}{\theta \in \mathbb{R}} \esp{et} \O_2(\mathbb{R}) = \enstq{R_{\theta}}{\theta \in \mathbb{R}} \cup \enstq{S_{\varphi}}{\varphi \in \mathbb{R}}$
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $A = \begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}$.
        \[ A \in \O_2(\mathbb{R}) \iff \left\{ \begin{array}{l}
            a^2 + c^2 = 1 \\
            b^2 + d^2 = 1 \\
            ab + cd = 0
        \end{array} \right. \] 
        Des deux premières équations, on déduit qu’il existe $\theta, \varphi$ tels que $a = \cos(\theta), c = \sin(\theta), b = \sin(\varphi), d = \cos(\varphi)$. En remplaçant dans la dernière équation, on obtient que $\sin(\theta + \varphi) = 0$, \textit{i.e.} $\eqmodulo{\theta}{-\varphi}{\pi}$.
        \begin{itemize}
            \item Si $\theta = -\varphi$, alors $d = \cos(\theta) = a$ et $b = \sin(-\theta) = -c$, et $A = R_{\theta}$. De plus, $\det(R_{\theta}) = 1$ donc $A \in \O_2(\mathbb{R})$.
            \item Sinon, $\theta = - \varphi + \pi$, et alors $a = \cos(-\varphi + \pi) = - \cos(\varphi) = -d$ et $c = \sin(-\varphi + \pi) = \sin(\varphi) = b$, d’où $A = S_{\varphi}$. De plus, $\det(S_{\varphi}) = -1$, donc $A \notin \O_2(\mathbb{R})$.
        \end{itemize}
    \end{demo}

    On peut interpréter géométriquement ces résultats, dans le plan rapporté au RON $R = (O, \vct{\imath}, \vct{\jmath})$. Soit $B = (\vct{\imath}, \vct{\jmath})$.
    \begin{itemize}
        \item Soit $f$ telle que $\mat{B}{f} = R_{\theta}$. Alors 
        \[ \et{f(\vct{\imath}) = \cos(\theta) \vct{\imath} + \sin(\theta) \vct{\jmath}}{f(\vct{\jmath}) = -\sin(\theta) \vct{\imath} + \cos(\theta) \vct{\jmath} = \cos(\theta + \pi /2 2) \vct{\imath} + \sin(\theta + \pi / 2) \vct{\jmath} } \]   
        Ainsi, $f$ est la rotation d’angle $\theta$ centrée sur l’origine -- ce qui est obligatoire pour une application linéaire puisque $f(0) = 0$ --.
        \item Soit $f$ telle que $\mat{B}{f} = S_{\varphi}$. $S_{\varphi}^{\top} = S_{\varphi} \in \O_2(\mathbb{R})$, donc $f$ est une symétrie orthogonale. Déterminons $\ker(f - \id)$, en résolvant $S_{\varphi} X = X$ où $X = \begin{bmatrix}
            x \\
            y
        \end{bmatrix}$. 
        \begin{align*}
            S_{\varphi} X = X 
            &\iff \et{\cos(\varphi) x + \sin(\varphi) y = x}{\sin(\varphi) x - \cos(\varphi) y = y} \\
            &\iff \et{(\cos(\varphi)-1)x + \sin(\varphi)y = 0}{\sin(\varphi)x - (1 + \cos(\varphi)y = 0)} \\
            &\iff \et{-2 \sin^2(\varphi / 2) x + 2 \sin(\varphi/2) \cos(\varphi/2) y = 0}{2 \sin(\varphi / 2)\cos(\varphi / 2)x - 2 \cos^2(\varphi/ 2)y = 0} \\
            &\iff \et{2 \sin(\varphi/2)(-\sin(\varphi/2)x + \cos(\varphi/2)y)  = 0}{2 \cos(\varphi/2)(\sin(\varphi/2)x - \cos(\varphi/2)y) = 0}
            &\iff \sin(\varphi/2)x - \cos(\varphi/2)y = 0
        \end{align*}
        $X$ est donc dans une droite vectorielle, de vecteur directeur $\vct{u} = \begin{bmatrix}
            \cos(\varphi/2) \\
            \sin(\varphi/2)
        \end{bmatrix}$. Ainsi, $f$ est la symétrie par rapport à la droite linéaire d’angle $\varphi / 2$ avec l’axe des abcisses.
    \end{itemize}

    \begin{prop}{}{}
        Soit $\theta, \varphi \in \mathbb{R}$ et $\theta', \varphi' \in \mathbb{R}$. Alors 
        \[ R_{\theta}R_{\theta'} = R_{\theta + \theta'} \esp{,} S_{\varphi} S_{\varphi'} = R_{\varphi - \varphi'} \esp{et} R_{\theta} S_{\varphi} = S_{\theta + \varphi} \]
    \end{prop}
    
    \begin{demo}{Idée de preuve}{myolive}
        Faire les calculs.
    \end{demo}

\subsection{Endomorphismes autoadjoints et matrices symétriques réelles}

    On se place toujours dans un espace vectoriel euclidien $(E,\spr{.}{.})$, et on pose $f \in \mathcal{L}(E)$.

    \subsubsection{Endomorphismes autoadjoints}

    \begin{defi}{Endomorphismes autoadjoints}{}
        $f$ est un \textbf{endomorphisme autoadjoint} -- ou \textbf{endomorphisme symétrique} --, si $(f^*)^* = f$, \textit{i.e.} si 
        \[ \forall (x,y) \in E^2, \quad \spr{f(x)}{y} = \spr{x}{f(y)} \]   
    \end{defi}

    En particulier, les projections et symétries orthogonales sont de tels endomorphismes :

    Si $f$ est une symétrie orthogonale par rapport à $F$ et $(x,y) \in E^2$, où $x = x_1 + x_2a$ et $y = y_1 + y_2$ avec $x_1,y_1 \in F$ et $x_2, y_2 \in F^{\perp}$, alors \begin{align*}
        \spr{f(x)}{y} &= \spr{x_1 - x_2}{y_1 + y_2} \\
        &= \spr{x_1}{y_1} - \spr{x_2}{y_2} \\
        \spr{x}{f(y)} &= \spr{x_1 + x_2}{y_1 - y_2} \\
        &= \spr{x_1}{y_1} - \spr{x_2}{y_2} \\
    \end{align*}

    \begin{prop}{}{}
        Si $B$ est une BON de $E$, alors $f$ est autoadjoint \textit{ssi} $\mat{B}{f}$ est une matrice symétrique réelle.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Posons $B = (e_1,\ldots,e_n)$ et $A = (a_{i,j}) = \mat{B}{f}$. Posons $j \in \intervalleEntier{1}{n}$. On a $f(e_j) = \sum_{i=1}^{n} a_{i,j}e_i$. Comme $B$ est une BON, $a_{i,j} = \spr{f(e_j)}{e_i}$. On en déduit, si $f$ est autoadjoint, que pour tout $i,j \in \intervalleEntier{1}{n}$, 
        \begin{align*}
            a_{i,j} 
            &= \spr{f(e_j)}{e_i} \\
            &= \spr{e_j}{f(e_i)} \\
            &= a_{j,i}
        \end{align*}
        Réciproquement, si $A$ est symétrique, en posant $x = \sum_{i=1}^{n} x_i e_i, y = \sum_{i=1}^{n} y_i e_i \in E$, on a 
        \begin{align*}
            \spr{f(x)}{y} 
            &= \sum_{i=1}^{n} \sum_{j=1}^{n} x_i y_j \spr{f(e_i)}{e_j} \\
            &= \sum_{i=1}^{n} \sum_{j=1}^{n} x_i y_j \spr{e_i}{f(e_j)} \\
            &= \spr{x}{f(y)}
        \end{align*}
    \end{demo}

    \subsubsection{Théorème spectral}

    \begin{theo}{Théorème spectral pour les endomorphismes}
        Soit $E$ un EVE et $f \in \mathcal{L}(E)$, autoadjoint. Alors il existe une BON de $E$ formée des vecteurs propres de $f$. On dit que $f$ est diagonalisable dans une BON.
    \end{theo}

    Pour démontrer ce théorème, introduisons tout d’abord deux résultats intermédiaires.

    \begin{lem}{}{}
        Un endomorphisme autoadjoint admet toujours au moins une valeur propre réelle dans un EVE.
    \end{lem}

    \begin{demo}{Preuve}{mybrown}
        Soit $B$ une BON de $E$ et $A = \mat{B}{f}$. Alors $A$ est symétrique. Soit $\lambda$ une racine complexe de $A$ -- qui existe assurément par le théorème de Gauss --. Il existe $X  = \begin{bmatrix}
        x_1 \\
        \vdots \\
        x_n
        \end{bmatrix}\in \mathcal{M}_{n,1}(\mathbb{C})$ tel que $AX = \lambda X$. La matrice $A$ est réelle, donc $\ovl{A} = \ovl{A}$. Ainsi, $\ovl{AX} = A \ovl{X}$. On a,
        \begin{align*}
            \ovl{X}^{\top} A X 
            &= \ovl{X}^{\top} (\lambda X) \\
            &= \lambda \sum_{i=1}^{n} \abs{x_i}^2
        \end{align*}
        D’autre part, 
        \begin{align*}
            \left(\ovl{X}^{\top} A X \right)^{\top} &\overset{i}{=} \lambda \sum_{i=1}^{n} \abs{x_i}^2 \\
            &\overset{ii}{=} X^{\top} A^{\top} \ovl{X} \\
            & \quad \downarrow \quad A^{\top} = A \\
            &= X^{\top} A \ovl{X} \\
            &= X^{\top} \ovl{AX} \\
            &= \ovl{\lambda} X^{\top} \ovl{X} \\
            &= \ovl{\lambda} \sum_{i=1}^{n} \abs{x_i}^2
        \end{align*}
        Donc $\lambda \in \mathbb{R}$.
    \end{demo}

    Nous avons au passage montré que les valeurs propres de $f$ sont toujours réelle, \textit{i.e.} $\sp_{\mathbb{C}}(f) \subset \mathbb{R}$.

    \begin{lem}{}{}
        Soit $E$ un EVE et $f \in \mathcal{L}(E)$ autoadjoint. Si $F$ est un sev de $E$ stable par $f$, alors $F^{\perp}$ est stable par $f$. 
    \end{lem}

    \begin{demo}{Démonstration}{mybrown}
        Soient $x \in F^{\perp}$ et $y \in F$. $\spr{f(x)}{y} = \spr{x}{f(y)} = 0$.
    \end{demo}

    \begin{demo}{Preuve du théorème spectral}{myred}
        \begin{itemize}
            \item[\textbf{I}] Si $n = 1$. Il existe une valeur propre réelle $\lambda$ de $f$, et n’importe quel vecteur non nul associé normalisé convient.
            \item[\textbf{H}] Par récurrence forte sur $n$. Soit $E$ un EVE de dimension $n+1$, et $f \in \mathcal{L}(E)$ autoadjoint. Il existe $\lambda \in \mathbb{R}$ qui est une valeur propre de $f$. De plus, $E_{\lambda}(f)$ est stable par $f$. On pose 
            \[ \fonction{\tilde{f}}{E_{\lambda}(f)^{\top}}{E_{\lambda}(f)^{\top}}{x}{f(x)} \]   
            $E_{\lambda}(f)^{\top}$ est un EVE pour le produit scalaire sur $E$. Soient $x,y \in E_{\lambda}(f)^{\perp}$, on a 
            \begin{align*}
                \spr{\tilde{f}(x)}{y} 
                &= \spr{f(x)}{y} \\
                &= \spr{x}{\tilde{f}(y)}
            \end{align*}
            Donc on peut appliquer l’hypothèse de récurrence à $\tilde{f}$. Il existe une BON de $E_{\lambda}(f)^{\perp}$ formée de vecteurs propres de $\tilde{f}$, que l’on peut compléter en BON de $E_{\lambda}(f)$, d’où le résultat.
        \end{itemize}
    \end{demo}

    \begin{theo}{Théorème spectral, version matricielle}{}
        Soit $M \in \mathcal{M}_n(\mathbb{R})$ une matrice symétrique. Il existe $P \in \O_n(\mathbb{R})$ et $D \in \mathcal{M}_n(\mathbb{R})$ diagonale telles que $M = P D P^{\top}$
    \end{theo}

    \begin{demo}{Idée}{myred}
        Soit $E$ un EVE de dimension $n$ et $B$ une BON de $E$. Il existe $f \in \mathcal{L}(E)$ tel que $M = \mat{B}{f}$. Comme $M$ est symétrique et $B$ une BON, on sait que $f$ est autoadjoint. D’après le théorème spectral sur les endomorphismes, il existe une BON $B'$ de $E$ formée de vecteurs propres de $f$. On note $P$ la matrice de passage de $B$ à $B'$. $P \in \O_n(\mathbb{R})$, donc $M = P \mat{B'}{f} P^{\top}$.
    \end{demo}

    En particulier, toute matrice réelle et symétrique est diagonalisable.

    \subsubsection{Endomorphismes autoadjoints positifs et définis positifs}

    Dans cette sous-sous-section, on considère toujours $E$ un EVE de dimension $n$, $f \in \mathcal{L}(E)$ autoadjoint, et $M \in \mathcal{M}_n(\mathbb{R})$, symétrique.

    \begin{defi}{}{}
        \begin{enumerate}
            \item \begin{enumerate}
            \item On dit que $f$ est positif si $\forall x \in E, \spr{x}{f(x)} \geq 0$. On note $f \in \S^+(E)$.
            \item On dit que $f$ est définie positive si $\forall x \in E \backslash {0}, \spr{x}{f(x)} > 0$. On note $f \in \S^{++}(E)$.
        \end{enumerate} 
            \item \begin{enumerate}
            \item On dit que $M$ est positive si $\forall X \in \mathcal{M}_{n,1}(\mathbb{R}), X^{\top} M X \geq 0$. On note $M \in \S_n^{+}(\mathbb{R})$.
            \item On dit que $M$ est définie positive si $\forall X \in \mathcal{M}_{n,1}(\mathbb{R}) \backslash {0}, X^{\top} M X > 0$. On note $M \in \S_n^{++}(\mathbb{R})$.
        \end{enumerate}
        \end{enumerate}
    \end{defi}

    \begin{prop}{}{}
        \begin{enumerate}
            \item \begin{enumerate}
                \item $f \in \S^+(E) \iff \sp(f) \subset \mathbb{R}_+$
                \item $f \in \S^{++}(E) \iff \sp(f) \subset \mathbb{R}_+^*$
            \end{enumerate}
            \item \begin{enumerate}
                \item $M \in \S_n^+(\mathbb{R}) \iff \sp(M) \subset \mathbb{R}_+$
                \item $M \in \S_n^{++}(\mathbb{R}) \iff \sp(M) \subset \mathbb{R}_+^*$
            \end{enumerate}
        \end{enumerate}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item \begin{enumerate}
                \item Soit $\lambda \in \sp(f)$. Il existe $v \in E, v \neq 0$ tel que $f(v) = \lambda v$. On a $\spr{v}{f(v)} = \spr{v}{\lambda v} = \lambda \norm{v}^2 \geq 0$. Donc $\lambda \in \mathbb{R}_+$. Réciproquement, il existe une BON  $B = (v_1,\ldots,v_n)$ de vecteurs propres de $f$. Soit $x \in E, x = \sum_{i=1}^{n} x_i v_i$. Alors $\spr{x}{f(x)} = \sum_{i=1}^{n} \lambda_i x_i^2 \geq 0$.
                \item Soit $\lambda \in \sp(f)$. Il existe $v \in E, v \neq 0$ tel que $f(v) = \lambda v$. On a $\spr{v}{f(v)} = \spr{v}{\lambda v} = \lambda \norm{v}^2 > 0$. Donc $\lambda \in \mathbb{R}_+^*$. Réciproquement, il existe une BON  $B = (v_1,\ldots,v_n)$ de vecteurs propres de $f$. Soit $x \in E \backslash {0}, x = \sum_{i=1}^{n} x_i v_i$. Alors $\spr{x}{f(x)} = \sum_{i=1}^{n} \lambda_i x_i^2 > 0$.
            \end{enumerate}
            \item \begin{enumerate}
                \item Si $\lambda \in \sp(M)$, il existe $X$ tel que $MX = \lambda X$. On a donc $X^{\top} M X = \lambda \sum_{i=1}^{n} x_i^2 \geq 0$. Réciproquement, d’après le théorème spectral pour les matrices, il existe $P \in \O_n(\mathbb{R})$ et $D \in \mathcal{M}_n(\mathbb{R})$ diagonale telles que $M = P D P^{\top}$. Soit $X \in \mathcal{M}_{n,1}(\mathbb{R})$, $X^{\top} M X = (P^{\top} X)^{\top} D (P^{\top} X)$. En posant $P^{\top} X = \begin{bmatrix}
                    y_1 \\
                    \vdots \\
                    y_n
                \end{bmatrix}$ et $D = \diag(\lambda_1,\ldots,\lambda_n)$ où les $\lambda \in \mathbb{R}_+$. Alors $X^{\top} M X = \sum_{i=1}^{n} \lambda_i y_i^2 \geq 0$
                \item Si $\lambda \in \sp(M)$, il existe $X$ tel que $MX = \lambda X$. On a donc $X^{\top} M X = \lambda \sum_{i=1}^{n} x_i^2 > 0$. Réciproquement, d’après le théorème spectral pour les matrices, il existe $P \in \O_n(\mathbb{R})$ et $D \in \mathcal{M}_n(\mathbb{R})$ diagonale telles que $M = P D P^{\top}$. Soit $X \in \mathcal{M}_{n,1}(\mathbb{R})$, $X^{\top} M X = (P^{\top} X)^{\top} D (P^{\top} X)$. En posant $P^{\top} X = \begin{bmatrix}
                    y_1 \\
                    \vdots \\
                    y_n
                \end{bmatrix}$ et $D = \diag(\lambda_1,\ldots,\lambda_n)$ où les $\lambda \in \mathbb{R}_+^*$. Alors $X^{\top} M X = \sum_{i=1}^{n} \lambda_i y_i^2 > 0$
            \end{enumerate}
        \end{enumerate}
    \end{demo}

    \begin{omed}{Application}{myolive}
        Soit $A \in \mathcal{M}_n(\mathbb{R})$ et $X,Y \in \mathcal{M}_{n,1}(\mathbb{R})$. À quelle condition l’application
        \[ \Phi : (X,Y) \longmapsto X^{\top} A Y \]   
        est un spr ?
    \end{omed}

    On peut intuiter que ce sera dans le cas où $A \in \S^{++}_n(\mathbb{R})$, mais il faut le montrer, ce à quoi nous allons nous employer :

    \begin{demo}{Justification}{myolive}
        L’application est une forme bilinéaire. 
        \begin{itemize}
            \item \textbf{Caractère symétrique} \quad $\forall X,Y \in \mathbb{R}^n, \Phi(X,Y) = \Phi(Y,X)$ \textit{ssi} $\Phi(E_i, E_j) = \Phi(E_j, E_i)$. Or $\Phi(E_i,E_j) = a_{i,j}$, il faut donc que $A$ soit symétrique.
            \item \textbf{Caractère défini positif} $\Phi$ est définie positive \textit{ssi}  $\forall X \in \mathbb{R}^n \neq 0, X^{\top} A X$, donc si $A$ est définie positive.
        \end{itemize}
    \end{demo}

    \begin{omed}{Exercice}{myolive}
        Soit $A = (a_{i,j}) \in \S_n^{++}(\mathbb{R})$. Montrer que :
        \begin{enumerate}
            \item $\det(A) \leq \left(\frac{\tr(A)}{n}\right)^n$ 
            \item $\forall i \intervalleEntier{1}{n}, \quad a_{i,i} > 0$ 
            \item En posant $D = \diag\left(\frac{1}{\sqrt{a_{1,1}}}, \ldots, \frac{1}{\sqrt{a_{n,n}}}\right)$ et $B = DAD$, $\det(A) \leq \prod_{i=1}^{n} a_{i,i}$
        \end{enumerate}
    \end{omed}

    \begin{demo}{Solution}{myolive}
        On peut remarquer dans un premier temps que $A$ est diagonalisable comme matrice symétrique réelle (théorème spectral). Il existe $P \in \O_n(\mathbb{R})$ telle que $A = P \diag(\lambda_1, \ldots, \lambda_n) P^{\top}$. De plus, on sait que $\forall i \in \intervalleEntier{1}{n}, \lambda_i > 0$.
        \begin{enumerate}
            \item $\det(A) = \prod_{i=1}^{n} \lambda_i$ et $\tr(A) = \sum_{i=1}^{n} \lambda_i$, et les $\lambda_i$ sont positifs, donc par inégalité arithmético-géométrique -- qui se déduit de la concavité de $\ln$ --, 
            \[ \left(\lambda_1\cdots \lambda_n\right)^{1/n} \leq \frac{\sum_{i=1}^{n} \lambda_i}{n} \]
            \item En prenant $X = \begin{bmatrix}
                x_1 \\
                \vdots \\
                x_n
            \end{bmatrix} \neq 0$, on a que $X^{\top} A X = \sum_{i=1}^{n} \sum_{j=1}^{n} x_i x_j a_{i,j} > 0$. En choisissant les vecteur de la base canonique de $\mathcal{M}_{n,1}(\mathbb{R})$ -- que l’on note ici $E_i$ --, on obtient que $\forall i \in \intervalleEntier{1}{n}, a_{i,i} > 0$.
            \item $\det(B) = \det(A) \det(D)^2 = \det(A) \times \prod_{i=1}^{n} \frac{1}{a_{i,i}}$. Donc 
            \[ \det(A) \leq \prod_{i=1}^{n} a_{i,i} \iff \det(B) \leq 1 \]    
            Montrons que $B \in \S_n^{++}(\mathbb{R})$. Clairement, $B \in \mathcal{n}(\mathbb{R})$ et $B^{\top} = D A^{\top} D = B$, donc $B$ est symétrique. Si $X = \begin{bmatrix}
                x_1 \\
                \vdots \\
                x_n
            \end{bmatrix}$, $X^{\top} B X = (DX)^{\top} A (DX) \geq 0$ par définition positive de $A$. De plus, $D$ est inversible donc $DX= 0$ admet une unique solution qui est $0$, ce qui permet de confirmer la définition positive de $B$. Donc $B \in \S_n^{++}(\mathbb{R})$, et ainsi $\det(B) \leq \left(\frac{\tr(B)}{n}\right)^n$. Or $\tr(B) = \sum_{i=1}^{n} d_i^2 a_{i,i} = n$ donc $\det(B) \leq 1$.
        \end{enumerate}
    \end{demo}

    \begin{omed}{Exercice}{myolive}
        Soit $A \in \mathcal{M}_n(\mathbb{R})$. On pose $S = \left\{X \in \mathcal{n,1}(\mathbb{R}), \quad \norm{X} = 1\right\}$. Montrer que 
        \[ \sup\left\{\norm{AX}, X \in S\right\} = \sqrt{\max\left(\sp(A^{\top} A)\right)} \]
    \end{omed}

    \begin{demo}{Résolution}{myolive}
        \begin{itemize}
            \item $(A^{\top} A)^{\top} = A^{\top} A$ donc $A^{\top} A$ est une matrice symétrique réelle, donc est diagonale, et son spectre est non vide. 
            \item Soit $X \in \mathcal{M}_{n,1}(\mathbb{R})$. $X^{\top} A^{\top} AX = \sum_{i=1}^{n} y_i^2 \geq 0$ où on a posé $AX = \begin{bmatrix}
                y_1 \\
                \vdots \\
                y_n
            \end{bmatrix}$. Donc $A^{\top} A \in \S_n^+(\mathbb{R})$. De plus, si $A$ est inversible, $A^{\top} A \in \S_n^{++}(\mathbb{R})$.
            \item $\norm{AX}^2 = \sum_{i=1}^{n} y_i^2 = X^{\top} A^{\top} A X$. D’après le théorème spectral, il existe $P \in \O_n(\mathbb{R})$ telle que $A^{\top} A = P \diag(\lambda_1,\ldots, \lambda_n) P^{\top}$. Ainsi,
            \begin{align*}
                \norm{AX}^2 
                &= X^{\top} A^{\top} AX \\
                &= (P^{\top} X)^{\top} D (P^{\top} X) \\
                &\quad \downarrow \quad P^{\top} X := \begin{bmatrix}
                    \alpha_1 \\
                    \vdots \\
                    \alpha_n 
                \end{bmatrix} \\
                &= \sum_{i=1}^{n} \lambda_i \alpha_i^2 \\
                &\quad \downarrow \quad \sp(A^{\top} A) \subset \mathbb{R}_+^* \\
                &\leq \max(\lambda_1,\ldots,\lambda_n) \sum_{i=1}^{n} \alpha_i^2 \\
                &= \max(\sp(A^{\top}A)) \norm{P^{\top}X}^2 \\
                \text{et } \norm{P^{\top} X}^2 &= X^{\top} P^{\top} P X \\
                &= \norm{X}^2 = 1
            \end{align*}
            Ainsi, on a obtenu que \[ \sup\left\{\norm{AX}^2, \quad X \in S\right\} \leq \max(\sp(A^{\top}A)) \] 
            Soit $i_0 \in \intervalleEntier{1}{n}$ tel que $\lambda_{i_0} = \max(\lambda_1,\ldots,\lambda_n)$. On choisit $X = P E_{i_0}$, tel que $\norm{X} = 1$, et $\norm{AX}^2 = \lambda_{i_0}$. Il y a donc égalité.
        \end{itemize}
    \end{demo}