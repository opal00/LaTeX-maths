\customchapter{Variables aléatoires}{}

\subsection{Variables aléatoires discrètes}

    \subsubsection{Loi d’une variable aléatoire}

    \begin{defi}{Variable aléatoire}{}
        Soit $\Omega$ un univers fini.
    
        Une \textbf{variable aléatoire} sur $\Omega$ est une application définie sur $\Omega$, à valeurs dans un ensemble $\mathcal{E}$.

        Lorsque $\mathcal{E} = \mathbb{R}$ (ou $\mathbb{C}$), on parle de variable aléatoire réelle (ou complexe).
    \end{defi}

    \begin{omed}{Notations}{myyellow}
        \begin{soient}
            \item $\Omega$ un univers fini
            \item $X$ une variable aléatoire sur $\Omega$, à valeurs dans $\mathcal{E}$
            \item $A$ une partie de $\mathcal{E}$
        \end{soient}
        Alors $X^{-1}(A) = \{ \omega \in \Omega, \, X(\omega) \in A \}$ est un sous-ensemble de $\Omega$ i.e. est un \textbf{événement}.
        \begin{itemize}
            \item Si $A = \{ x \}$, on note $X = x$ pour $ \{ \omega \in \Omega, \, X(\omega) = x \}$
            \item Si $A = \intervalleOF{-\infty}{x}$, on note $X \leq x$ pour $\{ \omega \in \Omega, \, X(\omega) \leq x \}$
        \end{itemize}
    \end{omed}

    \begin{defi}{Loi d’une variable aléatoire}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$ à valeurs dans un ensemble $\mathcal{E}$
        \end{soient}
        La \textbf{loi de probabilité} de la variable $X$ est la probabilité 
        \[ \fonction{P_X}{\mathcal{P}(E)}{\intervalleFF{0}{1}}{A}{P(X \in A)} \]
        Cette loi est une probabilité sur $\mathcal{E}$
    \end{defi}

    \begin{demo}{Justification}{myyellow}
        Vérifier les trois points définissant une probabilité.
    \end{demo}

    \begin{theo}{}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$ à valeurs dans un ensemble $\mathcal{E}$
        \end{soient}
        \begin{alors}
            \item $\left(P(X=x)\right)_{x \in X(\Omega)}$ est une distribution de probabilités sur $X(\Omega)$
            \item $P_X$ est l’unique probabilité qui correspond à cette distribution.
        \end{alors}
    \end{theo}

    \begin{defi}{VA de même loi de probabilité}{}
        \begin{soient}
            \item $(\Omega_1,P_1)$ et $(\Omega_2,P_2)$ deux espaces probabilisés finis
            \item $X$ et $Y$ des variables aléatoires sur $\Omega_1$ et $\Omega_2$ respectivement
        \end{soient}
        On dit que $X$ et $Y$ sont de \textbf{même loi de probabilité}, et on note $X \sim Y$ lorsque 
        \begin{enumerate}
            \item $X(\Omega_1) = Y(\Omega_2)$
            \item $\forall x \in X(\Omega_1) = Y(\Omega_2), \, P_1(X=x) = P_2(Y=x)$
        \end{enumerate}
    \end{defi}

    \begin{defi}{Loi uniforme}{}
        Soient un espace probabilisé fini $(\Omega,P)$ et $X$ une variable aléatoire sur $\Omega$.
    
        On dit que $X$ suit une \textbf{loi uniforme} sur $X(\Omega)$ lorsque 
        \[ \forall x \in X(\Omega), P(X = x) = \frac{1}{\card(X(\Omega))} \]
        On note $X \sim \mathcal{U}(X(\Omega))$
    \end{defi}

    \begin{defi}{Loi de Bernoulli}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
            \item $p \in \intervalleFF{0}{1}$
        \end{soient}
        On dit que $X$ suit une \textbf{loi de Bernoulli} de paramètre $p$ lorsque $\et{X(\Omega) = \{0,1\}}{P(X=1) = 1 \text{ et } P(X=0)= 1-p}$ 
        
        On note $X \sim \mathcal{B}(p)$
    \end{defi}

    \begin{prop}{Caractérisation de l’indicatrice par une probabilité}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $A \in \mathcal{P}(\Omega)$
        \end{soient}

        Alors $\mathbb{1}_A \sim \mathcal{B}(P(A))$
    \end{prop}

    \begin{defi}{Loi binomiale}{}
        \begin{soit}
            \item $(\Omega,P)$ un espace probabilisé
            \item $X$ une variable aléatoire sur $\Omega$
            \item $p \in \intervalleOO{0}{1}$
            \item $n \in \mathbb{N}^*$
        \end{soit}
        On dit que $X$ suit une \textbf{loi binomiale} de paramètres $n$ et $p$ lorsque 
        \[ \et{X(\Omega) \in \intervalleEntier{0}{n}}{\forall k \in \intervalleEntier{0}{n}, \, P(X=k) =\binom{n}{k} p^k (1-p)^{n-k}} \]
    \end{defi}

    \begin{prop}{Composition}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé
            \item $X$ une variable aléatoire sur $\Omega$
            \item $\mathcal{F}$ un ensemble
            \item $f\, : \, X(\Omega) \longrightarrow \mathcal{F}$
        \end{soient}
        \begin{alors}
            \item $\fonction{Y}{\Omega}{\mathcal{F}}{\omega}{f(X(\omega))}$ est une variable aléatoire qu’on note $f(X)$ (ou $f \circ X$)
            \item $Y(\Omega) = f(X(\Omega))$ et 
            \[ \forall y \in Y(\Omega), P(Y=y) = \sum\limits_{x \in f^{-1}(\{y\})} P(X=x) = \sum\limits_{\substack{x \in X(\Omega) \\ f(x) = y}} P(X = x) \]
        \end{alors}
    \end{prop}

    \begin{prop}{Composition de VA de même loi de probabité}{}
        \begin{soient}
            \item $(\Omega_1,P_1)$ et $(\Omega_2,P_2)$ deux espaces probabilisés finis
            \item $X$ et $Y$ des variables aléatoires sur $\Omega_1$ et $\Omega_2$ respectivement
            \item $f$ une fonction définie sur $X(\Omega_1)$ à valeurs dans un ensemble $\mathcal{E}$
        \end{soient}
        On suppose que $X \sim Y$.

        Alors $f(X) \sim f(Y)$.
    \end{prop}

\subsection{Familles de variables aléatoires et indépendance}

    \subsubsection{Couple de VA}

    \begin{defi}{Lois conjointes, marginales et conditionnelles}{}
        Soient $X$ et $Y$ deux variables aléatoires sur un même espace probabilisé fini $(\Omega,P)$.
        \begin{itemize}
            \item La loi du couple $(X,Y)$ est appelée \textbf{loi conjointe} de $(X,Y)$. On la note $P_{(X,Y)}$.
            \item Pour $(x,y) \in X(\Omega) \times Y(\Omega)$, on note $P(X=x,Y=y)$ pour $P((X,Y) = (x,y))$.
            \item Les lois de $X$ et $Y$ sont appelées \textbf{lois marginales} du couple $(X,Y)$.
            \item Pour tout $x \in X(\Omega)$ tel que $P(X=x) \neq 0$, la \textbf{loi conditionnelle} de $Y$ sachant $X=x$ est la loi de la variable $Y'$ définie par $\forall y \in Y(\Omega), \, P(Y' = y) = P(Y=y \, | \, X=x)$
        \end{itemize}
    \end{defi}

    \begin{defi}{VA indépendantes}{}
        Soit $(X,Y)$ un couple de variables aléatoires défini sur un espace probabilisé fini $(\Omega,P)$.

        On dit que les variables aléatoires $X$ et $Y$ sont \textbf{indépendantes} lorsque 
        \[ \forall (A,B) \in \mathcal{P}(X(\Omega)) \times \mathcal{P}(Y(\Omega)), \{ X \in A \} \text{ et } \{Y \in B \} \text{ sont indépendantes} \]
        On note $X \independent Y$.
    \end{defi}

    \begin{theo}{Caractérisation de l’indépendance de deux VA}{}
        \begin{soient}
            \item un espace probabilisé fini $(\Omega,P)$
            \item $(X,Y)$ un couple de variables aléatoires défini sur $\Omega$
        \end{soient}
        Alors 
        \[ X \independent Y \iff \forall (x,y) \in X(\Omega) \times Y(\Omega), \, P(X=x,Y=y) = P(X=x)P(Y=y) \]
    \end{theo}

    \begin{prop}{Composition par une fonction}{}
        \begin{soient}
            \item $(X,Y)$ un couple de variables aléatoires défini sur un espace probabilisé fini $(\Omega,P)$
            \item $f$ une fonction définie sur $X(\Omega)$, et $g$ une fonction définie sur $Y(\Omega)$
        \end{soient}
        On suppose que $X \independent Y$.

        Alors $f(X) \independent g(Y)$.
    \end{prop}

    \subsubsection{Famille de variables aléatoires (mutuellement indépendantes)}

    \begin{prop}{Caractérisation des VA indépendantes}{}
        Soient $X_1,\ldots,X_n$ des variables aléatoires définies sur un même espace probabilisé $(\Omega,P)$.

        Alors $X_1,\ldots,X_n$ sont indépendantes 
        \begin{align*}
            \iff &  \forall (A_1,\ldots,A_n) \in \mathcal{P}(X_1(\Omega)) \times \ldots \times \mathcal{P}(X_n(\Omega)), P(X_1 \in A_1,\ldots, X_n \in A_n) = \prod\limits_{i=1}^n P(X_i \in A_i) \\
            \iff & \forall (x_1,\ldots,x_n) \in X_1(\Omega) \times \ldots \times X_n(\Omega), \text{les év. } (X_1 = x_1), \ldots, (X_n = x_n) \text{ sont indépendants} \\
            \iff & \forall (x_1,\ldots,x_n) \in X_1(\Omega) \times \ldots \times X_n(\Omega), P(X_1=x_1,\ldots,X_n=x_n) = \prod\limits_{i=1}^n P(X_i = x_i)
        \end{align*}
    \end{prop}

    \begin{prop}{Lemme des coalitions}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé
            \item $n \geq 2$ et $m \in \intervalleEntier{1}{n-1}$
            \item $(X_1,\ldots,X_n)$ des variables aléatoires sur $\Omega$
            \item $f$ une fonction définie sur $X_1(\Omega) \times \ldots \times X_m(\Omega)$
            \item $g$ une fonction définie sur $X_{m+1}(\Omega) \times \ldots \times X_n(\Omega)$
        \end{soient}
        On suppose que $(X_1,\ldots,X_n)$ sont mutuellement indépendantes.

        Alors $f(X_1,\ldots,X_{m})$ et $g(X_{m+1},\ldots,X_n)$ sont indépendantes.
    \end{prop}

    \begin{prop}{Espérance du produit de variables aléatoires indépendantes}{Espérance du produit de variables aléatoires indépendantes}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé
            \item $n \geq 2$
            \item $(X_1,\ldots,X_n)$ des variables aléatoires sur $\Omega$
        \end{soient}
        On suppose que $(X_1,\ldots,X_n)$ sont indépendantes.

        Alors 
        \[ E\left(\prod\limits_{i=1}^n X_i\right) = \prod\limits_{i=1}^n E(X_i) \] 
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Pour $k \in \intervalleEntier{1}{n}$, on pose $\mathcal{H}_k$ : $E\left(\prod\limits_{i=1}^k X_i\right) = \prod\limits_{i=1}^k E(X_i)$

        Dans l’hérédité, utiliser le lemme des coalitions pour écrire que $\prod\limits_{i=1}^k X_i$ et $X_{k+1}$ sont indépendants.
    \end{demo}

    \begin{prop}{Espérance du produit de variables aléatoires indépendantes}{Espérance du produit de variables aléatoires indépendantes}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé
            \item $n \geq 2$
            \item $(X_1,\ldots,X_n)$ des variables aléatoires sur $\Omega$
        \end{soient}
        On suppose que $(X_1,\ldots,X_n)$ sont indépendantes.

        Alors 
        \[ E\left(\prod\limits_{i=1}^n X_i\right) = \prod\limits_{i=1}^n E(X_i) \] 
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Pour $k \in \intervalleEntier{1}{n}$, on pose $\mathcal{H}_k$ : $E\left(\prod\limits_{i=1}^k X_i\right) = \prod\limits_{i=1}^k E(X_i)$

        Dans l’hérédité, utiliser le lemme des coalitions pour écrire que $\prod\limits_{i=1}^k X_i$ et $X_{k+1}$ sont indépendants.
    \end{demo}

    \begin{prop}{Loi faible des grands nombres}{}
        Soit $(X_n)_{n \in \mathbb{N}^*}$ une famille de variables aléatoires réelles sur un univers probabilisé fini $(\Omega,P)$.

        \begin{suppose}
           \item $\exists \, m \in \mathbb{R}, \, \forall n \in \mathbb{N}^*, \, E(X_n) = m$
           \item $\exists \, \sigma \in \mathbb{R}, \, \forall n \in \mathbb{N}^*, \, V(X_n) = \sigma^2$
           \item les variables $X_n$ sont deux à deux indépendantes
        \end{suppose}
        Pour tout $n \in \mathbb{N}^*$, on pose $S_n = \frac{1}{n} \sum\limits_{k=1}^n X_k$

        Alors \[ \forall \varepsilon > 0, \, \lim\limits_{n \rightarrow + \infty} \left(P(\abs{S_n-n} \geq \varepsilon)\right) = 0 \] 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{align*}
            E(S_n) &= E\left(\frac{1}{n} \sum\limits_{k=1}^n X_k\right) \\
            &= \frac{1}{n} \sum\limits_{k=1}^n E(X_k) = m
        \end{align*}
        Comme $X_1,\ldots,X_n$ sont indépendantes,
        \begin{align*}
            V(S_n) &= \frac{1}{n^2} V(\sum\limits_{k=1}^n X_k) \\
            &= \frac{1}{n^2} \sum\limits_{k=1}^n V(X_k) \\
            &= \frac{\sigma^2}{n}
        \end{align*}
        Par l’inégalité de Bienaymé-Tchebycheff, 
        \begin{align*}
            P(\abs{S_n-n} \geq \varepsilon) & \leq \frac{V(S_n)}{\varepsilon^2} \\
            & \leq \frac{\sigma^2}{n \varepsilon^2}
        \end{align*}
    \end{demo}

    \begin{prop}{Somme de variables aléatoires de Bernoulli indépendantes}{Somme de variables aléatoires de Bernoulli indépendantes}
        Soient $X_1,\ldots,X_n$ des variables aléatoires et $p \in \intervalleOO{0}{1}$.

        \begin{suppose}
            \item $X_1,\ldots,X_n$ sont (mutuellement indépendantes)
            \item Pour $k \in \intervalleEntier{1}{n}, \, X_k \sim \mathcal{B}(p)$
        \end{suppose}

        Alors 
        \[ \sum\limits_{i=1}^n X_i \sim \mathcal{B}(n,p) \] 
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Pour $k \in \intervalleEntier{1}{n}$, on pose $\mathcal{H}_k$ : $\sum\limits_{i=1}^k X_i \sim \mathcal{B}(k,p)$
        
        Évident pour $k=1$.
        
        Soit $k \in \intervalleEntier{1}{n-1}$ tel que $\mathcal{H}_k$ est vraie. 
        
        $\left( \sum\limits_{i=1}^{k+1} X_i \right)(\Omega) = \intervalleEntier{0}{k+1}$.
        
        \begin{align*}
            P\left(\sum\limits_{i=1}^{k+1} X_i = 0\right) &= P(X_1 =0,\ldots,X_{k+1}=0) \\
            & \downarrow X_1,\ldots,X_n \text{ indép.} \\
            &= (1-p)^{k+1} \\
            &= \binom{k+1}{0} p^0 (1-p)^{k+1} \\
            P\left(\sum\limits_{i=1}^{k+1} X_i = k+1\right) &= P(X_1 =1,\ldots,X_{k+1}=1) \\
            & \downarrow X_1,\ldots,X_n \text{ indép.} \\
            &= p^{k+1} \\
            &= \binom{k+1}{k+1} p^{k+1} (1-p)^{k+1-(k+1)} \\
        \end{align*}
        Soit $j \in \intervalleEntier{1}{k}$. Comme $X_1,\ldots,X_n$ sont mutuellement indépendantes, par le lemme des coalitions, $\sum\limits_{i=1}^k X_i$ et $X_{k+1}$ sont indépendants.
        
        $\et{(X_{k+1}=0, X_{k+1}=1) \text{ est un syst. complet d’év.}}{P(X_{k+1}=0) = 1-p > 0 \text{ et } p(X_{k+1}=1) = p > 0}$ donc d’après la formule des probabilités totales, 
        \begin{align*}
            P\left(\sum\limits_{i=1}^{k+1} X_i = j\right) &=  P\left(\sum\limits_{i=1}^{k+1} X_i = j \, \big| \, X_{k+1} = 0\right) P(X_{k+1} = 0) \\
            &+ P\left(\sum\limits_{i=1}^{k+1} X_i = j \, \big| \, X_{k+1} = 1\right) P(X_{k+1} = 1) \\
            & \downarrow \text{lemme des coalitions} \\ 
            &= P\left(\sum\limits_{i=1}^{k+1} X_i = j\right) (1-p) \\
            & + P\left(\sum\limits_{i=1}^{k+1} X_i = j-1 \right) p \\
            & \downarrow \mathcal{H}_k \\
            &= \binom{k}{j} p^j (1-p)^{k+1-j} \\
            &+ \binom{k}{j-1}p^j (1-p)^{k+1-j} \\
            & \downarrow \text{formule du triangle de Pascal}  \\
            &= \binom{k+1}{j} p^j (1-p)^{k+1-j} 
        \end{align*}
        Donc $\sum\limits_{i=1}^{k+1} X_i \sim \mathcal{B}(k+1,p)$
        Par récurrence finie sur $\intervalleEntier{1}{n}$, $\mathcal{H}_n$ est vraie.
    \end{demo}

\section{Moments d’une variable aléatoire}

\subsection{Espérance}

    \begin{defi}{Espérance}{}
        Soient $(\Omega,P)$ un espace probabilisé fini et $X$ une variable aléatoire réelle ou complexe sur $\Omega$.
        \begin{itemize}
            \item L’\textbf{espérance} de $X$ est le nombre \[ E(X) = \sum\limits_{x \in X(\Omega)} xP(X=x) \]
            \item On dit que la variable est \textbf{centrée} lorsque le jeu est équitable, i.e. $E(X) = 0$.
        \end{itemize}
    \end{defi}

    \begin{prop}{Expression de l’espérance}{}
        Soient $(\Omega,P)$ un espace probabilisé fini et $X$ une variable aléatoire réelle ou complexe sur $\Omega$.

        Alors 
        \[ E(X) = \sum\limits_{\omega \in \Omega} X(\omega) P(\{\omega\}) \]
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{align*}
            E(X) &= \sum\limits_{x \in X(\Omega)} xP(X=x) \\
            &= \sum\limits_{x \in X(\Omega)} xP(\{\omega \in \Omega, \, X(\Omega) = x \}) \\
            &= \sum\limits_{x \in X(\Omega)} xP(\bigcup\limits_{\substack{\omega \in \Omega \\ X(\omega) = x}} \{ \omega \}) \\
            &= \sum\limits_{x \in X(\Omega)}x \sum\limits_{\substack{\omega \in \Omega \\ X(\omega) = x}} P(\{\omega\}) \\
            &= \sum\limits_{x \in X(\Omega)} \sum\limits_{\substack{\omega \in \Omega \\ X(\omega) = x}} X(\omega) P(\{\omega\}) \\
            &= \sum\limits_{\omega \in \Omega} X(\omega) P(\{\omega\})
        \end{align*}
    \end{demo}

    \begin{prop}{Propriétés de l’espérance}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X,Y$ deux variables aléatoires réelles ou complexes sur $\Omega$
        \end{soient}
        \begin{alors}
            \item Si $X$ est constante égale à $a$, $E(X) = a$
            \item Si $X$ est la fonction indicatrice d’un événement $A$, $E(X) = P(A)$
            \item $\forall \lambda \in \mathbb{K}, \, E(\lambda X + Y) = \lambda E(X) + E(Y)$
            \item $X \leq Y \implies E(X) \leq E(Y)$
            \item $\abs{E(X)} \leq E(\abs{X})$
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item $E(X) = a\underbrace{P(X=a)}_{=1} = 1$
            \item On suppose que $X= \mathbb{1}_A$
            \begin{align*}
                E(X) &= 0P(\mathbb{1}_A =0) + 1P(\mathbb{1}_A = 1) \\
                &= P(\{ \omega \in \Omega, \, \mathbb{1}_A(\omega) = 1 \}) \\
                &= P(A)
            \end{align*}
            \item \begin{align*}
                E(\lambda X + Y) &= \sum\limits_{\omega \in \Omega} (\lambda X(\omega) + Y(\omega))P(\omega) \\
                &= \lambda \sum\limits_{\omega \in \Omega} X(\omega) P(\omega) + \sum\limits_{\omega \in \Omega} Y(\omega) P(\omega) \\
                &= \lambda E(X) + E(Y)
            \end{align*}
            \item On suppose que $\forall \omega \in \Omega, \, X(\omega) \leq Y(\omega)$
             
            $\forall \omega \in \Omega, \, P(\{ \omega \}) \geq 0$ 
         
            donc $\forall \omega \in \Omega, \, X(\omega) P(\{ \omega \}) \leq Y(\omega)P(\{ \omega \})$. 
            
            Ainsi, $E(X) \leq E(Y)$
            \item \begin{align*}
                \abs{E(X)} &= \abs{\sum\limits_{\omega \in \Omega} X(\omega)P(\{ \omega \})} \\
                &\leq \sum\limits_{\omega \in \Omega} \abs{X(\omega)P(\{ \omega \})} \\
                & \downarrow P(\{\omega\}) \geq 0 \\
                &\leq \sum\limits_{\omega \in \Omega} \abs{X(\omega)}P(\{ \omega \}) = E(\abs{X})
            \end{align*}
        \end{enumerate}
    \end{demo}

    \begin{theo}{Dit « de transfert »}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
            \item $g \, : \, X(\Omega) \longrightarrow \mathbb{R}$ (ou $\mathbb{C}$)
        \end{soient}
        Alors 
        \[ E(g(X)) = \sum\limits_{x \in X(\Omega)} g(x) P(X = x) \] 
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        \begin{align*}
            E(g(X)) &= \sum\limits_{\omega \in \Omega} g(X(\omega)) P(\omega) \\
            &= \sum\limits_{x \in X(\Omega)} \sum\limits_{\substack{\omega \in \Omega \\ X(\omega) = x}} g(x) P(\omega) \\
            &= \sum\limits_{x \in X(\Omega)} g(x) \sum\limits_{\substack{\omega \in \Omega \\ X(\omega) = x}} P(\omega) \\
            &= \sum\limits_{x \in X(\Omega)} g(x) P\Big(\bigcup\limits_{\substack{\omega \in \Omega \\ X(\omega) = x}} \{ \omega \} \Big) \\
            &= \sum\limits_{x \in X(\Omega)} g(x) P(X = x)
        \end{align*}
    \end{demo}

\subsection{Variance, covariance et écart type}

    \begin{defi}{Variance et écart type}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
        \end{soient}
        \begin{itemize}
            \item La \textbf{variance} de la variable aléatoire $X$ est \[ V(X) = E\left((X-E(X))^2\right) \]
            \item L’\textbf{écart type} de la variable aléatoire $X$ est
            \[ \sigma(X) = \sqrt{V(X)} \]
            \item La variable aléatoire $X$ est dite \textbf{réduite} lorsque $\sigma(X) = 1$ $(=V(X))$
        \end{itemize}
    \end{defi}

    \begin{prop}{Formule de Huygens}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
        \end{soient}

        Alors 
        \[ V(X) = E(X^2) - (E(X))^2 \] 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{align*}
            V(X) &= E\left((X-E(X))^2\right) \\
            &= E\left(X^2 - 2XE(X) + (E(X))^2\right) \\
            &= E(X^2) - 2E(X)E(X) + (E(X))^2 \\
            &= E(X^2) - (E(X))^2
        \end{align*}
    \end{demo}

    \begin{prop}{Propriétés de la variance}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
        \end{soient}
        \begin{alors}
            \item $V(X) \geq 0$
            \item $\forall (a,b) \in \mathbb{R}^2, \, \et{V(aX + b) = a^2 V(X)}{\sigma(aX+b) = \abs{a}\sigma(X)}$
            \item Si $\sigma(X) > 0, \, \frac{X-E(X)}{\sigma(X)}$ est centrée réduite.
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item Comme $X$ est à valeurs réelles, 
            \[ E(X) = \sum\limits_{x \in X(\Omega)} x P(X=x) \in \mathbb{R} \] 
            puis $(X-E(X))^2 \geq 0$ donc $V(X) \geq 0$
            \item Soit $(a,b) \in \mathbb{R}^2$.
            \begin{align*}
                V(aX+b) &= E \left(((aX+b) - E(aX+b))^2\right) \\
                &= E\left((aX - E(aX))^2\right) \\
                &= E\left(a^2(X-E(X))^2\right) \\
                &= a^2 V(X)
            \end{align*}
            puis $\sigma(X) = \sqrt{V(X)} = \abs{a}\sigma(X)$
            \item $E\left(\frac{X-E(X)}{\sigma(X)}\right) = \frac{1}{\sigma(X)}(E(X)-E(X)) = 0$ et 
            \begin{align*}
                V\left(\frac{X-E(X)}{\sigma(X)}\right) &= V\left(\frac{1}{\sigma(X)}X - \frac{E(X)}{\sigma(X)}\right) \\
                &= \frac{1}{\sigma^2(X)}V(X) = 1 
            \end{align*}
        \end{enumerate}
    \end{demo}

    \subsubsection{Produit de VA}

    \begin{theo}{Espérance du produit de deux variables aléatoires indépendantes}{Espérance du produit de deux variables aléatoires indépendantes}
        Soit $(X,Y)$ un couple de variables aléatoires définies sur un espace probabilisé fini $(\Omega,P)$.

        On suppose que les variables $X$ et $Y$ sont indépendantes.

        Alors \[ E(XY) = E(X)E(Y) \] 
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        \begin{align*}
            E(XY) &= \sum\limits_{(x,y) \in X(\Omega)\times Y(\Omega)} xy P(X=x,Y=y)  \\
            &= \sum\limits_{(x,y) \in X(\Omega)\times Y(\Omega)} xP(X=x)yP(Y=y)  \\
            & \downarrow X, \, Y \text{ indép.} \\
            &= \sum\limits_{x \in X(\Omega)} \sum\limits_{y \in Y(\Omega)} xP(X=x)yP(Y=y)  \\
            &= \sum\limits_{x \in X(\Omega)} xP(X=x) \sum\limits_{y \in Y(\Omega)} yP(Y=y)  \\
            &= E(X) E(Y)
        \end{align*}
    \end{demo}

    \begin{defi}{Covariance}{}
        Soit $(X,Y)$ un couple de variables aléatoires défini sur un espace probabilisé fini $(\Omega,P)$. 
        \begin{itemize}
            \item La \textbf{covariance} des variables aléatoires est le nombre \[ \Cov(X,Y) = E(XY)-E(X)E(Y) \]
            \item On dit que les variables aléatoires sont \textbf{décorrélées} lorsque $\Cov(X,Y) = 0$
        \end{itemize}
    \end{defi}

    \begin{prop}{}{}
        Soit $(X,Y)$ un couple de variables aléatoires défini sur un espace probabilisé fini $(\Omega,P)$.

        Alors 
        \[ V(X+Y) = V(X) + V(Y) + 2\Cov(X,Y) \]
        En particulier, si $X \independent Y$ (ou sont même juste décorrélées), \[V(X+Y) = V(X) + V(Y) \]   
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{align*}
            V(X+Y) &= E((X+Y)^2) - (E(X+Y))^2 \\
            &= E(X^2 + 2XY + Y^2) - (E(X) + E(Y))^2 \\
            &= E(X^2) + E(Y^2)  + 2E(XY) \\ 
            &- (E(X))^2 - (E(Y))^2 - 2E(X)E(Y) \\
            &= V(X) + V(Y) + 2\Cov(X,Y)
        \end{align*}
    \end{demo}

    \subsubsection{Espérance et variance des lois classiques}

    \begin{prop}{Espérance et variance d’une loi de Bernoulli}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
            \item $p \in \intervalleFF{0}{1}$
        \end{soient}
        On suppose que $X \sim \mathcal{B}(p)$

        Alors $\et{E(X) = p}{V(X) = p(1-p)}$
    \end{prop}

    \begin{demo}{Idée}{myolive}
        Faire les calculs.
    \end{demo}

    \begin{prop}{Espérance et variance d’une loi binomiale}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé fini
            \item $X$ une variable aléatoire sur $\Omega$
            \item $n \in \mathbb{N}^*$ et $p \in \intervalleFF{0}{1}$
        \end{soient}
        On suppose que $X \sim \mathcal{B}(n,p)$

        Alors $\et{E(X) = np}{V(X) = np(1-p)}$
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{align*}
            E(X) &= \sum\limits_{k=0}^n k \binom{n}{k} p^k (1-p)^{n-k} \\
            &= 0 + \sum\limits_{k=1}^n \frac{n!}{(k-1)!(n-k)!} p^k (1-p)^{n-k} \\
            &= np \sum\limits_{k=1}^n \frac{(n-1)!}{(k-1)!((n-1)-(k-1))!} p^{k-1} (1-p)^{n-k} \\
            &= np \sum\limits_{k=0}^{n-1} \binom{n-1}{k-1} p^{k-1} (1-p)^{n-1-k} \\
            &= np \\
            E(X^2) &= \sum\limits_{k=0}^n k^2 \binom{n}{k} p^k (1-p)^{n-k} \\
            &= \underbrace{\sum\limits_{k=0}^n k(k-1) \binom{n}{k} p^k (1-p)^{n-k}}_B \\
            &+ \underbrace{\sum\limits_{k=0}^n k \binom{n}{k} p^k (1-p)^{n-k}}_{E(X)}
        \end{align*}
        Si $n =1$, on vérifie facilement le résultat.
        
        Si $n \geq 2$,
        \begin{align*}
            B &= 0 + \sum\limits_{k=2}^n \frac{n!}{(k-2)!(n-k)!} p^k (1-p)^{n-k} \\
            &= \sum\limits_{k=2}^n \frac{ p^2 n(n-1) \times (n-2)!}{(k-2)!((n-2)-(k-2))!} p^{k-2} (1-p)^{n-k} \\
            &= p^2 n(n-1) \sum\limits_{k=*}^{n-2} \binom{n-2}{k} p^{k-2} (1-p)^{n-2-k} \\
            &= p^2 n(n-1)
        \end{align*}
        Donc 
        \begin{align*}
            V(X) &= E(X^2) - (E(X))^2 \\
            &= p^2 n(n-1) + E(X) - (E(X))^2 \\
            &= np(1-p)
        \end{align*}
    \end{demo}

\subsection{Fonctions génératrices}

.

\subsection{Inégalités de concentration}

    \begin{prop}{Inégalité de Markov}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé
            \item $X$ une variable aléatoire réeel sur $\Omega$
            \item $a > 0$
        \end{soient}
        Alors 
        \[ P(\abs{X} \geq a) \leq \frac{1}{a} E(\abs{X}) \] 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On considère la variable aléatoire $\mathbb{1}_{\abs{X} \geq a}$.
        
        Soit $\omega \in \Omega$.
        \begin{itemize}
            \item Si $\abs{X} \geq a$, alors $\frac{1}{a} \abs{X(\omega)} \geq 1 = \mathbb{1}_{\abs{X} \geq a}(\omega)$
            \item Si $\abs{X} < a$, alors $\frac{1}{a} \abs{X(\omega)} \geq 0 = \mathbb{1}_{\abs{X} \geq a}(\omega)$
        \end{itemize}
        Donc $\frac{1}{a}\abs{X} \geq \mathbb{1}_{\abs{X} \geq a}$
        
        Donc $E\left(\frac{1}{a}\abs{X}\right) \geq E\left(\mathbb{1}_{\abs{X} \geq a}\right)$ \textit{i.e.} $\frac{1}{a}E\left(\abs{X}\right) \geq P\left(\abs{X} \geq a\right)$
    \end{demo}

    \begin{theo}{Inégalité de Bienaymé-Tchebycheff}{}
        \begin{soient}
            \item $(\Omega,P)$ un espace probabilisé
            \item $X$ une variable aléatoire réeel sur $\Omega$
            \item $a > 0$
        \end{soient}
        Alors 
        \[ P(\abs{X- E(X)} \geq a) \leq \frac{1}{a^2} \sigma^2(X) \] 
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        On applique l’inégalité de Markov à $\left(X - E(X)\right)^2$
        \begin{align*}
            P\big(\abs{X-E(X)} \geq a\big) &= P\left(\abs{X-E(X)}^2 \geq a^2\right) \\
            &\leq \frac{1}{a^2} E\left(\abs{X-E(X)}^2\right) = \frac{1}{a^2}V(X)
        \end{align*}
    \end{demo}
















