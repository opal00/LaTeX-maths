\chapter{Séries}
\chaptertoc

\section{Séries numériques}

    \subsection{Définition}

    \begin{defi}{Série numérique}{}
        Soit $(u_n)_{n \geq n_0}$ une suite numérique.
        \begin{itemize}
            \item Pour $n \geq n_0$, la \textbf{$n$-ième somme partielle} de la suite $(u_n)_{n \geq n_0}$ est le nombre \[ S_n = \sum\limits_{k=n_0}^n u_k \]
            \item On dit que la \textbf{série} $\sum u_n$ $\left(\text{ou } \sum\limits_{n \geq n_0} u_n \right)$ de \textbf{terme général} $(u_n)_n$ est convergente lorsque la suite $(S_n)_n$ converge.
            \item La nature de la série $\sum u_n$ est sa convergence ou sa divergence.
            \item Dans le cas où la série $\sum u_n$ est convergente, on définit la somme de la série comme étant le nombre \[ \sum\limits_{k = n_0}^{+\infty} u_k = \lim\limits_{n \rightarrow +\infty} (S_n) \]
            \item De même, pour une série convergente, on définit le $n$-ème reste par le nombre 
            \[ R_n = \sum\limits_{k = n+1}^{+\infty} u_k = \sum\limits_{k = n_0}^{+\infty} u_k - S_n \]
        \end{itemize}
    \end{defi}

    \subsection{Propriétés}

    \begin{prop}{Condition nécessaire de convergence d’une série}{}
        Soit $(u_n)_{n \geq n_0}$ une suite numérique.

        On suppose que la série $\sum u_n$ converge.

        Alors $u_n \underset{n \rightarrow +\infty}{\longrightarrow} 0$
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        $u_n = S_n - S_{n-1}$ et $(S_n)$ converge donc $\lim_{n \rightarrow +\infty} u_n = \lim_{n \rightarrow +\infty} S_n - \lim_{n \rightarrow +\infty} S_{n-1} = 0$.
    \end{demo}

    \begin{theo}{Nature des séries géométriques}{}
        Soient $z \in \mathbb{C}$ et $n_0 \in \mathbb{N}$.

        Alors la série $\sum z^n$ est convergente si et seulement si $\abs{z} < 1$. 

        Dans ce cas, $\sum\limits_{k = n_0}^{+\infty} z^k = \frac{z^{n_0}}{1-z}$
    \end{theo}

    \begin{prop}{Nature des séries télescopiques}{}
        Soit $(u_n)_{n \geq n_0}$ une suite numérique.
    
        Alors la série numérique $\sum u_{n+1}-u_n$ converge si et seulement si la suite $(u_n)_n$ converge. 
    
        Dans ce cas, $\sum\limits_{k=n_0}^{+\infty} (u_{k+1}-u_k) = \lim\limits_{n \rightarrow +\infty}u_n - u_{n_0}$
    \end{prop}

    \subsection{Série à terme général positif}

        \subsubsection{Convergence absolue}

    \begin{defi}{Convergence absolue}{}
        Soit $\sum u_n$ une série numérique.

        $\sum u_n$ est \textbf{absolument convergente} si $\sum \abs{u_n}$ est convergente.
    \end{defi}

    \begin{prop}{}{}
        Toute série absolument convergente est convergente.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Considérons $\sum u_n$ absolument convergente, et notons $S_n = \sum_{k=0}^{n} u_n$ et $T_n = \sum_{k=0}^{n} \abs{u_n}$. Rappelons que 
        \[ \sum u_n \text{ cv} \iff (S_n) \text{ cv} \iff (S_n) \text{ vérifie le C.C.} \] 
        Soient $n,p \in \mathbb{N}$, avec $n \leq p$.
            \begin{align*}
                \abs{S_n - S_p} &= \abs{\sum_{k=n+1}^{p} u_k} \\
                &\leq \sum_{k=n+1}^{p} \abs{u_k} = \abs{T_n - T_p}
            \end{align*}
        Or $(T_n)$ suit le critère de Cauchy, donc $(S_n)$ aussi.
    \end{demo}

        \subsubsection{Série à terme général positif}
    
    \begin{prop}{}{}
        \begin{soient}
            \item $\sum u_n$ une série numérique
            \item $(u_n) \in \mathbb{R}_+^{\mathbb{N}}$
        \end{soient}
        Alors $\sum u_n$ converge ssi $(S_n)$ est majorée. 

        Dans ce cas, 
        \[ \sum_{n=0}^{+\infty} u_n = \sup\left\{ S_n, n \in \mathbb{N} \right\} \] 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $n \geq 1$. $S_n - S_{n-1} = u_n \geq 0$, donc $(S_n)$ est croissante, et elle converge ssi elle est majorée.

        En cas de convergence, $\sum_{n=0}^{+\infty} u_n = \lim_{n \rightarrow +\infty}S_n = \sup\left\{ S_n, n \in \mathbb{N} \right\}$
    \end{demo}

    \begin{coro}{}{}
        \begin{soient}
            \item $\sum u_n$ et $\sum v_n$ des séries à terme général positif.
        \end{soient}
        On suppose que $(u_n) \leq (v_n)$.

        \begin{alors}
            \item Si $\sum v_n$ converge $\sum u_n$ converge.
            \item Si $\sum u_n$ diverge $\sum v_n$ diverge.
        \end{alors}
    \end{coro}

    \begin{demo}{Preuve}{myorange}
        On pose $S_n$ et $T_n$ les $n$-èmes sommes partielles de $\sum u_n$ et $\sum v_n$ respectivement. $\forall n \in \mathbb{N}, S_n \leq T_n$.
        \begin{enumerate}
            \item Si $\sum v_n$ converge, $(T_n)$ est majorée, donc $(S_n)$ aussi, donc $\sum u_n$ converge.
            \item Si $\sum u_n$ diverge, $(S_n)$ n’est pas majorée, donc $(T_n)$ non plus, donc $\sum v_n$ diverge.
        \end{enumerate}
    \end{demo}

    \begin{prop}{Critère de d’Alembert}{}
        Soit $\sum u_n$ une série numérique à terme général positif.
        \begin{suppose}
            \item $\lim_{n \rightarrow +\infty} \frac{u_{n+1}}{u_n} = \ell \in \mathbb{R}_+ \cup \{ +\infty \}$
        \end{suppose}
        \begin{alors}
            \item Si $\ell < 1$, $\sum u_n$ converge.
            \item Si $\ell > 1$, $\sum u_n$ diverge.
        \end{alors}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item Supposons $\ell < 1$. Il existe $\delta$ tel que $\ell < \delta < 1$. Comme $\lim_{n \rightarrow +\infty} \frac{u_{n+1}}{u_n} = \ell$, il existe $N$ tel que 
            \[ \forall n \geq N, \frac{u_{n+1}}{u_n} \leq \delta \quad \left(\iff u_{n+1} \leq \delta u_n\right) \] 
            On en déduit que, $\forall k \in \mathbb{N}$,
            \begin{align*}
                u_{N+k} &\leq \delta u_{N + k - 1} \\
                &\leq \delta^2 u_{N + k - 2} \\
                &\leq \delta^k u_N  
            \end{align*}
            De plus, $\sum_k \delta^k u_N$ converge car $\delta < 1$. On en déduit que $\sum u_n$ converge.
            \item Supposons $\ell > 1$. Il existe $\delta$ tel que $\ell > \delta > 1$. Comme $\lim_{n \rightarrow +\infty} \frac{u_{n+1}}{u_n} = \ell$, il existe $N$ tel que 
            \[ \forall n \geq N, \frac{u_{n+1}}{u_n} \geq \delta \quad \left(\iff u_{n+1} \geq \delta u_n\right) \] 
            On en déduit que, $\forall k \in \mathbb{N}$,
            \begin{align*}
                u_{N+k} &\geq \delta u_{N + k - 1} \\
                &\geq \delta^2 u_{N + k - 2} \\
                &\geq \delta^k u_N \limi{n}{+\infty} +\infty \quad \text{car } \delta > 1 
            \end{align*}
            Donc $\sum u_n$ diverge grossièrement.
        \end{enumerate}
    \end{demo}

    \begin{prop}{Sommation des relations de comparaison}{}
        \begin{soient}
            \item $\sum u_n$ et $\sum v_n$ des séries à termes $\geq 0$
            \item $S_n = \sum_{k=0}^n u_k$ et $S_n' = \sum_{k=0}^n v_k$
            \item $R_n = \sum_{k=n+1}^{+\infty} u_k$ et $R_n' = \sum_{k=n+1}^{+\infty} v_k$ (en cas de convergence)
        \end{soient}
        \begin{alors}
            \item Si $\sum v_n$ converge et \begin{enumerate}[label=(\alph*)]
                \item $u_n = \mathcal{O}(v_n)$, alors $\sum u_n$ converge et $R_n = \mathcal{O}(R_n')$.
                \item $u_n = o(v_n)$, alors $\sum u_n$ converge et $R_n = o(R_n')$.
                \item $u_n \sim v_n$, alors $\sum u_n$ converge et $R_n \sim R_n'$.
            \end{enumerate}
            \item Si $\sum v_n$ diverge et \begin{enumerate}[label=(\alph*)]
                \item $u_n = \mathcal{O}(v_n)$, alors on ne peut pas conclure sur la convergence de $u_n$, mais $S_n = \mathcal{O}(S_n')$.
                \item $u_n = o(v_n)$, alors on ne peut pas conclure sur la convergence de $u_n$, mais $S_n = o(S_n')$.
                \item $u_n \sim v_n$, alors $\sum u_n$ diverge, et $S_n \sim S_n'$.
            \end{enumerate}
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item Supposons que $\sum v_n$ converge, i.e. $(S_n')$ est majorée. \begin{enumerate}[label=(\alph*)]
                \item Si $u_n = \mathcal{O}(v_n)$, il existe $A \geq 0$ tq $\forall n \in \mathbb{N}, u_n \leq A v_n$. 
                
                On en déduit que $\forall n \in \mathbb{N}, S_n \leq A S_n'$, donc $(S_n)$ est majorée, puis la série $\sum u_n$ converge. 

                Soient $n < m \in \mathbb{N}$, on a 
                    \[ \sum_{k=n+1}^m u_k \leq A \sum_{k=n+1}^{m} v_k \]
                Quand $m \rightarrow +\infty$, on obtient $R_n \leq A R_n'$ donc $R_n = \mathcal{O}(R_n')$
                \item Si $u_n = o(v_n)$, $u_n = \mathcal{O}(v_n)$ donc $\sum u_n$ converge.
                
                Soit $\varepsilon > 0$, il existe $N$ tel que $\forall n \geq N, u_n \leq \varepsilon v_n$. On en déduit, si $N \leq n < m$,
                \[ \sum_{k=n+1}^{m} u_k \leq \varepsilon \sum_{k=n+1}^{m} \] 
                Quand $m \rightarrow +\infty$, on obtient $R_n \leq \varepsilon R_n'$, \textit{i.e.} $R_n = o(R_n')$.
                \item Si $u_n \sim v_n$, alors $u_n = \mathcal{O}(v_n)$, donc $\sum u_n$ converge. 
                
                Soit $\varepsilon > 0$, il existe $N$ tel que $\forall n \geq N, (1-\varepsilon)v_n \leq u_n \leq (1 + \varepsilon)v_n$. Si $N \leq n < m$, 
                \[ (1-\varepsilon)\sum_{k=n+1}^{m}v_k \leq \sum_{k=n+1}^{m}u_k \leq (1+\varepsilon)\sum_{k=n+1}^{m}v_k \] 
                Quand $m \rightarrow +\infty$, on obtient $(1-\varepsilon)R_n' \leq R_n \leq (1+\varepsilon)R_n$, \textit{i.e.} $R_n \sim R_n'$.
            \end{enumerate}
            \item Supposons que $\sum v_n$ diverge. \begin{enumerate}[label=(\alph*)]
                \item Si $u_n = O(v_n)$, il existe $A \geq 0$ tel que $\forall n \in \mathbb{N}, u_n \leq A v_n$. Donc $S_n = \sum_{k=0}^{n}u_k \leq A \sum_{k=0}^{n}v_k = A S_n'$, \textit{i.e.} $S_n = \mathcal{O}(S_n')$.
                \item Si $u_n = o(v_n)$. Soit $\varepsilon > 0$, il existe $N$ tq $\forall n \geq N, u_n \leq \frac{\varepsilon}{2} v_n$. 
                
                Soit $n \geq N$,
                \begin{align*}
                S_n = \sum_{k=0}^{n} u_k &= \sum_{k=0}^{N-1} u_k + \sum_{k=N}^{n} u_k \\
                &\leq \sum_{k=0}^{N-1} u_k + \frac{\varepsilon}{2} \sum_{k=N}^{n} v_k \\
                &\leq \sum_{k=0}^{N-1} u_k + \frac{\varepsilon}{2} \underbrace{\sum_{k=0}^{n} v_k}_{= S_n'}
                \end{align*} 
                La somme $\sum_{k=0}^{N-1} u_k$ ne dépend pas de $n$ et $S_n' \rightarrow + \infty$ (car $v_n \geq 0$). Donc $\frac{\varepsilon}{2} S_n' \rightarrow + \infty$. Donc il existe $N'$ tel que 
                \[ \forall n \geq N', \sum_{k=0}^{N-1} u_k \leq \frac{\varepsilon}{2} S_n' \] 
                Ainsi, si $n \geq \max(N,N')$, 
                \[ S_n \leq \frac{\varepsilon}{2} S_n' + \frac{\varepsilon}{2} S_n' = \varepsilon S_n' \] 
                D’où $S_n = o (S_n')$.
                \item Si $u_n \sim v_n$. Prouvons directement que $S_n \sim S_n'$.
                
                Soit $\varepsilon > 0$, il existe $N$ tq $\forall n \geq N, (1- \frac{\varepsilon}{2}) v_n \leq u_n \leq (1 + \frac{\epsilon}{2})v_n$. On en déduit que si $n \geq N$, 
                \begin{align*}
                    (1- \frac{\varepsilon}{2})\sum_{k=N}^{n} v_k &\leq \sum_{k=N}^{n} u_k \leq (1 + \frac{\varepsilon}{2})\sum_{k=N}^{n} v_k \\
                    (1- \frac{\varepsilon}{2})\sum_{k=N}^{n} v_k + \sum_{k=0}^{N-1} u_k &\leq \underbrace{ \sum_{k=0}^{n} u_k}_{= S_n} \leq (1 + \frac{\varepsilon}{2})\sum_{k=N}^{n} v_k + \sum_{k=0}^{N-1} u_k \\
                    \sum_{k=0}^{N-1} u_k - (1 - \frac{\varepsilon}{2}) \sum_{k=0}^{N-1} v_k + (1 - \frac{\varepsilon}{2})S_n' &\leq S_n \leq \sum_{k=0}^{N-1} u_k - (1 + \frac{\varepsilon}{2}) \sum_{k=0}^{N-1} v_k + (1 + \frac{\varepsilon}{2})S_n'\\
                    \underbrace{\sum_{k=0}^{N-1} (u_k - v_k)}_{=C \text{ une constante}} + (1 - \frac{\varepsilon}{2})S_n' &\leq S_n \leq \underbrace{\sum_{k=0}^{N-1} (u_k - v_k)}_{=C} + (1 + \frac{\varepsilon}{2})S_n' \\
                    C + (1- \frac{\varepsilon}{2})S_n' &\leq S_n \leq C + (1 + \frac{\varepsilon}{2})S_n' \\
                \end{align*}
                Comme $\sum v_n$ diverge et $v_n \geq 0, S_n' \rightarrow +\infty$. Il existe $N'$ tq $\forall n \geq N', C \leq \frac{\varepsilon}{2} S_n'$. De même, il existe $N''$ tq $\forall n \geq N'', -\frac{\varepsilon}{2} S_n' \leq C$. Ainsi, si $n \geq \max(N, N', N'')$,
                \[ (1-\varepsilon) S_n' \leq S_n \leq (1 + \varepsilon) S_n' \] 
                On a bien $S_n \sim S_n'$. D’où $S_n \rightarrow +\infty$ puis $\sum u_n$ diverge.
            \end{enumerate}
        \end{enumerate}
    \end{demo}

    \begin{omed}{Remarque}{myolive}
        Les points essentiels sont, si $\sum u_n$ et $\sum v_n$ sont à tg $\geq 0$, alors : 
        \begin{itemize}
            \item Si $u_n = \mathcal{O}(v_n)$ et $\sum v_n$ converge alors $\sum u_n$ converge.
            \item Si $u_n \sim v_n$, alors les séries sont de même nature.
        \end{itemize}
    \end{omed}

    \begin{omed}{Applications \textcolor{black}{(Série harmonique)}}{myolive}
        \begin{enumerate}
            \item On pose $u_n = \frac{1}{n^2}$ et $v_n = \frac{1}{n(n-1)}$. 
            
            Alors $u_n \sim v_n$ et $\sum v_n$ converge. Donc $\sum u_n$ converge, et on sait que $\sum_{k=n+1}^{+\infty} \frac{1}{k^2} \sim \sum_{k=n+1}^{+\infty} \frac{1}{k(k-1)}$.

            De plus, si on suppose $n < m$,
            \begin{align*}
                \sum_{k=n+1}^{m} \frac{1}{k(k-1)} &= \sum_{k=n+1}^{m} \frac{1}{k-1} - \frac{1}{k} \\
                &= \frac{1}{n} - \frac{1}{m} 
            \end{align*}
            Quand $m \rightarrow +\infty$, $\sum_{k=n+1}^{m} \frac{1}{k(k-1)} = \frac{1}{n}$, donc 
            \[ \sum_{k=n+1}^{+\infty} \frac{1}{k^2} \sim \frac{1}{n} \] 
            \item On appelle série harmonique
            \[ H_n = \sum_{k=1}^{n} \frac{1}{k} \] 
            Montrons que $H_n$ admet le développement asymptotique suivant :
            \[ H_n = \ln(n) + \gamma + \frac{1}{2n} + \underset{n \rightarrow + \infty}{o}(\frac{1}{n}) \] 
            On remarque que $\frac{1}{n} \sim \ln(1 + \frac{1}{n}) = \ln(n+1) - \ln(n)$. On pose donc $u_n = \frac{1}{n}$ et $v_n = \ln(1 + \frac{1}{n})$, qui sont toujours positifs. On a
            \begin{align*}
                S_n' = \sum_{k=1}^{n} \ln(1 + \frac{1}{k}) &= \sum_{k=1}^{n} \ln(k+1) - \ln(k) \\
                &= \ln(n+1) \rightarrow + \infty
            \end{align*} 
            Donc $\sum v_n$ diverge, d’où $\sum \frac{1}{n}$ diverge, et $H_n \sim S_n' = \ln(n+1)$. 

            De plus, 
            \begin{align*}
                H_n - \ln(n+1) &= \sum_{k=1}^{n} \frac{1}{k} - \sum_{k=1}^{n} \ln(1 + \frac{1}{k}) \\
                &= \sum_{k=1}^{n} \frac{1}{k} - \ln(1 + \frac{1}{k})
            \end{align*}
            On pose $u_n = \frac{1}{n} - \ln(1 + \frac{1}{n})$. Rappelons que 
            \[ \ln(1+x) = x - \frac{x^2}{2} + o(x^2) \text{ quand } x \rightarrow 0\] 
            Donc 
            \[ \ln(1 + \frac{1}{n}) = \frac{1}{n} - \frac{1}{2n^2} + o(\frac{1}{n^2}) \text{ quand } n \rightarrow +\infty \] 
            D’où $u_n \sim \frac{1}{2n^2}$. Posons désormais $v_n = \frac{1}{2n^2}$. Comme $\sum \frac{1}{n^2}$ converge et $\sum_{k=n+1}^{+\infty} \frac{1}{2n^2} \sim \frac{1}{2n}$, donc $\sum u_n$ converge. On pose 
            \[ \gamma = \sum_{n=1}^{+\infty} \frac{1}{n} - \ln(1 + \frac{1}{n}) \] 
            On sait que 
            \[ \sum_{k=n+1}^{+\infty} \frac{1}{k} - \ln(1 + \frac{1}{k}) \sim \sum_{k=n+1}^{+\infty} \frac{1}{2k^2} \sim \frac{1}{2n} \]
            Or $\sum_{k=n+1}^{+\infty} \frac{1}{k} - \ln(1 + \frac{1}{k}) = \gamma - H_n + \ln(n+1)$, donc on en déduit que 
            \[ H_n = \ln(n+1) + \gamma - \frac{1}{2n} + o(\frac{1}{n}) \] 
            Puis $\ln(n+1) = \ln(n) + \ln(1 + \frac{1}{n}) = \ln(n) + \frac{1}{n} + o(\frac{1}{n})$. En substituant à l’expression de $H_n$, on obtient le développement asymptotique de $H_n$ : 
            \[ H_n = \ln(n) + \gamma + \frac{1}{2n} + o(\frac{1}{n}) \] 
         \end{enumerate}
    \end{omed}

    \begin{prop}{Comparaison série \& intégrale}{}
        Soit $f : \intervalleFO{a}{+\infty} \rightarrow \mathbb{R}$, continue, à valeurs dans $\mathbb{R}_+$, et décroissante.

        Alors $\sum f(n)$ converge si et seulement si $\int_{a}^{X} f(t)dt$ a une limite quand $X \rightarrow +\infty$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Soit $n \geq a$.
        \[ f(n+1)  \leq \int_{n}^{n+1} f(t)dt \leq f(n) \] 
        Posons $n_0 = \lceil a \rceil$ (plus petit entier supérieur à $a$). Si $n \geq 0$, 
        \[ S_n = \sum_{k=n_0}^{n} f(k) \] 
        Comme $f(n) \geq 0$, $\sum f(n)$ converge ssi $(S_n)$ est majorée. Pour $k = n_0, \ldots, n-1$, on a 
        \[ f(k+1) \leq \int_{k}^{k+1} f(t)dt \leq f(k) \]
        En sommant ces inégalités, on obtient :
        \begin{align*}
             & \sum_{k=n_0}^{n-1} f(k+1) \leq \sum_{k=n_0}^{n-1} \int_{k}^{k+1} f(t)dt \leq \sum_{k=n_0}^{n-1} f(k) \\
        \iff & \sum_{k=n_0 + 1}^{n} f(k) \leq \int_{n_0}^{n} f(t)dt \leq S_{n-1} \\
        \iff & S_n - f(n_0) \leq \int_{n_0}^{n} f(t)dt \leq S_{n-1} 
        \end{align*}
        \begin{enumerate}
            \item Si $(S_n)$ est majorée, alors 
            \[ X \longmapsto \int_{a}^{X} f(t)dt \text{ est majorée} \] 
            Or cette application est croissante, donc elle possède une limite en $+\infty$.
            \item Si cette application a une limite en $+\infty$, elle est majorée car elle est croissante, donc $(S_n)$ est majorée, d’où la série $\sum f(n)$ converge.
        \end{enumerate}
    \end{demo}

    \begin{prop}{Séries de Riemann}{}
        Soit $\alpha \in \mathbb{R}$. On appelle \textbf{série de Riemann} la série de terme général $\sum \frac{1}{n^{\alpha}}$. 
        \[ \sum \frac{1}{n^{\alpha}} \text{ converge} \iff \alpha > 1 \] 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{itemize}
            \item Si $\alpha \leq 0$, la série diverge grossièrement.
            \item Si $\alpha > 0$, posons $f : x \longmapsto \frac{1}{x^{\alpha}}$. 
            
            Sur $\mathbb{R}_+^*$, l’application $f$ est \textit{continue} et \textit{décroissante} et à \textit{termes positifs}. Donc d’après la proposition précédente, 
            \[ \sum \frac{1}{n^{\alpha}} \text{ converge} \iff \lim_{X \rightarrow +\infty} \int_{1}^{X} \frac{1}{x^{\alpha}}dx \text{ existe} \]
            \begin{itemize}
                \item Si $\alpha \neq 1$, on a 
                \begin{align*} \int_{1}^{X} \frac{1}{x^{\alpha}}dx 
                &= \left[\frac{1}{(\alpha-1)x^{\alpha-1}}\right]_1^X \\
                &= \frac{1}{\alpha-1} \left(1 - \frac{1}{X^{\alpha-1}}\right)
                \end{align*}
                Donc 
                \[ \lim_{X \rightarrow +\infty} \int_{1}^{X} \frac{1}{x^{\alpha}}dx \text{ existe} \iff \alpha > 1 \]
                \item Si $\alpha = 1$, 
                \[ \int_{1}^{X} \frac{1}{x}dx = \ln(X) \limi{X}{+\infty} +\infty \] 
            \end{itemize}
        \end{itemize}
    \end{demo}

    \begin{prop}{Séries de Bertrand}{}
        Soient $\alpha, \beta \in \mathbb{R}$, on appelle \textbf{Série de Bertrand} la série $\sum \frac{1}{n^{\alpha}(\ln(n))^{\beta}}$.

        On a 
        \[ \sum \frac{1}{n^{\alpha}(\ln(n))^{\beta}} \text{ converge} \iff \ou{\alpha > 1}{\alpha = 1 \text{ et } \beta > 1} \] 
    \end{prop}

    \begin{demo}{Justification}{myolive}
        \begin{itemize}
            \item Si $\alpha < 0$, $n^{\alpha}(\ln(n))^{\beta} \limi{n}{+\infty} 0$ donc la série de Bertrand diverge grossièrement.
            \item Si $\alpha = 0$ et $\beta \leq 0$, la série de Bertrand diverge grossièrement de même.
            \item Si $\alpha > 1$, soit $\delta$ tel que $1 < \delta < \alpha$.
            
            \[ \frac{1}{n^{\alpha} (\ln(n))^{\beta}} = \limit{o}{n}{+\infty} \left(\frac{1}{n^{\delta}}\right) \quad \text{par croissances comparées} \] 

            De plus, $\sum \frac{1}{n^{\delta}}$ converge car $\delta > 1$. Par sommation de relations de comparaison, la série de Bertrand converge.
            \item Si $0 \leq \alpha < 1$, soit $\delta$ tel que $1 > \delta > \alpha$.
            \[ \frac{1}{n^{\delta}} = \limit{o}{n}{+\infty} \left(\frac{1}{n^{\alpha} (\ln(n))^{\beta}}\right) \quad \text{par croissances comparées} \]
            Donc apcr, on aura 
            \[ 0 \leq \frac{1}{n^{\delta}} \leq \frac{1}{n^{\alpha} (\ln(n))^{\beta}} \]
            De plus, $\sum \frac{1}{n^{\delta}}$ diverge donc d’après le premier résultat de comparaison, on sait que la série de Bertrand diverge.
            \item Si $\alpha = 1$, posons $f : x \longmapsto \frac{1}{x(\ln(x))^{\beta}}$, pour $x > 1$. On voit clairement que $f$ est continue et à termes positifs, et 
            \[ f'(x) = -\frac{\left((\ln(x))^{\beta} + \beta (\ln(x))^{\beta-1}\right)}{ \left(x(\ln(x))^{\beta}\right)^2} \] 
            est du signe de 
            \[ -\left((\ln(x))^{\beta} + \beta (\ln(x))^{\beta-1}\right) = -(\ln(x))^{\beta-1} (\ln(x) + \beta) \] 
            Donc $f$ est décroissante apcr (tel que $\ln(x) + \beta \geq 0$). Ainsi, $\sum \frac{1}{n(\ln(n))^{\beta}}$ converge ssi $\int_{A}^{X} \frac{1}{x(\ln(x))^{\beta}} dx $ a une limite finie lorsque $X \rightarrow +\infty$.
            \begin{itemize}
                \item Si $\beta \neq 1$, 
                \begin{align*}
                    \int_{A}^{X} \frac{1}{x(\ln(x))^{\beta}} dx 
                    &= \left[\frac{1}{1-\beta} (\ln(x))^{1 - \beta}\right]_A^X \\
                    &= \frac{1}{1- \beta}\left((\ln(X))^{1-\beta} - (\ln(A))^{1-\beta}\right)
                \end{align*}
                possède une limite quand $X \rightarrow +\infty$ si et seulement si $\beta > 1$.
                \item Si $\beta = 1$, on pose $f : x \mapsto \frac{1}{x \ln x}$. $f$ est continue, à termes positifs et décroissante sur $\intervalleOO{1}{+\infty}$. Donc $ \sum \frac{1}{n \ln n}$ converge si et seulement si $\int_{2}^{X} \frac{1}{x \ln x}dx$ a une limite en $X \rightarrow +\infty$. 
                
                Comme $\int_{2}^{X} \frac{1}{x \ln x} dx = \ln( \ln X) - \ln(\ln 2) \limi{X}{+\infty} + \infty$, la série $\sum \frac{1}{n \ln n}$ diverge.
            \end{itemize}
        \end{itemize}
    \end{demo}

\subsection{Série à termes quelconques}

    \begin{defi}{Suites alternées}{}
        Soit $(u_n) \in \mathbb{R}^{\mathbb{N}}$.

        On dit que $(u_n)$ est \textbf{alternée} si $\forall n \in \mathbb{N}, u_n u_{n+1} \leq 0$.
    \end{defi}

    \begin{theo}{CSSA}{}
        Soit $(u_n) \in \mathbb{R}^{\mathbb{N}}$. \begin{suppose}
            \item $(u_n)$ est alternée
            \item $(u_n) \limi{n}{+\infty} 0$
            \item $(\abs{u_n})$ est décroissante
        \end{suppose}
        Alors $\sum u_n$ converge.

        De plus, si on pose $R_n = \sum_{k=n+1}^{+\infty} u_k$, on a 
        \[ \abs{R_n} \leq \abs{u_{n+1}} \quad \text{et} \quad R_n u_{n+1} \geq 0 \]
    \end{theo}

    \begin{demo}{Démonstration du théorème}{myred}
        On pose $S_n = \sum_{k=0}^{n} u_k$. Comme $(u_n)$ est alternée, on peut supposer que $u_n$ est du signe de $(-1)^n$ (quitte à remplacer $(u_n)$ par $(-u_n)$).
        \begin{enumerate}
            \item Soit $n \in \mathbb{N}$, on a 
            \begin{align*}
                S_{2n+2} - S_{2n} 
                &= u_{n+2} + u_{n+1} \\
                &= \abs{u_{n+2}} - \abs{u_{n+1}} \\
                & \quad \downarrow \quad (\abs{u_n}) \text{ est décroissante} \\
                &\leq 0
            \end{align*}
            Donc $(S_{2n})$ est décroissante.
            \begin{align*}
                S_{2n+3} - S_{2n+1} 
                &= u_{n+3} + u_{n+2} \\
                &= + \abs{u_{n+3}} + \abs{u_{n+2}} \\
                & \quad \downarrow \quad (\abs{u_n}) \text{ est décroissante} \\
                &\geq 0
            \end{align*}
            Donc $(S_{2n+1})$ est croissante.

            Or $S_{2n+1} - S_{2n} = u_{n+1} \limi{n}{+\infty} 0$, donc $(S_{2n})$ et $(S_{2n+1})$ sont adjacentes, et convergent vers la même limite. Donc $(S_n)$ tend vers cette limite, \textit{i.e.} la série converge.
            \item Pour tout $n \in \mathbb{N}$, 
            \begin{align*}
                S_{2n+1} &\leq \ell \leq S_{2n} \\
                S_{2n+1} - \ell &\leq 0 \leq S_{2n} - \ell \\
                - R_{2n+1} &\leq 0 \leq -R_{2n} \\
                R_{2n+1} &\geq 0 \geq R_{2n}
            \end{align*}
            \begin{itemize}
                \item $R_{2n+1} \geq 0$ est du signe de $u_{n+2}$.
                \item $R_{2n} \leq 0$ est du signe de $u_{n+1}$.
            \end{itemize}
            D’où $R_n$ est du signe de $u_{n+1}$.

            Majorons désormais $R_n$. 
            \[ 0 \geq R_{2n} = \underbrace{R_{2n+1}}_{\geq 0} + u_{2n+1} \geq u_{2n+1} \] 
            D’où $\abs{R_{2n}} \leq \abs{u_{2n+1}}$. De la même façon, $\abs{R_{2n+1}} \leq \abs{u_{2n+2}}$. Ainsi, $\forall n \in \mathbb{N}, \abs{R_n} \leq \abs{u_{n+1}}$.
        \end{enumerate}
    \end{demo}

    \begin{omed}{Exemple \textcolor{black}{(Transformation d’Abel)} }{myolive}
        Posons $\theta \in \mathbb{R} \backslash \left(2 \pi \mathbb{Z}\right)$. On s’intéresse à $\sum \frac{\sin(n\theta)}{n^{\alpha}}$ 
        \begin{itemize}
            \item Si $\alpha \leq 0$, la série $\sum \frac{\sin(n\theta)}{n^{\alpha}}$ diverge grossièrement. 
            \item Si $\alpha > 1$, on a $\abs{\frac{\sin(n\theta)}{n^{\alpha}}} \leq \frac{1}{n^{\alpha}}$ où $\sum \frac{1}{n^{\alpha}}$ converge donc $\sum \frac{\sin(n\theta)}{n^{\alpha}}$ converge.
            \item Si $0 < \alpha \leq 1$, on utilise une \textbf{transformation d’Abel}. 
            
            Posons $a_n = \sin(n \theta)$ et $\varepsilon_n = \frac{1}{n^{\alpha}}$, puis $S_n = \sum_{k=1}^{n} a_k \varepsilon_k$ et $A_n = \sum_{k=1}^{n} a_k$.
    
            On a \begin{align*}
                S_n 
                &= \sum_{k=1}^n \left(A_k - A_{k+1}\right) \varepsilon_k \\
                &= \sum_{k=1}^n A_k \varepsilon_k - \sum_{k=1}^n A_{k-1} \varepsilon_k \\
                &= \sum_{k=1}^n A_k \varepsilon_k - \sum_{k=0}^{n-1} A_k \varepsilon_{k+1} \\
                &= A_n \varepsilon_n - A_0 \varepsilon_1 + \sum_{k=1}^{n-1} A_k \left(\varepsilon_k - \varepsilon_{k+1}\right)  \quad \textbf{transformation d’Abel}
            \end{align*}
            \begin{itemize}
                \item Prouvons que $(A_n)$ est majorée. 
                \begin{align*}
                    A_n 
                    &= \sum_{k=1}^n \sin(k \theta) \\
                    &= \sum_{k=0}^{n} \sin(k \theta) \\
                    &= \sum_{k=0}^{n} \Im(e^{i k \theta}) \\
                    &= \Im \left(\sum_{k=0}^{n} e^{i k \theta}\right) \\
                    &= \Im \left(\sum_{k=0}^{n} \left(e^{i \theta}\right)^k \right) \quad \text{où } e^{i \theta} \neq 1 \\
                    &= \Im \left( \frac{1 - (e^{i\theta})^{n+1}}{1 - e^{i\theta}} \right) \\
                    &= \Im \left( \frac{e^{i (n+1) \frac{\theta}{2}} \left(-2i \sin \frac{(n+1)\theta}{2}\right)}{e^{i \frac{\theta}{2}} (-2i \sin \frac{\theta}{2})} \right) \\
                    &= \Im \left(e^{i n \frac{\theta}{2}} \frac{\sin \frac{(n+1) \theta}{2}}{\sin \frac{\theta}{2}}\right) \\
                    &= \frac{\sin \frac{(n+1) \theta}{2}}{\sin \frac{\theta}{2}} \Im \left(e^{i n \frac{\theta}{2}}\right)
                \end{align*}
                Donc $ A_n = \frac{\sin\left((n+1)\frac{\theta}{2}\right) \sin\left(n \frac{\theta}{2}\right)}{\sin\left(\frac{\theta}{2}\right)}$ d’où $\abs{A_n} \leq \frac{1}{\abs{\sin\left(\frac{\theta}{2}\right)}}$ i.e. $(A_n)$ est bornée.
                \item Montrons que $\sum A_n \left(\varepsilon_n - \varepsilon_{n+1}\right)$ est absolument convergente. 
                
                Pour $n \in \mathbb{N}^*$, on a 
                \begin{align*}
                    \abs{A_n \left(\varepsilon_n - \varepsilon_{n+1}\right)} 
                    &\leq \overbrace{\frac{1}{\abs{\sin(\theta/2)}}}^{= M} \overbrace{\abs{\varepsilon_n - \varepsilon_{n+1}}}^{\geq 0} \\
                    &= M \left(\varepsilon_n - \varepsilon_{n+1}\right)
                \end{align*}
                Donc 
                \begin{align*}
                    \sum_{k=1}^{n} \abs{A_k \left(\varepsilon_k - \varepsilon_{k+1}\right)}
                    &\leq M \sum_{k=1}^{n} (\varepsilon_k - \varepsilon_{k+1}) \\
                    &= M (\varepsilon_1 - \varepsilon_{n+1}) \\
                    &= M \varepsilon_1
                \end{align*}
                D’où $\sum \abs{A_n \left(\varepsilon_n - \varepsilon_{n+1}\right)}$ est à termes positifs, avec des sommes partielles majorées, donc converge. Donc $\sum A_n \left(\varepsilon_n - \varepsilon_{n+1}\right)$ est absolument convergente \textit{i.e.} converge.
                \item En conclusion, de par l’écriture de la transformation, $(S_n)$ converge.
            \end{itemize}
    
            On a finalement obtenu que 
            \[ \sum \frac{\sin(n \theta)}{n^{\alpha}} \text{ converge} \iff \alpha > 0 \] 
        \end{itemize}
        \end{omed}
    
        \begin{omed}{Remarques}{myolive}
            \begin{enumerate}
                \item On en déduit que si $\sum a_n \varepsilon_n$ avec 
            \begin{itemize}
                \item $A_n = \sum_{k=0}^{n} a_n$ est bornée.
                \item $(\varepsilon_n)$ est décroissante et tend vers 0.
            \end{itemize}
            Alors $\sum a_n \varepsilon_n$ converge.
            \item On a donc une généralisation du CSSA (qui serait ici le cas où $a_n = (-1)^n$)
            \end{enumerate}
        \end{omed}

        \begin{defi}{Produit de Cauchy}{}
            Soient $\sum a_n$ et $\sum b_n$ deux séries numériques. 
    
            On appelle \textbf{Produit de Cauchy} la série $\sum c_n$ où, 
            \[ \forall n \in \mathbb{N}, c_n = \sum_{k=0}^{n} a_k b_{n-k} = \sum_{p+q = n} a_p b_q \] 
        \end{defi}

        \begin{prop}{Convergence d’un produit de Cauchy}{}
            Soient $\sum a_n$ et $\sum b_n$ deux séries absolument convergentes.
    
            Alors leur produit de Cauchy $\sum c_n$ est absolument convergent. De plus, 
            \[ \sum_{n=0}^{+\infty} c_n = \left(\sum_{n=0}^{+\infty} a_n\right) \cdot \left(\sum_{n=0}^{+\infty} b_n\right) \]
        \end{prop}
    
        \begin{demo}{Preuve}{myolive}
            \begin{enumerate}
                \item Dans le cas des séries à termes positifs ($a_n, b_n \geq 0$). Posons
                \[ A_n = \sum_{k=0}^{n} a_k \quad B_n = \sum_{k=0}^{n} b_k \quad C_n = \sum_{k=0}^{n} c_k \] 
                Comparons $A_n \cdot B_n$ et $C_n$.
                \begin{align*}
                    A_n B_n 
                    &= \left( \sum_{p=0}^{n} a_p \right) \left( \sum_{q=0}^{n} b_q \right) \\
                    &= \sum_{p=0}^{n} \sum_{q=0}^{n} a_p b_q \\
                    &= \sum_{(p,q) \in \intervalleEntier{0}{n}^2} a_p b_q 
                \end{align*}
                D’autre part 
                \begin{align*}
                    C_n 
                    &= \sum_{k=0}^{n} c_k \\
                    &= \sum_{k=0}^{n} \sum_{i = 0}^{k} a_i b_{k-i} \\
                    &= \sum_{k=0}^{n} \sum_{p+q = k} a_p b_q \\
                    &= \sum_{\substack{p+q \leq n \\ (p,q) \in \intervalleEntier{0}{n}^2}} a_p b_q
                \end{align*}
                On en déduit que $C_n \leq A_n B_n \quad (1)$
    
                De même, on pourrait obtenir 
                \[ C_{2n} = \sum_{\substack{p+q \leq 2n \\ (p,q) \in \intervalleEntier{0}{2n}^2}} a_p b_q \] 
                On en déduit que $A_n B_n \leq C_{2n} \quad (2)$
    
                On sait que $(A_n)$ et $(B_n)$ sont majorées car convergent et sont à termes positifs. Avec $(1)$, on obtient que $(C_n)$ est majorée d’où $\sum c_n$ converge. De plus, $C_n \leq A_n B_n \leq C_{2n}$, donc 
                \[ \lim_{n \rightarrow +\infty} C_n = \lim_{n \rightarrow +\infty} A_n \cdot \lim_{n \rightarrow +\infty} B_n \] ce qui nous donne le résultat.
                \item Posons $\sum a_n$ et $\sum b_n$ deux séries numériques et $\sum c_n$ leur produit de Cauchy. On note
                \[ A_n = \sum_{k=0}^{n} a_k \quad B_n = \sum_{k=0}^{n} b_k \quad C_n = \sum_{k=0}^{n} c_k \]  
                De plus, on pose 
                \[ \alpha_n = \abs{a_n} \quad \beta_n = \abs{b_n} \quad \gamma_n = \abs{c_n} \quad \delta_n = \sum_{k=0}^{n} \alpha_k \beta_{n-k} \]
                Soit $n \in \mathbb{N}$. 
                \begin{align*}
                    \abs{A_n B_n - C_n} 
                    &= \abs{\sum_{(p,q) \in \intervalleEntier{0}{n}^2} a_p b_q - \sum_{\substack{(p,q) \in \intervalleEntier{0}{n}^2 \\ p+q \leq n}} a_p b_q} \\
                    &= \abs{\sum_{\substack{(p,q) \in \intervalleEntier{0}{n}^2 \\ p+q > n}} a_p b_q} \\
                    & \quad \downarrow \quad \text{par inégalité triangulaire} \\
                    &\leq \sum_{\substack{(p,q) \in \intervalleEntier{0}{n}^2 \\ p+q > n}} \alpha_p \beta_q \\
                    &= \sum_{(p,q) \in \intervalleEntier{0}{n}^2} \alpha_p \beta_q - \sum_{\substack{(p,q) \in \intervalleEntier{0}{n}^2 \\ p+q \leq n}} \alpha_p \beta_q \\
                    &= \left(\sum_{p=0}^{n} \alpha_p\right) \cdot \left(\sum_{q=0}^{n} \beta_q\right) - \sum_{k=0}^{n} \delta_k 
                \end{align*}
                La série $\sum \delta_n$ est le produit de Cauchy de $\sum \alpha_n$ et $\sum \beta_n$. D’après \textbf{(i)}, 
                \[ \lim_{n \rightarrow +\infty} \left(\sum_{p=0}^{n} \alpha_p\right) \cdot \left(\sum_{q=0}^{n} \beta_q\right) - \sum_{k=0}^{n} \delta_k  = 0 \] 
                Par encadrement, on obtient donc que 
                \[ \lim_{n \rightarrow +\infty} C_n = \lim_{n \rightarrow +\infty} A_n \cdot \lim_{n \rightarrow +\infty} B_n \]
                D’où le résultat.
            \end{enumerate}
        \end{demo}

        \begin{prop}{Formule de Stirling}{}
            \[ n! \limit{\sim}{n}{+\infty} \sqrt{2 \pi n} \left( \frac{n}{e} \right)^n \]
        \end{prop}
    
        \begin{demo}{Démonstration}{myolive}
            Montrons plus précisément que 
            \[ n! = \sqrt{2 \pi n} \left( \frac{n}{e} \right)^n \left(1 + \frac{1}{12n} + \comp{o}{n}{+\infty}{\frac{1}{n}}\right) \]
            On a 
            \begin{align*}
                \ln(n!) 
                &= \ln \left(\prod_{k=1}^{n} k\right) \\
                &= \sum_{k=1}^{n} \ln(k)
            \end{align*}
            Posons $h$ l’application telle que : 
            \begin{itemize}
                \item $\forall k \in \mathbb{N}^*, \ln(h) = \ln(k)$
                \item $\forall k \in \mathbb{N}^*, \restr{\ln}{\intervalleFF{k}{k+1}}$ est affine
            \end{itemize}
            $h$ est une fonction formée des cordes de $\ln$ entre entiers consécutifs.
            Pour tout $k \in \mathbb{N}^*$, 
            \[ \int_{k}^{k+1} h(t)dt = \frac{1}{2} \left(\ln(k) + \ln(k+1)\right) \quad \text{Aire d’un trapèze} = \frac{\text{PC} + \text{GC}}{2} \]
            Donc 
            \begin{align*}
                \int_{1}^{n+1} h(t)dt 
                &= \sum_{k=1}^{n} \int_{k}^{k+1} h(t)dt \\
                &= \sum_{k=1}^{n} \frac{1}{2} \left(\ln(k) + \ln(k+1)\right) \\
                &= \sum_{k=1}^{n} \ln(k) + \frac{1}{2} \ln(n+1) \\
                &= \ln(n!) + \frac{1}{2} \ln(n+1)
            \end{align*}
            De plus, 
            \begin{align*}
                \int_{1}^{n+1} \ln(t)dt 
                &= \left[ t\ln(t) - t \right]_1^{n+1} \\
                &= (n+1)\ln(n+1) - n
            \end{align*}
            Par concavité de $\ln$, on a 
            \begin{align*}
                0 \geq \int_{1}^{n+1} \ln(t)dt - \int_{1}^{n+1} h(t)dt
                &= (n+1) \ln(n+1) -n - \ln(n!) - \frac{1}{2} \ln(n+1) \\
                &= \left( n + \frac{1}{2} \right)\ln(n+1) - n - \ln(n!) \\
                \text{Or} \quad \int_{1}^{n+1} \ln(t)dt - \int_{1}^{n+1} h(t)dt 
                &= \sum_{1}^{n} \int_{k}^{k+1} (\ln(t) - h(t)) dt \\
                \text{et} \quad e_k 
                &= \int_{k}^{k+1} (\ln(t) - h(t))dt \\
                &= \left[ t \ln(t) - t \right]_k^{k+1} - \frac{1}{2} (\ln(k) + \ln(k+1)) \\
                &= (k+1) \ln(k+1) - k \ln k -1 - \frac{1}{2} (\ln(k) + \ln(k+1)) \\
                &= \left(k + \frac{1}{2}\right) \ln\left(1 + \frac{1}{k}\right) - 1 \\
                &= \left(k+ \frac{1}{2}\right) \left(\frac{1}{k} - \frac{1}{2k^2} + \frac{1}{3k^3} + o\left(\frac{1}{k^3}\right)\right) - 1 \\
                &= \left(-\frac{1}{3} + \frac{1}{4}\right) \frac{1}{k^2} + o\left(\frac{1}{k^2}\right) \quad \text{quand } k \rightarrow +\infty \\
                &= \frac{1}{12k^2} + o\left(\frac{1}{k^2}\right) \quad \text{quand } k \rightarrow +\infty 
            \end{align*}
            Par critère de comparaison, $\sum e_k$ converge car $e_k \sim \frac{1}{12k^2} \geq 0$ et $\sum \frac{1}{n^2}$ converge.
    
            Posons $S = \sum_{n=1}^{+\infty} e_n$ et $S_n = \sum_{k=1}^{n} e_k$. On sait, par sommation des relations de comparaison, que 
            \begin{align*}
                S - S_n
                &\sim \sum_{k=n+1}^{+\infty} \frac{1}{12 k^2} \\
                &= \frac{1}{12n} \\
                \textit{i.e.} \quad S_n 
                &= S - \frac{1}{12n} + o\left(\frac{1}{n}\right) 
            \end{align*}
            De plus, 
            \[ S_n = \left(n + \frac{1}{2}\right)\ln(n+1) - n - \ln(n!) \] 
            En égalisant ces deux expressions, on obtient 
            \begin{align*}
                \ln(n!) 
                &= \left( n+\frac{1}{2} \right)\ln(n+1) - n - \left( S - \frac{1}{12n} \right) + \comp{o}{n}{+\infty}{\frac{1}{n}} \\
                &\quad \downarrow \quad \ln(n+1) = \ln(n) + \frac{1}{n} - \frac{1}{2n^2} + \comp{o}{n}{+\infty}{\frac{1}{n^2}} \\
                &= \left( n+\frac{1}{2} \right) \left( \ln(n) + \frac{1}{n} - \frac{1}{2n^2} + \comp{o}{n}{+\infty}{\frac{1}{n^2}} \right) - n - S + \frac{1}{12n} + \comp{o}{n}{+\infty}{\frac{1}{n}} \\
                &= \left( n+\frac{1}{2} \right)\ln(n) - n  + 1 - S + \frac{1}{12n} + \comp{o}{n}{+\infty}{\frac{1}{n}} \\
                &\quad \downarrow \quad \text{Par application de l’exp} \\
                n! 
                &= n^{n + \frac{1}{2}} \cdot e^{-n} \cdot e^{1-S} \cdot e^{\frac{1}{12n} + \comp{o}{n}{+\infty}{\frac{1}{n}}} \\
                &= \underbrace{e^{1-S}}_{= \text{cste}} \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 + \frac{1}{12n} + \comp{o}{n}{+\infty}{\frac{1}{n}} \right)
            \end{align*}
            En conclusion, il existe $K$ tel que 
            \[ n! = K \sqrt{n} \left( \frac{n}{e} \right)^n \left( 1 + \frac{1}{12n} + \comp{o}{n}{+\infty}{\frac{1}{n}} \right) \] 
            Il reste à calculer $K$. 
    
            Si $n \geq 2$, 
            \begin{align*}
                W_n 
                &= \int_{0}^{\pi / 2} \sin(t) \sin^{n-1} (t) dt \\
                &= \underbrace{\left[ - \cos(t) \sin^{n-1}(t) \right]_0^{\pi / 2}}_{= 0} + (n-1) \int_{0}^{\pi / 2} \cos^2(t) \sin^{n-2}(t) dt \\
                &= (n-1) \left( W_{n-2} - W_n\right) \\
                \text{d’où} \quad W_n 
                &= \frac{n-1}{n} W_{n-2}
            \end{align*}
            \begin{itemize}
                \item Si $n$ est pair, on pose $n = 2p$.
                \begin{align*}
                    W_{2p} 
                    &= \frac{2p-1}{2p} W_{2p-2} \\
                    &= \frac{2p-1}{2p} \frac{2p-3}{2p-2} W_{2p-4} \\
                    &= \frac{(2p-1)(2p-3)\cdots(1)}{(2p)(2p-2)\cdots(2)} W_0 \\
                    &= \frac{(2p)!}{(2^p p!)^2} W_0 \\
                    & \quad \downarrow \quad W_0 = \int_{0}^{\pi / 2} dt = \frac{\pi}{2} \\
                    &= \frac{\pi}{2} \frac{(2p)!}{(2^p p!)^2} 
                \end{align*}
                \item Si $n$ est impair, on pose $n = 2p + 1$.
                \begin{align*}
                    W_{2p+1} 
                    &= \frac{2p}{2p+1} W_{2p-1} \\
                    &= \frac{2p}{2p+1} \frac{2p-2}{2p-1} W_{2p-3} \\
                    &= \frac{(2p)(2p-2)\cdots(2)}{(2p+1)(2p-1)\cdots(3)} W_1 \\
                    &= \frac{(2^p p!)^2}{(2p+1)!} W_0 \\
                    & \quad \downarrow \quad W_1 = \int_{0}^{\pi / 2} \sin(t)dt = 1 \\
                    &= \frac{(2^p p!)^2}{(2p+1)!} 
                \end{align*}
            \end{itemize}
            Pour tout $t \in \intervalleFF{0}{\frac{\pi}{2}}$, $\sin^{n+1}(t) \leq \sin^n(t)$. D’où, en intégrant, $W_{n+1} \leq W_{n}$. Ainsi, 
            \[ \frac{W_{2p+1}}{W_{2p+1}} = 1 \leq \frac{W_{2p}}{W_{2p+1}} \leq \frac{W_{2p-1}}{W_{2p+1}} = \frac{2p+1}{2p} \] 
            Par encadrement, $\frac{W_{2p}}{W_{2p+1}} \limi{n}{+\infty} 1$.
            Or 
            \begin{align*}
                \frac{W_{2p}}{W_{2p+1}}
                &= \frac{(2p)! \cdot (2p+1)!}{(2^p p!)^4} \frac{\pi}{2} \\
                & \quad \downarrow \quad n! \limit{\sim}{n}{+\infty} K \sqrt{n} \left( \frac{n}{e} \right)^n \\
                &\limit{\sim}{p}{+\infty} \frac{K \sqrt{2p} \left( \frac{2p}{e} \right)^{2p} \cdot K \sqrt{2p+1} \left( \frac{2p+1}{e} \right)^{2p+1}}{\left( 2^p K \sqrt{p} \left( \frac{p}{e} \right)^p \right)^4} \\
                &\limit{\sim}{p}{+\infty} \frac{\frac{\pi}{2}}{K^2} \cdot \left( \frac{2p \cdot (2p)^2p \cdot (2p+1)^{2p+1} \cdot e^{-(4p+1)}}{p^2 \cdot 2^{4p} \cdot p^{4p} \cdot e^{-4p}} \right) \\
                &\limit{\sim}{p}{+\infty} \frac{\frac{\pi}{2}}{K^2} \left( \frac{2}{p} \cdot \frac{(2p+1)^{2p+1}}{(2p)^{2p}} \cdot \frac{1}{e} \right) \\ 
                &\limit{\sim}{p}{+\infty} \frac{\frac{\pi}{2}}{K^2} \left( \frac{2}{p} \cdot 2p \cdot \frac{1}{e} \cdot \left( \frac{2p+1}{2p} \right)^{2p} \right) \\
                &\limit{\sim}{p}{+\infty} \frac{\frac{\pi}{2}}{K^2} \frac{1}{e} \left(1 + \frac{1}{2p}\right)^{2p} \\
                & \quad \downarrow \quad \left( 1 + \frac{1}{x} \right)^x = \exp\left( x \ln(1 + 1/x) \right) \limi{x}{+\infty} e \\
                &\limit{\sim}{n}{+\infty} \frac{\frac{\pi}{2}}{K^2} 
            \end{align*}
            Donc $\frac{2\pi}{K^2} = 1 \iff K = \sqrt{2\pi}$. 
        \end{demo}

\section{Série de fonctions}

\subsection{Modes de convergence d’une série de fonctions}

    \begin{omed}{Remarque}{myyellow}
        On prendra le soin de remarquer que toutes les propriétés des séries de fonction sont les propriétés des suites de fonctions appliquées à la suite de sommes partielles de la série $\sum u_n$.
    \end{omed}

    \begin{defi}{Convergence simple}{}
        Soit $\sum f_n$ une série de fonction $I \rightarrow \mathbb{K}$ (\textit{i.e.} pour tout $n \in \mathbb{N}, f_n : I \rightarrow \mathbb{K}$).

        Alors on dit que $\sum f_n$ \textbf{converge simplement} vers $S : I \rightarrow \mathbb{K}$ si pour tout $x \in I$, la série numérique $\sum f_n(x)$ converge vers $S(x)$.

        Dans ce cas, on pose $S = \sum_{k=0}^{+\infty} f_n$.
    \end{defi}

    \begin{defi}{Convergence uniforme}{}
        Soit $\sum f_n$ une série de fonctions. 

        On dit que $\sum f_n$ \textbf{converge uniformément} vers $S$ si 
        \[ \nnorm{\infty}{S - \sum_{k=0}^{n} f_k} \limi{n}{+\infty} 0 \] 
    \end{defi}

    \begin{prop}{}{}
        Soit $\sum f_n$ une série de fonctions. 

        Si $\sum f_n$ converge uniformément vers $S$, alors $\sum f_n$ converge simplement vers $f$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $x \in I$, 
        \begin{align*}
            \abs{S(x) - \sum_{k=0}^{n}f_k(x)} 
            &\leq \nnorm{\infty}{S - \sum_{k=0}^{n} f_k} \\
            & \limi{n}{+\infty} 0 
        \end{align*}
        D’où $\sum f_n$ converge simplement vers $S$.
    \end{demo}

    \begin{defi}{Convergence normale}{}
        Soit $\sum f_n$ une série de fonction telle que pour tout $n \in \mathbb{N}, f_n : I \rightarrow \mathbb{K}$ et est bornée.

        On dit que $\sum f_n$ \textbf{converge normalement} si $\sum \nnorm{\infty}{f_n}$ converge.
    \end{defi}

    \begin{prop}{}{}
        Soit $\sum f_n$ telle que $\sum f_n$ converge normalement sur $I$.

        Alors $\sum f_n$ converge uniformément sur $I$.
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        Soit $x \in I$. Montrons que $\sum f_n(x)$ converge. 

        On sait que 
        \[ 0 \leq \abs{f_n(x)} \leq \nnorm{\infty}{f_n} \quad \text{et } \sum \nnorm{\infty}{f_n} \text{ converge} \] 
        Par critère de comparaison des séries à termes positifs, $\sum f_n$ converge absolument donc converge simplement sur $I$. Posons $S(x) = \sum_{n=0}^{+\infty} f_n(x)$. 

        Prouvons que $\sum f_n$ converge uniformément vers $S$. Soit $x \in I$.
        \begin{align*}
            \abs{S(x) - \sum_{k=0}^{n} f_k(x)} 
            &= \abs{\sum_{k=n+1}^{+\infty} f_k(x)} \\
            &\leq \sum_{k=n+1}^{+\infty} \abs{f_k(x)} \\
            &\leq \sum_{k=n+1}^{+\infty} \nnorm{\infty}{f_k} \quad \text{indép de } x \\
            & \quad \downarrow \quad \text{En passant à la borne supérieure} \\
            \nnorm{\infty}{S - \sum_{k=0}^{n} f_k} 
            &\leq \sum_{k=n+1}^{+\infty} \nnorm{\infty}{f_k} \limi{n}{+\infty} 0
        \end{align*}
        Donc $\sum f_n$ converge uniformément.
    \end{demo}

\subsection{Propriétés des séries de fonctions}

    \begin{prop}{Continuité de la somme}{}
        Soit $\sum f_n$ une série de fonctions $I \rightarrow \mathbb{K}$. 
        \begin{suppose}
            \item Pour tout $n \in \mathbb{N}, f_n \in \mathcal{C}^0$
            \item $\sum f_n$ converge uniformément sur $I$
        \end{suppose}
        Alors $S := \sum_{n=0}^{+\infty} f_n$ est une fonction continue sur $I$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On applique le résultat de continuité d’une suite de fonctions à $S_n := \sum_{k=0}^n f_k$.
    \end{demo}

    \begin{theo}{dit de la double limite}{}
        \begin{soient}
            \item $\sum f_n$ une série de fonction $I \to \mathbb{K}$
            \item $x_0$ une extrémité de $I$
        \end{soient}
        \begin{suppose}
            \item Pour tout $n \in \mathbb{N}$, $f_n$ possède une limite en $x_0$, notée $\ell_n := \lim_{x \to x_0} f_n(x)$
            \item $\sum f_n$ converge uniformément sur $I$
        \end{suppose}
        \begin{alors}
            \item $\sum \ell_n$ converge.
            \item $\lim_{x \to x_0} \sum_{n=0}^{+\infty} f_n(x) = \sum_{n=0}^{+\infty} \ell_n$
        \end{alors}
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        \begin{itemize}
            \item Montrons que $\sum \ell_n$ converge avec le critère de Cauchy. Posons $S_n = \sum_{k=0}^{n} \ell_k$ et $R_n = \sum_{k=n+1}^{+\infty} f_k$. Soient $\varepsilon > 0$ et $x \in I$. On pose $S(x) = \sum_{k=0}^{+\infty} f_k(x)$, qui existe car $\sum f_n$ converge. Pour $m \geq n$, 
            \begin{align*}
                \abs{\sum_{k=0}^{m} f_k(x) - \sum_{k=0}^{n} f_k(x)} 
                &= \abs{S(x)- \sum_{k=m+1}^{+\infty} f_k(x) - \left( S(x)- \sum_{k=n+1}^{+\infty} f_k(x) \right)} \\
                &= \abs{R_n(x) - R_m(x)} \\
                &\leq \abs{R_n(x)} + \abs{R_m(x)} \\
                &\leq \nnorm{\infty}{R_n} + \nnorm{\infty}{R_m} \\
                & \quad \downarrow \quad x \to x_0 \\
                \abs{\sum_{k=0}^{m} \ell_k - \sum_{k=0}^{n} \ell_k} 
                &\leq \nnorm{\infty}{R_n} + \nnorm{\infty}{R_m} 
            \end{align*}
            Comme $\sum f_n$ converge uniformément, on a $\nnorm{\infty}{R_n} \to 0$, donc il existe $N$ tel que $\forall n \geq N, \nnorm{\infty}{R_n} \leq \frac{\varepsilon}{2}$. Donc, si $m,n \geq N$, 
            \[ \abs{S_m - S_n} \leq \varepsilon \]   
            \textit{i.e.} $(S_n)$ vérifie le critère de Cauchy.
            \item Soit $N \in \mathbb{N}$ et $x \in I$.
            \begin{align*}
                \abs{\sum_{n=0}^{+\infty} \ell_n - \sum_{n=0}^{+\infty} f_n(x)}
                & = \abs{\sum_{n=0}^{+\infty} \ell_n - \sum_{n=0}^{N} \ell_n + \sum_{n=0}^{N} \ell_n - \sum_{n=0}^{N}f_n(x) + \sum_{n=0}^{N}f_n(x) - \sum_{n=0}^{+\infty} f_n(x)} \\
                & \leq \abs{\sum_{n=0}^{+\infty} \ell_n - \sum_{n=0}^{N} \ell_n} + \abs{\sum_{n=0}^{N} \ell_n - f_n(x)} + \abs{\sum_{n=0}^{N}f_n(x) - \sum_{n=0}^{+\infty} f_n(x)} \\
                & \leq \abs{\sum_{n=0}^{+\infty} \ell_n - \sum_{n=0}^{N} \ell_n} + \sum_{n=0}^{N} \abs{\ell_n - f_n(x)} + \nnorm{\infty}{R_N}
            \end{align*}
            Soit $\varepsilon>0$. Comme $\sum \ell_n$ converge, il existe $N_1$ tel que $\forall n \geq N_1, \abs{\sum_{k=0}^{+\infty} \ell_k - \sum_{k=0}^{n} \ell_k} \leq \frac{\varepsilon}{3} $. Comme $\sum f_n$ converge uniformément, $\nnorm{\infty}{R_n} \limi{n}{+\infty} 0$. Donc il existe $N_2$ tel que $\forall n \geq N_2, \nnorm{\infty}{R_n} \leq \frac{\varepsilon}{3}$. Posons $N = \max(N_1, N_2)$.

            Pour tout $k \in \intervalleEntier{0}{N}$, $\lim_{x \to x_0} f_k(x) = \ell_k$ donc il existe $\delta_k > 0$ tel que $\forall x \in \intervalleOO{x_0 - \delta_k}{x_0 + \delta_k} \cap I, \abs{\ell_k - f_k(x)} \leq \frac{\varepsilon}{3(N+1)}$. Posons $\delta = \min\left\{ \delta_0, \ldots, \delta_N \right\}$.

            Finalement, 
            \[ \forall x \in \intervalleOO{x_0 - \delta}{x_0 + \delta} \cap I, \forall n \geq N, \abs{\sum_{n=0}^{+\infty} \ell_n - \sum_{n=0}^{+\infty} f_n(x)} \leq \varepsilon \]   
            \textit{i.e.}
            \[ \lim_{x \to x_0} \sum_{n=0}^{+\infty} f_n(x) = \sum_{n=0}^{+\infty} \ell_n \]
        \end{itemize}
    \end{demo}

    \begin{prop}{Intégration terme à terme}{}
        \begin{soient}
            \item $\sum f_n$ définies $\intervalleFF{a}{b} \to \mathbb{K}$
        \end{soient}
        \begin{suppose}
            \item Pour tout $n \in \mathbb{N}$, $f_n$ est $\mathcal{C}^0$
            \item $\sum f_n$ converge uniformément sur $\intervalleFF{a}{b}$
        \end{suppose}
        Alors 
        \[ \int_{a}^{b} \sum_{n=0}^{+\infty} f_n(x) dx = \sum_{n=0}^{+\infty} \int_{a}^{b} f_n(x) dx \] 
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On applique le résultat sur les suites de fonctions à $(S_n)$ où $S_n = \sum_{k=0}^{n} f_k$.
    \end{demo}

    \begin{prop}{Caractère $\mathcal{C}^1$ de la somme d’une série de fonctions}{}
        Soit $\sum f_n$ une série de fonctions $I \to \mathbb{K}$. 
        \begin{suppose}
            \item Pour tout $n \in \mathbb{N}, f_n \in \mathcal{C}^1$
            \item $\sum f_n$ converge simplement vers $f$ sur $I$
            \item $\sum f_n'$ convrge uniformément vers $g$ en $I$
        \end{suppose}
        \begin{alors}
            \item $f = \sum_{n=0}^{+\infty} f_n$ est $\mathcal{C}^1$.
            \item $f' = g$.
            \item $\sum f_n$ converge uniformément vers $f$ sur $I$.
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On pose $S_n = \sum_{k=0}^{n} f_k$. On applique ensuite le résultat qui donne le caractère $\mathcal{C}^1$ à la limite d’une suite de fonctions, ici $(S_n)$.
    \end{demo}

    \begin{prop}{Caractère $\mathcal{C}^k$ d’une série de fonctions}
        Soit $\sum f_n$ une série de fonctions $I \to \mathbb{K}$.
        \begin{suppose}
            \item Pour tout $n \in \mathbb{N}$, $f_n$ est $\mathcal{C}^k$
            \item Pour tout $i \in \intervalleEntier{0}{k-1}$, $\sum f_n^{(i)}$ converge simplement sur $I$
            \item $\sum f_n^{(k)}$ converge uniformément sur $I$
        \end{suppose}
        \begin{alors}
            \item $f : = \sum f_n$ est de classe $\mathcal{C}^k$.
            \item Pour tout $i \in \intervalleEntier{1}{k}, f^{(i)} = \sum_{n=0}^{+\infty} f_n^{(i)}$.
            \item $\sum f_n^{(i)}$ est uniformément convergente sur tout $\intervalleFF{a}{b} \subset I$.
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On pose $S_n = \sum_{k=0}^{n} f_k$, et on applique le résultat qui donne le caractère $\mathcal{C}^k$ à la limite de $(S_n)$.
    \end{demo}

    \begin{omed}{Application \textcolor{black}{(Développement en série de exp)}}{myolive}
        Posons $\fonction{f_n}{\mathbb{R}}{\mathbb{R}}{x}{\frac{x^n}{n!}}$.

        \begin{itemize}
            \item Si $x \in \mathbb{R}, \frac{x^n}{n!} = \comp{\mathcal{O}}{n}{+\infty}{\frac{1}{n!2}}$. Donc $\sum f_n$ converge simplement vers $\mathbb{R}$. On note $S(x) = \sum_{k=0}^{+\infty} \frac{x^n}{n!}$.
            
            \textit{\textcolor{myolive}{Rappel :}} La fonction exp est l’unique solution du système de Cauchy \[ \et{y' = y}{y(0)=1} \quad \text{(\ding{45})} \]   

            \item Montrons que $S$ est de classe $\mathcal{C}^1$ sur $\mathbb{R}$.
            \begin{itemize}
                \item Pour tout $n \in \mathbb{N}$, $f_n$ est $\mathcal{C}^1$ sur $\mathbb{R}$.
                \item $\sum f_n$ converge simplement sur $\mathbb{R}$.
                \item Soit $\intervalleFF{a}{b} \subset \mathbb{R}$. Soient $x \in \mathbb{B}$ et $n \geq 1$.
                \[ f_n'(x) = \frac{x^{n-1}}{(n-1)!} = f_{n-1}(x) \quad \text{et} \quad f_0' = 0 \]
                Or, 
                \begin{align*}
                    \nnorm{\infty, \intervalleFF{a}{b}}{f_n'} 
                    &= \frac{\max(\abs{a}, \abs{b})^{n-1}}{(n-1)!} \\
                \end{align*}
                Qui est le terme général d’une série convergente, donc $\sum \nnorm{\infty,\intervalleFF{a}{b}}{f_n'}$ converge donc $\sum f_n'$ est normalement convergente sur $\intervalleFF{a}{b}$ donc uniformément convergente sur ce même intervalle. 
            \end{itemize}
            On en déduit que $S \in \mathcal{C^1}(\mathbb{R})$. Or,
            \begin{align*}
                S'(x)
                &= \sum_{n=0}^{+\infty} f_n'(x) \\
                &= \sum_{n=0}^{+\infty} f_{n-1}(x) \\
                &= \sum_{n=0}^{+\infty} f_n(x) \\
                &= S(x) 
            \end{align*}
            De plus, $S(0) = 1$, donc $S$ est solution du système de Cauchy (\ding{45}). Donc $S(x) = e^x$.
        \end{itemize}
    \end{omed}

    \begin{omed}{Remarques}{myolive}
        \begin{itemize}
            \item On pourrait retrouver le développement en série de exp avec la formule de Taylor-Lagrange.
            \item On pourrait étendre $x \mapsto e^x$ à $\mathbb{C}$, en posant 
            \[ e^z = \sum_{n=0}^{+\infty} \frac{z^n}{n!} \]
            En effet, pour tout $z \in \mathbb{C}$, $\abs{\frac{z^n}{n!}} = \comp{\mathcal{O}}{n}{+\infty}{\frac{1}{n^2}}$.
            \item On peut vérifier que, pour tout $a,b \in \mathbb{R}$, 
            \[ e^{a+b} = e^a e^b \] 
            En effet, 
            \begin{align*}
                e^a e^b 
                &= \left( \sum_{k=0}^{+\infty} \frac{a^n}{n!}  \right) \left( \sum_{k=0}^{+\infty} \frac{b^n}{n!} \right)
            \end{align*}
            On utilise le produit de Cauchy :
            \begin{itemize}
                \item $\frac{a^n}{n!} = \comp{\mathcal{O}}{n}{+\infty}{\frac{1}{n^2}}$ donc $\sum_{k=0}^{+\infty} \frac{a^n}{n!}$ est absolument convergente.
                \item $\frac{b^n}{n!} = \comp{\mathcal{O}}{n}{+\infty}{\frac{1}{n^2}}$ donc $\sum_{k=0}^{+\infty} \frac{b^n}{n!}$ est absolument convergente.
            \end{itemize}
            Donc leur produit de Cauchy, qui est la série de terme général $c_n$, converge. Or 
            \begin{align*}
                c_n 
                &= \sum_{k=0}^{n} \frac{a^k}{k!} \frac{b^{n-k}}{(n-k)!} \\
                &= \frac{1}{n!} \sum_{k=0}^{n} \binom{n}{k} a^k b^{n-k} \\
                &= \frac{1}{n!} (a+b)^n 
            \end{align*}
            Donc \[ \sum_{k=0}^{+\infty} = e^{a+b} \]   
            On en déduit que $e^{a+b} = e^a e^b$.
        \end{itemize}
    \end{omed}

\section{Série de Fourier}

\subsection{Expression}

    Soit $T \in \mathbb{R}^*$. On définit la pulsation, comme en physique, par $\omega = \frac{2 \pi}{T}$.
    
    Les fonctions $x \mapsto \cos(\omega x)$, $x \mapsto \sin(\omega x)$ et $x \mapsto \exp(i \omega x)$ sont alors périodiques de période $T$. De la même façon, les fonctions $x \mapsto \cos(n\omega x)$, $x \mapsto \sin(n\omega x)$ et $x \mapsto \exp(i n \omega x)$ sont $T$-périodiques pour tout entier $n$.
    
    L’objectif est de décomposer toute fonction $T$-périodique sous forme d’une combinaison linéaire de $x \mapsto \cos(n\omega x)$ et $x \mapsto \sin(n\omega x)$, ou d’une combinaison linéaire de $x \mapsto \exp(i n \omega x)$.

    \begin{prop}{Coefficients de Fourier exponentiels}{}
        Soient $f \in \mathcal{C}_{pm}(\mathbb{R},\mathbb{R})$ et $T \in \mathbb{R}^*_+$.
    
        On suppose que $f$ est $T$-périodique.
    
        On appelle \textbf{coefficients de Fourier exponentiels} de $f$ la suite $(c_n(f))_{n \in \mathbb{Z}}$ définie par 
        \[ c_n(f)= \frac{1}{T}\int_{T} f(t)e^{-i n \omega t}dt \]
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On peut faire une démonstration analogue à celle qui suit pour les coeffictients trigonométriques, ou on peut passer des $c_n$ aux $a_n$ et $b_n$ directement par les relations, pour $n \geq 1$
        \begin{align*}
            c_0 &= a_0 \\
            c_n &= \frac{1}{2} (a_n - i b_n) \\
            c_{-n} &= \frac{1}{2} (a_n + i b_n) 
        \end{align*}
    \end{demo}

    \begin{omed}{Remarque}{myolive}
        Si la fonction $f$ est paire, $c_n = c_{-n}$
        
        Si la fonction est impaire, $c_n = - c_{-n}$
        
        Si $f$ est réelle, $\ovl{c_n} = c_{-n}$
        
        Si $f$ est imaginaire pure, $\ovl{c_n} = -c_{-n}$
    \end{omed}

    \begin{prop}{Coefficients de Fourier trigonométriques}{}
        Soient $f \in \mathcal{C}_{pm}(\mathbb{R},\mathbb{R})$ et $T \in \mathbb{R}^*_+$.
        
        On suppose que $f$ est $T$-périodique.
        
        On appelle \textbf{coefficients de Fourier trigonométriques} de $f$ les deux suites $(a_n(f))_{n \geq 0}$ et $(b_n(f))_{n \geq 1}$ définies par 
        \begin{align*}
            a_0(f) &= \frac{1}{T} \int_{T} f(t)dt \\
            a_n(f) &= \frac{2}{T} \int_{T} f(t)\cos(n \omega t)dt \\
            b_n(f) &= \frac{2}{T} \int_{T} f(t)\sin(n \omega t)dt
        \end{align*}
    \end{prop}

    \begin{omed}{Démonstration}{myolive}
        On suppose que $f$ peut s’écrire comme sa forme de série de Fourier, i.e. 
        \[ S(f)(t) = a_0 + \sum\limits_{n \geq 1} \left(a_n\cos(n \omega t) + b_n \sin(n \omega t)\right) \]
        Pour alléger les calculs, nous étudierons ici seulement le cas $T = 2 \pi$ i.e. $\omega = 1$.
        \begin{enumerate}
            \item On calcule dans un premier temps $\int_{-\pi}^{\pi} f(t)dt$ i.e. la moyenne de $f$ sur sa période (à un facteur $2 \pi$ près). On a :
            \[ \forall n \geq 1, \int_{-\pi}^{\pi} \cos(nt)dt = 0 \qquad \int_{-\pi}^{\pi} \sin(nt)dt = 0 \]
            On obtient alors $ \int_{-\pi}^{\pi} f(t)dt = a_0 2 \pi$, soit 
            \[ a_0 = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)dt \]
            \item Pour obtenir l’expression des coefficients $\{a_n, b_n\}_{n \geq 1}$, on calcule d’abord :
                \begin{align*}
                    \int_{-\pi}^{\pi} \cos(nt)\cos(kt)dt &= \frac{1}{2} \int_{-\pi}^{\pi} \cos((n+k)t)dt \\
                    &+ \frac{1}{2}\cos((n-k)t)dt \\
                    &= 0 + \frac{1}{2} \int_{-\pi}^{\pi} \cos((n-k)t)dt \\
                    &= \pi \delta_{n,k} \\
                    \int_{-\pi}^{\pi} \cos(nt) \sin(kt) dt&= \frac{1}{2} \int_{-\pi}^{\pi} \sin((n+k)t)dt \\
                    &+ \frac{1}{2}\sin((n-k)t)dt \\
                    &= 0 \\
                    \int_{-\pi}^{\pi} \sin(nt)\sin(kt)dt &= \frac{1}{2} \int_{-\pi}^{\pi}\cos((n-k)t)dt \\
                    &- \frac{1}{2} \cos((n+k)t)dt \\
                    &= \frac{1}{2} \int_{-\pi}^{\pi} \cos((n-k)t)dt + 0 \\
                    &= \pi \delta_{n,k}
                \end{align*}
            Ensuite, on calcule $ \int_{-\pi}^{\pi} f(t)\cos(kt)dt$ et $\int_{-\pi }^{\pi} f(t)\sin(kt)dt$. On obtient :
                \begin{align*}
                    \int_{-\pi}^{\pi} f(t) \cos(kt) dt &= a_0 \int_{-\pi}^{\pi} \cos(kt)dt \\
                    &+ \sum\limits_{n \geq 1} a_n \int_{-\pi}^{\pi} \cos(kt) \cos(nt)dt \\
                    & + \sum\limits_{n \geq 1} b_n \int_{-\pi}^{\pi} \cos(kt) \sin(nt) dt \\
                    &= 0 + \sum\limits_{n \geq 1} \pi a_n \delta_{n,k} + 0 \\
                    &= \pi a_k \\
                    \int_{-\pi}^{\pi} f(t) \sin(kt)dt &= a_0 \int_{-\pi}^{\pi} \sin(kt)dt \\
                    &+ \sum\limits_{n \geq 1} a_n \int_{-\pi}^{\pi} \sin(kt) \cos(nt)dt \\
                    &+ \sum\limits_{n \geq 1} b_n \int_{-\pi}^{\pi} \sin(kt) \sin(nt) dt \\
                    & = 0 + 0 + \pi b_k 
                \end{align*}
                On a ainsi obtenu que $\forall n \geq 1$,
                \[ a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(t) \cos(nt)dt \quad b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(t) \sin(nt) \]
        \end{enumerate}
        \null\hfill{\textcolor{myolive}{\ding{113}}}
    \end{omed}

    \begin{omed}{Remarque}{myolive}
        Le coefficient $a_0$ est la valeur moyenne de $f$ sur une période.
    \end{omed}
    
    \begin{omed}{Exemple}{myolive}
        Soit $n \in \mathbb{N}^*$.
        
        Les coefficients de Fourier de la fonction $2\pi$-périodique définie par $f(t) = t$ pour $t \in \intervalleFO{-\pi}{\pi}$ sont 
        \[ a_0 = 0, \quad a_n = 0, \quad b_n = (-1)^{n+1}\frac{2}{n} \]
    \end{omed}

    \begin{omed}{Remarque}{myolive}
        \begin{itemize}
            \item Si $f$ est paire, $\forall n \in \mathbb{N}^*, b_n = 0$
            \item Si $f$ est impaire, $\forall n \in \mathbb{N}, a_n = 0$
            \item Si $f$ vérifie pour tout $x \in \mathbb{R}$ la relation $f\left(x + \frac{T}{2}\right) = -f(x)$, alors pour tout entier $n \in \mathbb{N}^*, a_0 = a_{2n} = b_{2n} = 0$
        \end{itemize}
    \end{omed}

    \begin{defi}{Série de Fourier}{}
        Soit $f \in \mathcal{C}_{pm}(\mathbb{R},\mathbb{R})$.

        On suppose que $f$ est $2 \pi$-périodique.

        On appelle \textbf{série de Fourier} de $f$ la série de fonctions 
        \begin{align*}
           S(f)(t) &= \sum\limits_{n=-\infty}^{+\infty} c_n e^{in \omega t}  \\
           &= a_0 + \sum\limits_{n \geq 1} \left(a_n\cos(n \omega t) + b_n \sin(n \omega t)\right)
        \end{align*}
        Les sommes partielles de cette série seront notées 
        \begin{align*}
            S_n(f)(t) &= \sum\limits_{k=-n}^n c_k(f)e^{ik \omega t} \\
            &= a_0(f) + \sum\limits_{k=1}^n\left(a_k(f)\cos(k \omega t)+b_k(f)\sin(k \omega t)\right)
        \end{align*}
    \end{defi}

    \begin{omed}{Attention}{myyellow}
        La série de Fourier n’est pas nécessairement convergente !
    \end{omed}

    \begin{omed}{Exemple}{myyellow}
        En reprenant la fonction utilisée en exemple, on a 
        \[ S(f)(t) = \sum\limits_{n=1}^{+ \infty} (-1)^{n+1} \frac{2}{n} \sin(nt) \]
    \end{omed}

\subsection{Structure préhilbertienne}

    On note $E = \mathcal{C}_T (\mathbb{R},\mathbb{R})$ l’ensemble des fonctions continues sur $\mathbb{R}$ à valeurs dans $\mathbb{R}$ qui sont $T$-périodiques.

    \begin{prop}{}{}
        L’application 
        \[ \fonction{\spr{.}{.}}{\mathcal{C}_T(\mathbb{R},\mathbb{R}) \times \mathcal{C}_T(\mathbb{R},\mathbb{R})}{\mathbb{R}}{f,g}{\spr{f}{g}=\frac{1}{T}\int_{T} f(t)g(t)dt} \] 
        est un produit scalaire sur $\mathcal{C}_T(\mathbb{R},\mathbb{R})$.
    \end{prop}

    \begin{coro}{}{}
        Le couple $\left(E, \spr{.}{.}\right)$ est un espace préhilbertien.
    \end{coro}

    \begin{prop}{}{}
        La famille 
        \[ \mathcal{F} = \left\{ t \mapsto 1, t \mapsto \sqrt{2} \cos(k \omega t), t \mapsto \sqrt{2} \sin (k \omega t) \quad k \in \mathbb{N}^* \right\}\] 
        de l’espace préhilbertien $\left(E, \spr{.}{.}\right)$ est orthonormée.
    \end{prop}

    \begin{demo}{Idée de la preuve}{myolive}
        On vérifie les deux points qui définissent une famille orthonormée : 
        \begin{enumerate}
            \item On vérifie aisément que 
            \[ \forall f \in \mathcal{F}, \norm{f}^2 = \spr{f}{f} = 1 \]
            \item Les calculs permettant de conclure que 
            \[ \forall f,g \in \mathcal{F}, f \neq g \implies \spr{f}{g} = 0 \] ont été faits dans la démonstration des formules des coefficients de Fourier trigonométriques.
        \end{enumerate}
    \end{demo}

    \begin{coro}{}{}
        Si $f \in E$, alors la fonction $S_n(f)$ est la projection orthogonale de la fonction $f$ sur le sous-espace vectoriel 
        \[ \mathcal{P}_{n,T} = \vect\left(t \mapsto 1, t \mapsto \cos(k \omega t), t \mapsto \sin (k \omega t), k \in \intervalleEntier{1}{n} \right) \]
    \end{coro}

    \begin{demo}{Preuve}{myorange}
        La famille $\mathcal{F} = \left\{ t \mapsto 1, t \mapsto \sqrt{2} \cos(k \omega t), t \mapsto \sqrt{2} \sin (k \omega t) \quad k \in \mathbb{N}^* \right\}$ est une base orthonormée de $\mathcal{P}_{n,T}$.

        Pour montrer que $S_n(f)$ est le projeté orthogonal de $f$ sur $\mathcal{P}_{n,T}$, on va décomposer $S_n(f)$ en utilisant des produits scalaires, pour retrouver ses coordonnées dans ce sous-espace vectoriel.

        Soit $n \in \mathbb{N}^*$.
        \begin{align*}
            S_n(f) &= a_0(f) + \sum\limits_{k=1}^n a_k(f) \cos(kt) + b_k(f) \sin(kt) \\
            a_0(f) &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t)dt = \spr{f}{1} \\
            & \ \downarrow \text{Soit } k \in \intervalleEntier{1}{n} \\
            a_k(f) &= \frac{2}{2\pi} \int_{-\pi}^{\pi} f(t) \cos(kt) dt = 2 \spr{f}{\cos(kt)} \\
            b_k(f) &= \frac{2}{2\pi} \int_{-\pi}^{\pi} f(t) \sin(kt) dt = 2 \spr{f}{\sin(kt)} \\
            \text{Donc } S_n(f) &= \spr{f}{1} + \sum\limits_{k=1}^n \left[2 \spr{f}{\cos(kt)}\cos(kt) + 2 \spr{f}{\sin(kt)}\sin(kt)\right] \\
            &= \spr{f}{1}1 + \sum\limits_{k=1}^n \left[\spr{f}{\sqrt{2} \cos(kt)} \sqrt{2}\cos(kt) + \spr{f}{\sqrt{2}\sin(kt)}\sqrt{2}\sin(kt)\right] \\
            &= \sum\limits_{\epsilon \in \mathcal{F}} \spr{f}{\epsilon}\epsilon
        \end{align*}
        On a exprimé $S_n(f)$ avec les coordonnées de $f$ dans une base orthonormée de $\mathcal{P}_{n,T}$, donc $S_n(f)$ est le projeté orthogonal de $f$ dans ce sev.
    \end{demo}

    \begin{coro}{}{}
        Si $f \in E$, on a l’égalité 
        \[ \norm{S_n(f)}^2 = a_0^2 + \frac{1}{2} \sum\limits_{k=1}^n \left(a_k^2 + b_k^2\right) \]
    \end{coro}

    \begin{omed}{Démonstration}{myorange}
        On calcule « simplement » $\norm{S_n(f)}^2$. On se contentera ici du cas $T = 2\pi$, ce qui permet de s’affranchir du terme $\omega$ car $\omega=1$.
        \begin{align*}
            &\norm{S_n(f)}^2 = \spr{S_n(f)}{S_n(f)}  \\
            &= \frac{1}{2 \pi} \int_{-\pi}^{\pi} \left(a_0 + \sum\limits_{k=1}^n \left(a_k\cos(k t)+b_k\sin(k t)\right) \right)^2 dt \\
            &= \underbrace{\frac{1}{2 \pi} \int_{-\pi}^{\pi} a_0^2 dt}_{= a_0^2} + \frac{1}{2 \pi} \int_{-\pi}^{\pi} 2 a_0 \sum\limits_{k=1}^n \left(a_k \cos(kt) + b_k \sin(kt)\right)dt + \frac{1}{2 \pi} \int_{-\pi}^{\pi} \left( \sum\limits_{k=1}^n \left(a_k \cos(kt) + b_k \sin(kt)\right) \right)^2dt \\
            &= a_0^2 + \underbrace{2a_0 \sum\limits_{k=1}^n \left(a_k \frac{1}{2\pi}\underbrace{\int_{-\pi}^{\pi} \cos(kt)dt}_{= 0} + b_k \frac{1}{2 \pi} \underbrace{\int_{-\pi}^{\pi} \sin(kt)dt}_{= 0} \right)}_{= 0} \\
            &+ \frac{1}{2 \pi} \int_{-\pi}^{\pi} \left( \sum\limits_{k=1}^n \left(a_k \cos(kt) + b_k \sin(kt)\right)^2 + \sum\limits_{k \neq j} (a_k \cos(kt) + b_k \sin(kt))(a_j \cos(jt) + b_j \sin(jt))\right)dt \\
            &= a_0^2 + \frac{1}{2 \pi} \sum\limits_{k=1}^n \int_{-\pi}^{\pi} \left(a_k^2 \cos^2(kt) + 2a_kb_k \cos(kt) \sin(kt) + b_k^2 \sin^2(kt)\right)dt \\
            &+ \frac{1}{2\pi} \underbrace{\sum\limits_{k \neq j} \int_{-\pi}^{\pi} \left(a_k a_j \cos(kt) \cos(jt) + a_k b_j \cos(kt) \sin(jt) + b_k a_j \sin(kt) \cos(jt) + b_k b_j \sin(kt) \sin(jt)\right) dt}_{= 0} \\
            & = a_0^2 + \frac{1}{2 \pi} \sum\limits_{k=1}^n \left( \int_{-\pi}^{\pi} \left(a_k^2 \cos^2(kt) + b_k^2\sin^2(kt)\right)dt + \underbrace{\int_{-\pi}^{\pi} 2 a_k b_k \cos(kt) \sin(kt) dt}_{= 0} \right) \\
            &= a_0^2 + \frac{1}{2 \pi} \sum\limits_{k=1}^n \pi a_k^2 + \pi b_k^2  = a_0^2 + \frac{1}{2} \sum\limits_{k=1}^n a_k^2 + b_k^2  
        \end{align*}
        \null\hfill{\textcolor{myorange}{\ding{113}}}
    \end{omed}

\subsection{Théorèmes de convergence}

    \subsubsection{Le théorème de Parseval}

    \begin{theo}{Théorème de Parseval}{}
        Soit $f \in \mathcal{C}_{pm}(\mathbb{R},\mathbb{R})$.

        On suppose que $f$ est $2 \pi$-périodique.

        Alors les séries $\sum a_n^2 (f)$ et $\sum b_n^2 (f)$ convergent et 
        \[ \frac{1}{T} \int_{T} f^2(t) dt = a_0^2(f) + \frac{1}{2} \sum\limits_{n \geq 1} \left(a_n^2(f) + b_n^2(f)\right) \]
        De même, la série $\sum c_n^2(f)$ converge et 
        \[ \frac{1}{T} \int_{T} f^2(t) dt = \sum\limits_{n \in \mathbb{Z}} c_n^2(f) \]
    \end{theo}

    \begin{omed}{Exemple}{myred}
        En reprenant la fonction $2\pi$-périodique définie par $f(t) = t$ pour $t \in \intervalleFO{-\pi}{\pi}$, le théorème de Parseval donne 
        \[ \frac{\pi^2}{3} = 2 \sum\limits_{n=1}^{+\infty} \frac{1}{n^2} \quad \text{donc } \sum\limits_{n=1}^{+\infty} \frac{1}{n^2} = \frac{\pi^2}{6} \]
    \end{omed}

    \subsubsection{Le théorème de Dirichlet}

    \begin{defi}{Régularisée d’une fonction}{}
        Soit $f \in \mathcal{C}_{pm}(\mathbb{R},\mathbb{R})$.
    
        La régularisée de la foction $f$ est la fonction $\tilde{f} \in \mathcal{F}(\mathbb{R},\mathbb{R})$ définie par 
        \[ \tilde{f}(t) = \lim\limits_{h \rightarrow 0} \left(\frac{f(t+h) + f(t-h)}{2}\right) \]
    \end{defi}

    \begin{omed}{Remarque}{myyellow}
        Si $f$ est continue, on a $f = \tilde{f}$.
    \end{omed}

    \begin{theo}{Théorème de Dirichlet}{}
        Soient $f \in \mathcal{C}^1_{pm}(\mathbb{R},\mathbb{R})$ et $T \in \mathbb{R}^*_+$.

        On suppose que $f$ est $T$-périodique.

        Alors la série de Fourier de $f$ converge en tout point de $\mathbb{R}$ vers la fonction $\tilde{f}$. 
        
        Autrement dit, on a 
        \[ \forall t \in \mathbb{R}, \tilde{f} = a_0 + \sum\limits_{n \geq 1} a_n \cos(n \omega t) + b_n \sin(n \omega t) \]
    \end{theo}

    \subsubsection{Un exemple (bien) choisi}

    Ces théorèmes de convergence permettent d’établir des résultats très intéressants sur les séries (de Riemann le plus souvent), nous en verrons ici un exemple.

    \begin{exo}{}{}
        Soit $f : \mathbb{R} \rightarrow \mathbb{R}$ la fonction $2 \pi$-périodique et paire définie par $\forall x \in \intervalleFF{0}{\pi}, f(x) = x$
        \begin{enumerate}
            \item Calculer les coefficients de Fourier de $f$
            \item En déduire la valeur des sommes
            \[ \sum\limits_{n \geq 0} \frac{1}{(2n+1)^2} \quad \text{et} \quad \sum\limits_{n \geq 0} \frac{1}{(2n+1)^4} \]
            \item En déduire la valeur des sommes
            \[ \sum\limits_{n \geq 0} \frac{1}{n^2} \quad \text{et} \quad \sum\limits_{n \geq 0} \frac{1}{n^4} \]
        \end{enumerate}
    \end{exo}

    \begin{omed}{Résolution}{nfpgreen}
        \begin{enumerate}
            \item La fonction $f$ est paire, par conséquent, $\forall n \in \mathbb{N}^*, b_n = 0$. De plus, elle est $2\pi$-périodique, donc $\omega = \frac{2\pi}{2\pi} = 1$, ce qui simplifiera les calculs.
            
            On calcule donc $a_n$ pour $n \in \mathbb{N}$. 
            \begin{align*}
                a_0 &= \frac{1}{2\pi} \int_{0}^{2\pi} f(t)dt = 2 \frac{1}{2\pi} \int_{0}^{\pi} f(t)dt = \frac{1}{\pi} \left[\frac{t^2}{2}\right]_0^{\pi} = \frac{\pi}{2} \\
                \forall n \in \mathbb{N}^*, a_n &= \frac{1}{\pi} \int_{0}^{2\pi} f(t) \cos(n\omega t) dt \\
                &= \frac{2}{\pi} \int_{0}^{\pi} t\cos(nt)dt \\
                &= \frac{2}{\pi} \Bigg(\underbrace{\left[\frac{t}{n}\sin(nt)\right]_0^{\pi}}_{= 0} - \frac{1}{n} \int_{0}^{\pi} \sin(nt)dt\Bigg) \\
                &= \frac{2}{n \pi } \left[\frac{1}{n}\cos(nt)\right]_0^{\pi} = \frac{2}{n^2 \pi} \left((-1)^n - 1\right)
            \end{align*}
            En distinguant selon la parité de $n \in \mathbb{N}^*$, on obtient 
            \[ a_0 = \frac{\pi}{2} \quad \text{et} \quad \forall p \in \mathbb{N}^*, a_{2p} = 0 \text{ et } a_{2p-1} = \frac{-4}{(2p-1)^2 \pi} \]
            \item D’une part, comme la fonction $f$ est $2\pi$-périodique, continue, et de classe $\mathcal{C}^1$ par morceaux, la série de Fourier $f$ converge vers la régularisée $\tilde{f} = f$ d’après le théorème de Dirichlet. On a donc 
            \[ \forall x \in \mathbb{R}, f(x) = \frac{\pi}{2} + \sum\limits_{p=0}^{+\infty} \frac{-4}{(2p+1)^2 \pi} \cos((2p+1)x) \] 
            En particulier, en $x = 0$, on a 
            \[ 0 = \frac{\pi}{2} + \sum\limits_{p=0}^{+\infty} \frac{-4}{(2p+1)^2 \pi} \quad \text{i.e.} \quad \sum\limits_{p=0}^{+\infty} \frac{1}{(2p+1)^2} = \frac{\pi^2}{8} \]
            D’autre part, si on applique le théorème de Parseval, on obtient 
            \begin{align*}
                \frac{\pi^2}{3} = \frac{1}{2\pi} \int_{0}^{2 \pi} f^2(t) dt &= \left(\frac{\pi}{2}\right)^2 + \frac{1}{2} \sum\limits_{p=0}^{+\infty} \left(\frac{-4}{(2p+1)^2 \pi}\right)^2 \\
                &=\frac{\pi^2}{4} + \frac{8}{\pi^2} \sum\limits_{p=0}^{+\infty} \frac{1}{(2p+1)^4}
            \end{align*} 
            Ce qui nous donne directement 
            \[ \sum\limits_{p=0}^{+\infty} \frac{1}{(2p+1)^4} = \frac{\pi^4}{96} \] 
            \item La série de terme général $\frac{1}{n^2}$ est convergente. On peut donc écrire 
            \[ \sum\limits_{k=1}^{2n} \frac{1}{k^2} = \sum\limits_{p=1}^{n} \frac{1}{(2p)^2} + \sum\limits_{p=0}^{n-1} \frac{1}{(2p+1)^2} = \frac{1}{4}\sum\limits_{p=1}^n \frac{1}{p^2} +  \sum\limits_{p=0}^{n-1} \frac{1}{(2p+1)^2} \] 
            Puis, en passant à la limite, on obtient 
            \[ \sum\limits_{n=1}^{+\infty} \frac{1}{n^2} = \frac{1}{4}\sum\limits_{n=1}^{+\infty} \frac{1}{n^2} + \frac{\pi^2}{8} \quad \text{soit} \quad \sum\limits_{n=1}^{+\infty} \frac{1}{n^2} = \frac{\pi^2}{6} \] 
            De même, la série de terme général $\frac{1}{n^4}$ est convergente et on a 
            \[ \sum\limits_{k=1}^{2n} \frac{1}{k^4} = \sum\limits_{p=1}^{n} \frac{1}{(2p)^4} + \sum\limits_{p=1}^{n-1} \frac{1}{(2p+1)^4} = \frac{1}{16} \sum\limits_{p=1}^{n} \frac{1}{p^4} +  \sum\limits_{p=1}^{n-1} \frac{1}{(2p+1)^4} \] 
            Donc en passant à la limite, 
            \[ \sum\limits_{n=1}^{+\infty} \frac{1}{n^4} = \frac{1}{16}\sum\limits_{n=1}^{+\infty} \frac{1}{n^4} + \frac{\pi^4}{96} \quad \text{soit} \quad \sum\limits_{n=1}^{+\infty} \frac{1}{n^2} = \frac{\pi^4}{90} \] 
        \end{enumerate}
    \end{omed}

\section{Séries entières}

\subsection{Rayon de convergence}

    On note $\mathbb{K}$ le corps $\mathbb{R}$ ou $\mathbb{C}$.

    \subsubsection{Définitions}

    \begin{defi}{Série entière}{}
        On appelle \textbf{série entière} une série dont le terme général est de la forme $a_n z^n$, où $(a_n)$ est une suite d’éléments de $\mathbb{K}$.

        On la note $\sum a_n z^n$.
    \end{defi}

    Il ne faut pas confondre ces séries avec des séries numériques classiques. Pour cela, on identifie si le $z$ est défini ou non. Si ce n’est pas le cas, c’est une série entière.

    \begin{defi}{Domaine de convergence}{}
        Soit $\sum a_n z^n$ une série entière.

        On appelle \textbf{domaine de convergence} de la série entière l’ensemble, noté $D$, des scalaires $z$ pour lesquels la série numérique $\sum a_n z^n$ converge.
    \end{defi}

    \begin{omed}{Exemples}{myyellow}
        \begin{enumerate}[label=\textcolor{myyellow}{\arabic*.}]
            \item $\sum z^n \quad ; \quad D = \left\{z \in \mathbb{C}, \quad \abs{z} < 1\right\}$
            \item $\sum \frac{z^n}{n!} \quad ; \quad D = \mathbb{K}$
        \end{enumerate}
    \end{omed}

    \begin{lem}{dit d’Abel}{}
        Soit $\sum a_n z^n$ une série entière et $z_0 \in \mathbb{K}$ tel que la suite $(a_n z_0^n)$ est bornée. 

        Alors pour tout scalaire $z$ tel que $\abs{z} < \abs{z_0}$, la série numérique $\sum a_n z^n$ est absolument convergente.
    \end{lem}

    \begin{demo}{Démonstration}{mybrown}
        Il existe $M \in \mathbb{R}_+$ tel que $\forall n \in \mathbb{N}, \abs{a_n z_0^n} \leq M$. Soit $z \in \mathbb{K}^*$ tel que $\abs{z} \leq \abs{z_0}$. Alors $\forall n \in \mathbb{N}$,
        \begin{align*}
            \abs{a_n z^n} 
            &= \abs{a_n z_0^n} \times \abs{\frac{z}{z_0}}^n \\
            &\leq M \abs{\frac{z}{z_0}}^n
        \end{align*}
        Comme $\abs{\frac{z}{z_0}} < 1$, la série $\sum \abs{\frac{z}{z_0}}^n$ converge. Par comparaison des séries à termes positifs, on a donc $\sum a_n z^n$ converge absolument.
    \end{demo}

    \begin{defi}{Rayon de convergence}{}
        Soit $\sum a_n z^n$ une série entière.

        On appelle \textbf{rayon de convergence} de la série entière le réel $R$ défini par 
        \[ R = \sup \big\{ \rho \in \mathbb{R}_+,\quad (a_n \rho^n) \text{ est bornée} \big\} \quad \in \mathbb{R}_+ \cup \{ +\infty \} \]
    \end{defi}

    \begin{omed}{Exemples}{myyellow}
        \begin{enumerate}[label=\textcolor{myyellow}{\arabic*.}]
            \item $\sum z^n \quad ; \quad R = 1$
            \item $\sum \frac{z^n}{n!} \quad ; \quad R = +\infty$
        \end{enumerate}
    \end{omed}

    \begin{prop}{}{}
        Soit $\sum a_n z^n$ une SE de rayon $R$.
        \begin{enumerate}
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} < R$. Alors $\sum a_n z^n$ est absolument convergente donc converge.
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} > R$. Alors $\sum a_n z^n$ diverge grossièrement.
        \end{enumerate}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item Si $\abs{z} < R$, il existe $z_0 \in \mathbb{K}$ tel que $(a_n z_0^n)$ est bornée et $\abs{z} \leq \abs{z_0}$. On conclut par le lemme d’Abel.
            \item Si $\abs{z} > R$, $(a_n z^n)$ n’est pas bornée donc $\sum a_n z^n$ diverge grossièrement.
        \end{enumerate}
    \end{demo}

    Par la connaissance du rayon, on s’approche donc largement du disque de convergence. Il manque simplement la donnée des cas pathologiques $z$ tels que $\abs{z} = R$, qu’il faudra étudier à part.

    \begin{coro}{Encadrement du domaine de convergence}{}
        Soit $\sum a_n z^n$ une série entière de rayon de convergence $R$ et de domaine de convergence $D$. Alors 
        \[ \enstq{z \in \mathbb{K}}{\abs{z} < R} \subset D \subset \enstq{z \in \mathbb{K}}{\abs{z} \leq R} \]
    \end{coro}

    \begin{defi}{Disque ouvert de convergence}{}
        Soit $\sum a_n z^n$ une série entière de rayon de convergence $R$.

        On appelle \textbf{disque ouvert de convergence} l’ensemble 
        \[ \enstq{z \in \mathbb{K}}{\abs{z} < R} \] 
        sur lequel il y a convergence absolue de la série.
    \end{defi}

    \subsubsection{Détermination du rayon de convergence}

    \begin{prop}{Critère de d’Alembert}{}
        Soit $a_n z^n$ une série entière.
        \begin{suppose}
            \item il existe $n_0$ tel que pour tout $n \geq n_0$, $a_n \neq 0$ ;
            \item $\abs{\frac{a_{n+1}}{a_n}} \limi{n}{+\infty} \ell \in \mathbb{R}_+ \cup \{+\infty\}$
        \end{suppose}
        Alors le rayon de convergence de la série entière $\sum a_n z^n$ est $R = \frac{1}{\ell}$, avec les convention $\frac{1}{0} = +\infty$ et $\frac{1}{+\infty} = 0$.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $z \in \mathbb{K}^*$. Posons $u_n(z) = a_n z^n$. On applique le critère de d’Alembert pour les séries numériques :
        \begin{align*}
            \abs{\frac{u_{n+1}(z)}{u_{n}(z)}} &= \abs{\frac{a_{n+1} z^{n+1}}{a_n z^n}} \\
            &= \abs{z} \times \abs{\frac{a_{n+1}}{a_n}} \\
            &\limi{n}{+\infty} \abs{z} \ell
        \end{align*}
        \begin{itemize}
            \item Si $\abs{z} \ell < 1$, $\sum u_n(z)$ converge, \textit{i.e.} $\sum a_n z^n$ converge. Donc $z \leq R$. Ainsi, $\abs{z} < \frac{1}{\ell} \implies \abs{z} \leq R$.
            \item Si $\abs{z} \ell > 1$, $\sum a_n z^n$ diverge, donc $z \geq R$. Ainsi, $\abs{z} > \frac{1}{\ell} \implies \abs{z} \geq R$.
        \end{itemize}
        Donc $R = \frac{1}{\ell}$. 

        Dans le cas où $\ell = 0$, On a que $\abs{\frac{a_{n+1} z^{n+1}}{a_n z^n}} \limi{n}{+\infty} 0$ donc $\sum a_n z^n$ converge et $\abs{z} \leq R$ pour tout $z \in \mathbb{K}$.
    \end{demo}

    Il faut toujours être vigilent à l’hypothèse $a_n \neq 0$ apcr, et bien noter que la réciproque est fausse.

    \begin{omed}{Exemples}{myyellow}
        \begin{enumerate}[label=\textcolor{myyellow}{\arabic*.}]
            \item $\sum z^n \quad ; \quad R = 1$
            \item $\sum \frac{z^n}{n!}$. $a_n = \frac{1}{n!}$ est bien non nul apcr, et $\frac{a_{n+1}}{a_n} = \frac{1}{n+1} \limi{n}{+\infty} 0$.
            \item $\sum n^n z^n$. $a_n = n^n$ est bien non nul apcr, et $\frac{a_{n+1}}{a_n} = (n+1)(1 + \frac{1}{n})^n \limi{n}{+\infty}$ donc le rayon de cette SE est $0$.
        \end{enumerate}
    \end{omed}

    \begin{prop}{Comparaison de deux rayons de convergence}{}
        Notons $R_a$ et $R_b$ les rayons de convergence respectifs des séries entières $\sum a_n z^n \text{ et } \sum b_n z^n$.
        \begin{alors}
            \item Si $a_n = \comp{\mathcal{O}}{n}{+\infty}{b_n}$, alors $R_a \geq R_b$.
            \item Si $a_n \limit{\sim}{n}{+\infty} b_n$, alors $R_a = R_b$.
        \end{alors}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        \begin{enumerate}
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} < R_b$. Alors $\sum b_n z^n$ est absolument convergente, et comme $\abs{a_n z^n} = \comp{\mathcal{O}}{n}{+\infty}{\abs{b_n z^n}}$, la série $\sum a_n z^n$ converge absolument pareil. Ainsi, $\abs{z} \leq R_a$. On a donc obtenu $\abs{z} \leq R_a$. 
            \item Si $a_n \limit{\sim}{n}{+\infty} b_n$, on applique le premier point dans les deux sens, d’où $R_a = R_b$.
        \end{enumerate}
    \end{demo}

    \begin{omed}{Application}{myolive}
        Soit $\sum z^{2n}$. Posons $a_n = \sisi{0}{n \text{ impair}}{1}{n \text{ pair}}$. Le critère de d’Alembert ne s’applique pas car la nullité apcr n’est pas garantie. On a que $a_n = \comp{\mathcal{O}}{n}{+\infty}{1}$ et que $\sum z^n$ a pour rayon de convergence $R = 1$. Donc $R \geq 1$. Si $z =1$, $\sum z^{2n}$ diverge, donc $R = 1$.
    \end{omed}

    \begin{omed}{Exemples}{myolive}
        \begin{enumerate}[label=\textcolor{myolive}{\arabic*.}]
            \item $\sum \ln\left(1 + \frac{1}{n}\right)$. Posons $a_n = \ln\left(1 + \frac{1}{n}\right)$ et $b_n = \frac{1}{n}$. $a_n \sim b_n$ et $\sum \frac{z^n}{n}$ a pour RDC, $R_b = 1$, donc $R_a = 1$.
            \item $\sum a_n z^n$ où $a_n = \tan\left(n \frac{\pi}{7}\right)$. La fonction tan est $\pi$ périodique donc $a_{n+7} = a_n$ d’où $(a_n)$ est bornée et $a_n = \comp{\mathcal{O}}{n}{+\infty}{1}$. Or, $\sum z^n$ a pour RDC $1$, donc $R \geq 1$. Si $z = 1$, $\sum \tan(n \frac{\pi}{7}) z^n$ diverge, donc $R \leq 1$. Ainsi, $R = 1$.
        \end{enumerate}
    \end{omed}

    \subsubsection{Opérations sur les séries entières}

    \begin{prop}{Somme de séries entières}{}
        Soient $\sum a_n z^n$ et $\sum b_n z^n$ deux séries entières de rayons de convergence $R_a$ et $R_b$. Alors la série entière $\sum (a_n + b_n) z^n$ admet un rayon de convergence $R$ tel que 
        \begin{enumerate}
            \item $R \geq \min(R_a,R_b)$ ;
            \item si $R_a \neq R_b$, alors $R = \min(R_a,R_b)$
        \end{enumerate}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} < \min(R_a, R_b)$. Alors $\sum a_n z^n$ converge et $\sum b_n z^n$ converge, donc $\sum (a_n + z_n) z^n$ converge, et donc $\abs{z} \leq R$. Ainsi, $\min(R_a,R_b) \leq R$. 
            \item Si $R_a \neq R_b$, supposons $R_a < R_b$. Soit $z \in \mathbb{K}$ tel que $R_a < \abs{z} < R_b$. Alors $\sum a_n z^n$ diverge et $\sum b_n z^n$ converge, donc $\sum (a_n + b_n) z^n$ diverge et $\abs{z} \geq R$. Donc $R_a \geq R$ \textit{i.e.} $\min(R_a,R_b) \geq R$ Ainsi, $R = \min(R_a,R_b)$.
        \end{enumerate}
        
    \end{demo}

    \begin{defi}{Produit de Cauchy de deux séries entières}{}
        Soient $\sum a_n z^n$ et $\sum b_n z^n$ deux séries entières. On appelle \textbf{produit de Cauchy} des deux séries la série entière $\sum c_n z^n$ définie par 
        \[ \forall n \in \mathbb{N}, \quad c_n = \sum_{k=0}^{n} a_k b_{n-k} = \sum_{i + j = n} a_i b_j \]
    \end{defi}

    \begin{omed}{Remarque}{myyellow}
        L’ensemble des séries entières (noté $\mathbb{K}(X)$) muni des opérations d’addition et de multiplication (au sens du produit de Cauchy) et du produit externe est une $\mathbb{K}$-algèbre.
    \end{omed}

    \begin{prop}{Produit de Cauchy}{}
        Soient $\sum a_n z^n$ et $\sum b_n z^n$ deux séries entières de rayons de convergence $R_a$ et $R_b$.
        \begin{alors}
            \item Le produit de Cauchy des  séries entières $\sum a_n z^n$ et $\sum b_n z^n$ a un rayon de convergence $R \geq \min(R_a,R_b)$.
            \item Pour tout $z \in \mathbb{K}$ tel que $\abs{z} < \min(R_a, R_b)$, 
            \[ \sum_{n=0}^{+\infty} c_n z^n = \left(\sum_{n=0}^{+\infty} a_n z^n\right) \cdotp \left(\sum_{n=0}^{+\infty} b_n z^n\right) \]
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Rappelons dans un premier temps le résultat sur les séries numériques : Si $\sum a_n$ et $\sum b_n$ sont deux séries absolument convergentes, alors $\sum c_n$ est absolument convergente et $\sum_{n \geq 0} c_n = \left(\sum_{n \geq 0} a_n\right)\times\left(\sum_{n \geq 0} b_n\right)$.
        \begin{enumerate}
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} < \min(R_a,R_b)$. Alors $\sum a_n z^n$ et $\sum b_n z^n$ sont absolument convergentes, donc leur produit de Cauchy est absolument convergente, \textit{i.e.} $\sum c_n z^n$ converge. Donc $\min(R_a, R_b) \leq R_c$.
            \item De plus, si $\abs{z} < \min(R_a,R_b)$, alors $\sum_{n \geq 0} c_n z^n = \left(\sum_{n \geq 0} a_n z^n\right)\left(\sum_{n \geq 0} b_n z^n\right)$.
        \end{enumerate}
    \end{demo}

    \begin{omed}{Exemple}{myolive}
        Pour tout $z \in \mathbb{C}$, on pose $e^z = \sum_{n=0}^{+\infty} \frac{z^n}{n!}$. On sait que le RDC de la SE $\sum a_n z^n$ où $a_n = \frac{1}{n!}$ est $+\infty$. Soit $z \in \mathbb{C}$. 
        \begin{align*}
            e^z \times e^z 
            &= \sum_{n=0}^{+\infty} \left(\sum_{k=0}^{n} \frac{1}{k!} \times \frac{1}{(n-k)!}\right) z^n \\
            &= \sum_{n=0}^{+\infty} \left(\sum_{k=0}^{n} \binom{n }{k } \right) \frac{z^n}{n!} \\
            &= e^{2 z}
        \end{align*}
        On appelle ce résultat (généralisable en $e^z e^{z'} = e^{z + z'}$) l’équation fonctionnelle de exp.
    \end{omed}

    \begin{omed}{Remarque}{myolive}
        Si on pose $\sum a_n z^n$ et $\sum b_n z^n$ deux SE de RDC $R_a$ et $R_b$, et $\sum c_n z^n$ leur produit de Cauchy de RDC $R_c$. On peut avoir $R_a \neq R_b$ et $R_c > \min(R_a,R_b)$.

        En effet, considérons $a_n = 1$ pour tout $n \in \mathbb{R}$ (d’où $R_a = 1$) et $b_0 = 1, b_1 = -1$ puis $b_n = 0$ si $n \geq 2$ (d’où $R_b = +\infty)$. Si $\abs{z} < 1$, 
        \[ \left(\sum_{k=0}^{+\infty} z^n\right)(1-z) = 1 \]    

        Calculons le produit de Cauchy de $\sum a_n z^n$ et $\sum b_n z^n$. Si $n = 0$, $c_0 = a_0 b_0 = 1$. Si $n \geq 1$, $c_n = \sum_{k=0}^{n} a_k b_{n-k} = \sum_{k=0}^{1} b_k = 0$, donc $R_c = +\infty \neq \min(R_a,R_b)$.
    \end{omed}

    \begin{omed}{Exercice}{myolive}
        Soient $\sum a_n z^n$ et $\sum b_n z^n$ deux SE telles que $R_a = R_b$, et $\forall n \in \mathbb{N}, a_n b_n = 0$. Montrer que le RDC de $\sum (a_n + b_n)z^n$ est égal à $R_a = R_b$.
    \end{omed}

    \begin{demo}{Résolution}{myolive}
        On note $R$ le RDC cherché. On sait dans un premier temps que $R \geq \min(R_a,R_b)$. Soit $z \in \mathbb{K}$ tel que $\abs{a_n z^n}$ n’est pas majoré, \textit{i.e.} $\left(a_n \abs{z}^n\right)$ n’est pas bornée. Alors $\abs{z} \geq R_a$. On a, pour tout $n \in \mathbb{N}$, $\abs{a_n + b_n}\abs{z^n} \geq \abs{a_n} \abs{z}^n$ donc $\abs{a_n + b_n} \abs{z}^n$ n’est pas majorée, donc $\abs{z} \geq R$. Ainsi, $R_a \geq R$ et puis $R = R_a$.
    \end{demo}

    \begin{defi}{Primitives et dérivée d’une série entière}{}
        Soit $\sum a_n z^n$ une série entière. On appelle \textbf{primitive} de cette série une série entière 
        \[ \sum a_n \frac{z^{n+1}}{n+1} + C \]
        On appelle \textbf{dérivée} de la SE la série entière 
        \[ \sum_{n \geq 1} n a_n z^{n-1} = \sum (n+1) a_{n+1} z^n \]   
    \end{defi}

    En particulier, toute série entière possède une dérivée et des primitives.

    \begin{prop}{Rayon de convergence de la primitive}{}
        Soit $\sum a_n z^n$ une SE de rayon de convergence $R > 0$. 
        Les séries entières $\sum n a_n z^{n-1}$ et $\sum a_n \frac{z^{n+1}}{n+1}$ ont le même rayon de convergence.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Il suffit de montrer que les séries entières $\sum a_n z^n$ et $\sum a_n \frac{z^{n+1}}{n+1}$ ont le même RDC. Notons $R$ et $R'$ les RDC respectifs de ces deux SE. 

        On montre, comme dans de nombreuses preuves avec les SE, que $\et{R \geq R'}{R \leq R'}$. \begin{itemize}
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} < R$. D’après le lemme d’abel, $\sum a_n z^n$ est absolument convergente, donc $a_n z^n \limi{n}{+\infty} 0$ et donc il existe $M$ tel que $\forall n \in \mathbb{N}, \abs{a_n z^n} < M$. Donc $\abs{a_n \frac{z^{n+1}}{n+1}} = \frac{\abs{z}}{n+1} \times \abs{a_n z^n} \leq \frac{\abs{z} M}{n+1} \limi{n}{+\infty} 0$. Donc $\abs{z} \leq R'$, \textit{i.e.} $R \leq R'$.
            \item Soit $z \in \mathbb{K}$ tel que $\abs{z} < R'$. IL existe $\rho \in \mathbb{R}_+$ tel que $\abs{z} < \rho < R'$, donc $\sum a_n \frac{\rho^{n+1}}{n+1}$ converge absolument, d’où l’existence de $M$ tel que $\forall n \in \mathbb{N}, \abs{a_n \frac{\rho^{n+1}}{n+1}} < M$. On en déduit que, pour $z \neq 0$,
            \begin{align*}
                \abs{a_n z^n} 
                &= \frac{n+1}{\abs{z}} \times \abs{a_n \frac{z^{n+1}}{n+1}} \\
                &= \frac{n+1}{\abs{z}} \times \abs{a_n \frac{\rho^{n+1}}{n+1}} \times \abs{\frac{z}{\rho}}^{n+1} \\
                &= \frac{n+1}{\rho} \times \abs{a_n \frac{\rho^{n+1}}{n+1}} \times \abs{\frac{z}{\rho}}^{n} \\
                &\leq \frac{M}{\rho} (n+1) \abs{\frac{z}{\rho}}^n \\
                &\limi{n}{+\infty} 0
            \end{align*}
            Donc $\abs{z} \leq R$ \textit{i.e.} $R' \leq R$.
        \end{itemize}
        Finalement, on a obtenu que $R = R'$.
    \end{demo}

\subsection{Régularité de la somme d’une série entière}

    Dans cette section, on s’intéresse à des SE réelles.

    \subsubsection{Convergence normale}

    \begin{prop}{}{}
        Soit $\sum a_n z^n$ une série entière de rayon de convergence $R > 0$.

        Alors pour tout $D = \intervalleFF{a}{b} \subset \intervalleOO{-R}{R}$ (ou tout disque fermé $D$ inclus dans $\enstq{z \in \mathbb{C}}{\abs{z} < R}$), la série converge normalement.
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Posons $f_n : x \mapsto a_n x^n$ définie sur $\intervalleOO{-R}{R}$. On a 
        \begin{align*}
            \nnorm{\infty, \intervalleFF{a}{b}}{f_n} 
            &= \abs{a_n} \max\left(\abs{a}, \abs{b}\right)^n \\
        \end{align*}
        où $\max(\abs{a}, \abs{b}) < R$ donc la série $\sum \nnorm{\infty, \intervalleFF{a}{b}}{f_n}$ converge, \textit{i.e.} la série est normalement convergente sur $\intervalleFF{a}{b} \subset \intervalleOO{-R}{R}$.
    \end{demo}

    Cette convergence n’est pas valable sur le disque ouvert, mais seulement sur tout intervalle qui y est inclus. 

    \begin{omed}{Contre-exemple sur le disque ouvert}{myolive}
        OC $\sum x^n$ de rayon de convergence $R = 1$. OP $f_n(x) = x^n$. $\nnorm{\infty, \intervalleOO{-1}{1}}{f_n} = 1$ et $\sum 1$ diverge, donc $\sum x^n$ ne converge pas normalement sur $\intervalleOO{-1}{1}$.
    \end{omed}

    \begin{coro}{Continuité de la somme d’une SER}{}
        L’application $S : x \longmapsto \sum_{n=0}^{+\infty} a_n x^n$ est continue sur $\intervalleOO{-R}{R}$.
    \end{coro}

    \begin{demo}{Idée}{myorange}
        OP $f_n : x \mapsto a_n x^n$, qui est $\mathcal{C}^0$ sur $\intervalleOO{-R}{R}$. De plus, $\sum f_n$ converge normalement donc uniformément sur tout $\intervalleFF{a}{b} \subset \intervalleOO{-R}{R}$, donc $S$ est continue (d’après le théorème de continuité pour une somme de série de fonction).
    \end{demo}

    On se place désormais dans le cas réel. Rappelons que la fonction somme est alors définie sur $\intervalleFO{-R}{R}$, $\intervalleOO{-R}{R}$, $\intervalleFO{-R}{R}$ ou $\intervalleFF{-R}{R}$. Si le théorème précédent garanti la continuité de la somme sur $\intervalleOO{-R}{R}$, il ne dit rien de l’éventuelle continuité en $R$ et $-R$ en cas de convergence. 
    
    Remarquons que si la série $\sum a_n R^n$ converge absolument, $\sum a_n x^n$ converge normalement sur $\intervalleFF{-R}{R}$ puisque 
    \[ \forall x \in \intervalleFF{-R}{R}, \quad \abs{a_n x^n} \leq \abs{a_n} R^n \]   
    Dans ce cas, la somme est continue sur $\intervalleFF{-R}{R}$. Le résutat reste vrai en cas de semi-convergence comme l’exprime le théorème de convergence radial d’Abel.

    \begin{theo}{Théorème de convergence radial d’Abel}{}
        Soit $S(x) = \sum_{n=0}^{+\infty} a_n x^n$ telle que $\sum a_n x^n$ est de rayon de convergence $R > 0$ et $\sum a_n R_n$ converge, alors $S$ est continue en $R$ \textit{i.e.} 
        \[ \lim_{x \to R^-} S(x) = S(R) \]  
    \end{theo}

    \begin{demo}{Preuve}{myred}
        Quitte à procéder à un changement de variable, on peut se contenter de montrer le résultat pour une série entière $\sum a_n x^n$ de RDC $R = 1$. Supposons donc que $\sum a_n$ converge et montrons que $f(x) = \sum_{n = 0}^{+\infty} a_n x^n \limi{x}{1^-} \sum_{n=0}^{+\infty} a_n = S$.
        \begin{itemize}
            \item Soit $x \in \intervalleFO{0}{1}$ et $p \in \mathbb{N}$. En notant $R_n$ le reste de la série $\sum a_n$ au rang $n$, 
            \begin{align*}
                \sum_{n=1}^{p} a_n x^n - \sum_{n=1}^{p} a_n 
                &= \sum_{n=1}^{p} a_n (x^n - 1) \\
                &= \sum_{n=1}^{p} (R_{n-1} - R_n)(x^n - 1) \\
                &= \sum_{n=0}^{p-1} R_n (x^{n+1} - 1) - \sum_{n=1}^{p} R_n (x^n - 1) \\
                &= (x-1) \sum_{n=0}^{p} R_n x^n + (1 - x^p) R_p 
            \end{align*}
            En faisant tendre $p$ vers $+\infty$, il vient que $f(x)- S = (x-1) \sum_{n=0}^{+\infty} R_n x^n$.
            \item Soient maintenant $\varepsilon > 0$ et $x \in \intervalleFO{0}{1}$. Comme $R_n \limi{n}{+\infty} 0$, il existe $p \in \mathbb{N}^*$ tel que $\abs{R_p} < \varepsilon$. Ainsi, 
            \[ \abs{f(x) - S} < (1 - x) \sum_{n=0}^{+\infty} \abs{R_n} x^n < (1 - x) \sum_{n = 0}^{p-1} \abs{R_n} x^n + (1 - x)\varepsilon \sum_{n=p}^{+\infty} x^n < (1 - x) \sum_{n=0}^{p-1} \abs{R_n} x^n + \varepsilon \]  
            De plus, $(1-x) \sum_{n=0}^{p-1} \abs{R_n} x^n \limi{x}{1^-} 0$ donc il existe $\delta$ tel que pour tout $x \in \intervalleOF{1 - \delta}{1}$, $(1 - x) \sum_{n=0}^{p-1} \abs{R_n} x^n < \varepsilon$. Ainsi, pour pour tout $x \in \intervalleOF{1 - \delta}{1}$, $\abs{f(x) - S} < 2 \varepsilon$.  
        \end{itemize}
    \end{demo}

    L’argument précédent étant valable pour $x = -R$, on obtient le précieux corollaire suivante, valable uniquement pour les SE$\mathbb{R}$ : 

    \begin{coro}{}{}
        La somme d’une série entière de la variable réelle est continue sur l’intervalle de convergence.
    \end{coro}

    \subsubsection{Intégration terme à terme}

    \begin{theo}{Intégration terme à terme}{}
        Soit $\sum a_n x^n$ une série entière \textcolor{myred}{réelle} de rayon de convergence $R > 0$. 

        Pour tout $x \in \intervalleOO{-R}{R}$,
        \begin{align*}
            \int_{0}^{x} \left(\sum_{n=0}^{+\infty} a_n t^n \right) dt 
            &= \sum_{n=0}^{+\infty} \int_{0}^{x} a_n t^n dt \\
            &= \sum_{n=0}^{+\infty} a_n \frac{x^{n+1}}{n+1}
        \end{align*}
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        OP $f_n(t) = a_n t^n$. La série de fonction $\sum f_n$ converge normalement sur $\intervalleFF{0}{x}$ donc uniformément sur $\intervalleFF{0}{x}$. Par le théorème d’intégration terme à terme, on a donc 
        \[ \int_{0}^x \sum_{n=0}^{+\infty} a_n t^n dt = \sum_{n=0}^{+\infty} \int_{0}^{x} a_n t^n dt \]
    \end{demo}

    On peut aussi passer par une preuve directe, qui est applicable dans la pratique aussi :

    \begin{demo}{Preuve directe}{myred}
        \begin{align*}
            \abs{\int_{0}^{x} \sum_{n=0}^{+\infty} a_n t^n dt - \sum_{n=0}^{N} \int_{0}^{x} a_n t^n dt} 
            &= \abs{\int_{0}^{x} \sum_{n=0}^{+\infty} a_n t^n - \sum_{n=0}^{N} a_n t^n dt} \\
            &= \abs{\int_{0}^{x} \sum_{n=N+1}^{+\infty} a_n t^n dt} \\
            &\leq \int_{0}^{x} \sum_{n=N+1}^{+\infty} \abs{a_n} x^n dt \\
            & \quad \downarrow \quad \text{reste d’une série cv} \\
            &\limi{n}{+\infty} 0
        \end{align*}
    \end{demo}

    \begin{omed}{Exemple}{myred}
        OC $\sum x^n$, de RDC $R = 1$. Pour $x \in \intervalleOO{-1}{1}$, on a 
        \[ \frac{1}{1-x} = \sum_{n=0}^{+\infty} x^n \]   
        Par le théorème d’ITT, 
        \begin{align*}
            \int_{0}^{x} \frac{dt}{1-t} 
            &= \sum \int_{0}^{x} t^n dt \\
            \textit{i.e. } -\ln(1-x) 
            &= \sum_{n=0}^{+\infty} \frac{x^{n+1}}{n+1} \\
        \end{align*}
        Donc, pour $x \in \intervalleOO{-1}{1}$, $\ln(1-x) = \sum_{n=1}^{+\infty} \frac{x^{n+1}}{n+1}$, ou encore $\ln(1+x) = \sum_{n=0}^{+\infty} (-1)^n \frac{x^{n+1}}{n+1}$.
    \end{omed}

    \subsubsection{Caractère C8}

    \begin{theo}{Caractère $\mathcal{C}^{\infty}$}{}
        Soit $\sum a_n x^n$ une série entière \textcolor{myred}{réelle} de rayon de convergence $R > 0$. 

        Alors l’application $S : x \mapsto \sum_{n=0}^{+\infty} a_n x^n$ est de classe $\mathcal{C}^{\infty}$ sur $\intervalleOO{-R}{R}$.

        De plus, pour tout $k \in \mathbb{N}$, pour tout $x \in \intervalleOO{-R}{R}$,
        \begin{align*}
            S^{(k)}(x) 
            &= \sum_{n=k}^{+\infty} n(n-1) \cdots (n-k+1) a_n x^{n-k} \\
            &= \sum_{n=0}^{+\infty} \frac{(n+k)!}{n!} a_{n+k} x^n
        \end{align*}
        En particulier, toutes les séries entières ont le même rayon de convergence $R$.
    \end{theo}

    \begin{demo}{Preuve}{myred}
        On s’appuie sur le théorème donnant le caractère $\mathcal{C}^k$ d’une série de fonctions. OP $f_n : x \mapsto a_n x^n$.
        \begin{enumerate}[label=$(h_{\alph*})$]
            \item $\forall n \in \mathbb{N}$, $f_n \in \mathcal{C}^{\infty}$ sur $\intervalleOO{-R}{R}$.
            \item Si $\intervalleFF{a}{b} \subset \intervalleOO{-R}{R}$, $\sum f_n^{(k)} = \sum \frac{(n+k)!}{n!} a_{n+k} x^n$ a le même rayon de convergence que $\sum a_n x^n$ donc son rayon de convergence est $R > 0$. Ainsi, $\sum f_n^{(k)}$ converge normalemente sur $\intervalleFF{a}{b}$.
        \end{enumerate}
        Ainsi, $S^{(k)}$ est de classe $\mathcal{C}^{\infty}$ sur $\intervalleOO{-R}{R}$.
    \end{demo}

    \begin{coro}{}{}
        Soit $\sum a_n x^n$ une série entière \textcolor{myorange}{réelle} de rayon de convergence $R > 0$. 

        Pour tout $n \in \mathbb{N}$, 
        \[ a_n = \frac{S^{(n)}(0)}{n!} \]   
        D’où $S(x) = \sum_{n=0}^{+\infty} \frac{S^{(n)}(0)}{n!} x^n$, \textit{i.e.} $S$ est la somme de sa série de Taylor.
    \end{coro}

    \begin{omed}{Exercice}{myred}
        Soit la SER $\sum n^2 x^n$.
        \begin{enumerate}
            \item RDC ?
            \item Expression de la somme $S$.
        \end{enumerate}
    \end{omed}

    \begin{demo}{Résolution}{myred}
        \begin{enumerate}
            \item On applique le critère de d’Alembert : $\frac{(n+1)^2}{n^2} \limi{n}{+\infty} 1$ donc $R = 1$.
            \item Soit $x \in \intervalleOO{-1}{1}$. OP $S(x) = \sum_{n=0}^{+\infty} n^2 x^n$. 
            \begin{align*}
                S(x) 
                &= \sum n(n-1) x^n + n x^n \\
                &\quad \downarrow \quad R = 1 \text{ pour les deux SE} \\
                &= \sum n(n-1) x^n + \sum n x^n \\
                &= x^2 \sum n(n-1)x^{n-2} + x \sum n x_{n-1} \\
                &= x^2 \left(\sum_{n=0}^{+\infty} x^n\right)''+ x \left(\sum_{n=0}^{+\infty}\right)' \\
                &= x^2 \left(\frac{1}{1-x}\right)'' + x \left(\frac{1}{1-x}\right)' \\
                &= \frac{2 x^2}{(1-x)^3} + \frac{x}{(1-x)^2} \\
                &= \frac{x(1 + x)}{(1-x)^3}
            \end{align*}
            Ainsi, pour tout $x \in \intervalleOO{-1}{1}$, on a 
            \[ \sum_{n=0}^{+\infty} n^2 x^n = \frac{x(1+x)}{(1-x)^3} \]   
        \end{enumerate}
    \end{demo}

    On a obtenu ce qu’on appelle un développement en série entière de l’application $f : x \mapsto \frac{x(1+x)}{(1-x)^3}$. De la même façon, on a obtenu précédemment un DSE de l’application $\ln(1+x) = \sum_{n=0}^{+\infty} (-1)^n \frac{x^{n+1}}{n+1}$.

\subsection{Développement en série entière}

    Dans cette section, toute les séries entières seront réelles.

    \subsubsection{Fonction développable en série entière}

    \begin{defi}{Fonction développable en série entière}{}
        \begin{soit}
            \item $I$ un intervalle de $\mathbb{R}$
            \item $f : I \to \mathbb{R}$
            \item $R_0 > 0$ tel que $\intervalleOO{-R_0}{R_0} \subset I$
        \end{soit}
        On dit que $f$ est \textbf{développable en série entière} sur $\intervalleOO{-R_0}{R_0}$ s’il existe une série entière $\sum a_n x^n$ telle que 
        \[ \forall x \in \intervalleOO{-R_0}{R_0}, \quad f(x) = \sum_{n=0}^{+\infty} a_n x^n \]

        On dira de même que $f$ est développable en série entière \textbf{au voisinage de} $x_0$ s’il existe $\delta > 0$ tel que l’application $x \longmapsto f(x_0 + x)$ est développable en série entière sur $\intervalleOO{-\delta}{\delta}$.
    \end{defi}

    Il faut notamment que la SE $\sum a_n (x-x_0)^n$ aie un RDC $R \geq R_0$. 

    Il est important de garder en tête le fait que toute fonction de classe $\mathcal{C}^{\infty}$ n’a pas nécessairement de DSE en tout point. Par exemple, OP $f : x \mapsto \sisi{0}{x \leq 0}{e^{-1/x^2}}{x > 0}$. Alors $f$ est $\mathcal{C}^{\infty}$ sur $\mathbb{R}$ et n’admet pas de DSE au voisinage de $0$.

    \begin{omed}{Exemple}{myyellow}
        Soit $\fonction{f}{\mathbb{R}_+^*}{\mathbb{R}}{x}{\sisinon{1}{x=1}{\frac{\ln(x)}{x-1}}}$. Montrons que $f$ admet un DSE(1). OP $g(u) = f(1+u) = \sisinon{1}{u=0}{\frac{\ln(1+u)}{u}}$. On sait que $\forall u \in \intervalleOO{-1}{1}$, $\ln(1+u) = \sum_{n=0}^{+\infty} (-1)^n \frac{u^{n+1}}{n+1}$. Donc si $u \in \intervalleOO{-1}{1} \backslash \left\{0\right\}$, on a 
        \[ \frac{\ln(1+u)}{u} = \sum_{n=0}^{+\infty} (-1)^n \frac{u^n}{n+1} \]   
        Donc si $x \in \intervalleOO{0}{1} \cup \intervalleOO{1}{2}$, 
        \begin{align*}
            f(x) &= g(x-1) \\
            &= \sum_{n=0}^{+\infty} (-1)^n \frac{(x-1)^n}{n+1} \\
            &= \sum_{n=0}^{+\infty} \frac{(1-x)^n}{n+1}
        \end{align*}
        On remarque que cette identité est vraie si $x = 1$, donc on a obtenu un DSE de $f$ sur $\intervalleOO{0}{2}$. 
    \end{omed}

    \begin{prop}{}{}
        Soit $f : I \to \mathbb{R}$ développable en série entière sur $\intervalleOO{-R}{R}$ avec, pour tout $x \in \intervalleOO{-R}{R}$, 
        \[ f(x) = \sum_{n=0}^{+\infty} a_n x^n \]
        \begin{alors}
            \item Toute primitive $F$ de $f$ est développable en série entière sur $\intervalleOO{-R}{R}$, avec 
            \[ \forall x \in \intervalleOO{-R}{R}, \quad F(x) = F(0) + \sum_{n=1}^{+\infty} \frac{a_{n-1}}{n} x^n \]
            \item La dérivée $f'$ de $f$ est développable en série entière sur $\intervalleOO{-R}{R}$, avec 
            \[ \forall x \in \intervalleOO{-R}{R}, \quad f'(x) = \sum_{n=0}^{+\infty} (n+1) a_{n+1} x^n \]
        \end{alors}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On sait que $\sum a_n x^n$ et $\sum n a_n x^{n-1}$ ont même RDC, et on sait que pour tout $x \in \intervalleOO{-R}{R}$, 
        \[ f'(x) = \sum_{n=1}^{+\infty} n a_n x_{n-1} = \sum_{n=0}^{+\infty} (n+1) a_{n+1} x^n \]   
        De plus, on sait que $\sum a_n x^n$ et $\sum a_n \frac{x^{n+1}}{n+1}$ ont même RDC, et que pour tout $x \in \intervalleOO{-R}{R}$, 
        \begin{align*}
            F(x) - F(0) 
            &= \int_{0}^{x} f(t)dt \\
            &= \sum_{n=0}^{+\infty} a_n \frac{x^{n+1}}{n+1}
        \end{align*}
    \end{demo}

    \begin{prop}{Stabilité des développements en SE}{}
        Si $f$ et $g$ sont deux fonctions développables en SE sur $\intervalleOO{-R}{R}$ et $\intervalleOO{-R'}{R'}$, telles que 
        \[ f(x) = \sum a_n x^n \esp{et} g(x) = \sum b_n x^n \]   
        Alors $f+g$ et $fg$ sont DSE(0) avec, pour $x \in \intervalleOO{-\min(R,R')}{\min(R,R')}$,
        \[ (f+g)(x) = \sum (a_n + b_n) x^n \esp{et} (fg)(x) = \sum \left(\sum_{k=0}^{n} a_k b_{n-k}\right) x^n \]
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        \begin{enumerate}
            \item On sait que la SE $\sum (a_n + b_n) x^n$ a un RDC $R'' \geq \min(R,R')$ puis l’expression est donnée par somme.
            \item On sait que la SE $\sum (\sum_{k=0}^{n} a_n b_{n-k})x^n$ a un RDC $R''' \geq \min(R,R')$ puis l’expression est donnée par produit de Cauchy.
        \end{enumerate}
    \end{demo}

    \subsubsection{Exemples} 

    \begin{theo}{CS de DSE(0)}{}
        Soit $f \in \mathbb{C}^{\infty}(\intervalleOO{-R}{R})$. OS qu’il existe $M$ tel que $\forall k \in \mathbb{N}, \forall x \in \intervalleOO{-R}{R}, \abs{f^{(k)}(x)} \leq M$. Alors $f$ est DSE(0), sur $\intervalleOO{-R}{R}$. 

        Dans ce cas, $\forall x \in \intervalleOO{-R}{R}$, 
        \[ f(x) = \sum_{n=0}^{+\infty} \frac{f^{(n)}(0)}{n!} x^n \]   
    \end{theo}

    \begin{demo}{Démonstration}{myred}
        Soit $N \in \mathbb{N}$, $x \in \intervalleOO{-R}{R}$, on sait que 
        \[ \abs{f(x) - \sum_{n=0}^{N} \frac{f^{(n)}(0)}{n!} x^n} \leq \nnorm{\infty}{f^{(N+1)}} \frac{\abs{x}^{N+1}}{(N+1)!} \leq M \frac{R^{N+1}}{(N+1)!} \limi{N}{+\infty} 0 \] 
    \end{demo}

    \begin{omed}{Exemples}{myred}
        \begin{enumerate}[label=\textcolor{myred}{\alph*.}]
            \item \textbf{sin et cos} \quad $\sin, \cos \in \mathcal{C}^{\infty}(\mathbb{R})$, et $\forall x \in \mathbb{R}, \forall k \in \mathbb{N}$, on peut prendre $M = 1$. Ainsi, 
            \begin{align*}
                \cos(x) &= \sum_{n=0}^{+\infty} \frac{\cos^{(n)}(0)}{n!} x^n \\
                &= \sum_{n = 0}^{+\infty} (-1)^n \frac{x^{2n}}{(2n)!} \\
                \sin(x) &= \sum_{n=0}^{+\infty} \frac{\sin^{(n)}(0)}{n!} x^n \\
                &= \sum_{n=0}^{+\infty} (-1)^n \frac{x^{2n + 1}}{(2n+1)!}
            \end{align*}
            \item \textbf{exp} \quad $\exp \in \mathcal{C}^{\infty}(\mathbb{R})$ et en prenant $M = e^R$, on obtient que 
            \begin{align*}
                \exp(x) = \sum_{n=0}^{+\infty} \frac{x^n}{n!}
            \end{align*}
            \item OP $f(x) = \sisinon{0}{x \leq 0}{e^{-1/x^2}}$. $f \in \mathcal{C}^{\infty}(\mathbb{R})$. On cherche $f^{(k)}(0)$. On se ramène pour cela à l’utilisation du théorème de la limite de la dérivée, en procédant par récurrence :
            \[ (H_n) : \left\{ \begin{array}{l}
                f \in \mathcal{C}^n(\mathbb{R}) \\
                f^{(n)}(0) = 0 \\
                \forall x \in \mathbb{R}^*_+, \exists P_n \in \mathbb{R}[X], f^{(n)}(x) = \frac{P_n(x)}{x^{3n}} e^{-1/x^2} 
            \end{array} \right. \]
            \begin{itemize}
                \item[\textbf{I}] $\lim_{x \to 0^+} f(x) = 0 = f(0)$ donc $f \in \mathcal{C}^0(\mathbb{R})$, et on peut prendre $P_0 = 1$, donc $(H_0)$ est vraie.
                \item[\textbf{H}] Par hypothèse, $f \in \mathcal{C}^n(\mathbb{R})$ et $f \in \mathcal{C}^{n+1}(\mathbb{R}^*)$. On applique donc le théorème de la limite de la dérivée à $f^{(n)}$. Si $x \in \mathbb{R}_+^*$,
                \begin{align*}
                    f^{(n+1)}(x) 
                    &= \left(\frac{2}{x^3} \frac{P_n(x)}{x^{3n}} + \frac{P_n'(x)}{x^{3n}} - \frac{3n P_n(x)}{x^{3n + 1}}\right)\times e^{-1/x^2} \\
                    &= \frac{2 P_n(x) + x^3 P_n'(x) - 3n x^2 P_n(x)}{x^{3(n+1)}} e^{-1/x^2} \\
                    &= \frac{P_{n+1}(x)}{x^{3n+1}} e^{-1/x^2}
                \end{align*}
                On en déduit que $\lim_{x \to 0^+} f^{(n+1)}(x) = 0$. Or $\lim_{x \to 0^-} f^{(n+1)}(x) = 0$, d’où $f^{(n)}$ est dérivable en $0$ et $f^{(n+1)}(0) = 0$. Ainsi, $f \in \mathcal{C}^{n+1}(\mathbb{R})$ d’où $(H_{n+1})$ est vraie.
            \end{itemize}
            Ainsi, on a bien que $f \in \mathcal{C}^{\infty}(\mathbb{R})$. OS que $f$ est DSE(0), \textit{i.e.} il existe $R > 0$ tel que 
            \[ \forall x \in \intervalleOO{-R}{R}, \quad f(x) = \sum_{n=0}^{+\infty} \frac{f^{(n)}(0)}{n!} x^n = 0 \]   
            C’est absurde, d’où la non-existence d’un DSE(0) pour $f$.
        \end{enumerate}
    \end{omed}

    \begin{omed}{Application (\textcolor{black}{Solution d’une ED})}{myred}
        OP $(E) : 4x y'' + 2 y' - y = 0$. Supposons que $(E)$ possède une solution DSE(0) \textit{i.e.} $y(x) = \sum a_n x^n$ de RDC $R$. On sait que $y'(x) = \sum_{n \geq 1} n a_n x^{n-1}$ et $y''(x) = \sum_{n \geq 2} n(n-1) a_n x^{n-2}$ pour tous $x \in \intervalleOO{-R}{R}$. En remplaçant dans $(E)$, on obtient que 
        \begin{align*}
            (E) 
            & \iff \sum_{n=2}^{+\infty} 4n(n-1) a_n x^{n-1} + \sum_{n=1}^{+\infty} 2 n a_n x^{n-1}  - \sum_{n=0}^{+\infty} a_n x^n = 0 \\
            & \iff \sum_{n=1}^{+\infty} 4 (n+1) n a_{n+1} x^n + \sum_{n=0}^{+\infty} 2 (n+1) a_{n+1} x^n - \sum_{n=0}^{+\infty} a_n x^n = 0 \\
            & \iff \sum_{n = 0}^{+\infty } 4 (n+1) n a_{n+1} x^n + \sum_{n=0}^{+\infty} 2 (n+1) a_{n+1} x^n - \sum_{n=0}^{+\infty} a_n x^n = 0 \\
            & \iff \sum_{n=0}^{+\infty} \left(2(2n+1)(n+1) a_{n+1} - a_n\right)x^n = 0 
        \end{align*}
        Donc $y$ est solution de $(E)$ \textit{ssi} $\forall n \in \mathbb{N}$, $a_{n+1} = \frac{1}{(2n+2)(2n+1)}$. Ainsi, $a_n = \frac{1}{(2n)!} a_0$. En calculant $\frac{a_{n+1}}{a_n} = \frac{1}{(2n+2)(2n+1)} \limi{n}{+\infty} 0$ d’où $R = +\infty$. Ainsi, la fonction $y(x) = a_0 \sum_{n=0}^{+\infty} \frac{x^n}{(2n)!}$ est solution sur $\mathbb{R}$.
    \end{omed}

    \subsubsection{DSE usuels}

    \begin{prop}{Exponentielle complexe}{}
        Pour tout $z \in \mathbb{C}$, on a 
        \[ e^z = \sum_{n=0}^{+\infty} \frac{z^n}{n!} \qquad R = +\infty \]
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Posons $z = a + ib$. On a $e^z = e^a e^{ib} = e^a (\cos(b) + i \sin(b))$ où $e(a) = \sum_{n=0}^{+\infty} \frac{a^n}{n!}$, $\cos(b) = \sum_{n=0}^{+\infty} (-1)^n \frac{b^{2n}}{(2n)!}$ et $\sin(b) = \sum_{n=0}^{+\infty} (-1)^n \frac{b^{2n+1}}{(2n+1)!}$. Ainsi,
        \begin{align*}
            \cos(b) + i \sin(b) &= \sum_{n=0}^{+\infty} \lambda_n b^n \esp{où} \lambda_n = \sisi{\frac{(-1)^p}{(2p)!} = \frac{i^{2p}}{(2p)!}}{n = 2p}{\frac{i(-1)^p}{(2p+1)!} = \frac{i^{2p+1}}{(2p+1)!}}{n = 2p + 1} = \frac{i^{n}}{n!} \\
            e^a (\cos(b) + i \sin(b)) 
            &= \sum_{n=0}^{+\infty} \left(\sum_{k=0}^n \frac{a^k}{k!} \lambda_{n-k} b^{n-k}\right) \\
            &= \sum_{n=0}^{+\infty} \left(\sum_{k=0}^{n} \frac{a^k (ib)^{n-k}}{k! (n-k)!}\right) \\
            &= \sum_{n=0}^{+\infty} \left(\frac{1}{n!} (a + ib)^n \right) \\
            &= \sum_{n=0}^{+\infty} \frac{z^n}{n!}
        \end{align*}
    \end{demo}

    \begin{prop}{Fonctions hyperboliques}{}
        \begin{align*}
            \cosh(x) &= \sum_{n=0}^{+\infty} \frac{x^{2n}}{(2n)!} & R = +\infty \\
            \sinh(x) &= \sum_{n=0}^{+\infty} \frac{x^{2n+1}}{(2n+1)!} & R = +\infty
        \end{align*}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Soit $x \in \mathbb{R}$. 
        \begin{align*}
            \cosh(x) 
            &= \frac{e^x + e^{-x}}{2} \\
            &= \frac{1}{2} \left( \sum_{n=0}^{+\infty} \frac{x^n}{n!} + \sum_{n=0}^{+\infty} \frac{(-x)^n}{n!} \right)
            &= \sum_{n=0}^{+\infty} \frac{x^{2n}}{(2n)!}
        \end{align*}
        De même pour sinh.
    \end{demo}

    \begin{prop}{Logarithmes}{}
        \begin{align*}
            \ln(1-x) &= - \sum_{n=1}^{+\infty} \frac{x^n}{n} & R = 1 \\
            \ln(1+x) &= \sum_{n=1}^{+\infty} (-1)^{n+1} \frac{x^n}{n} & R = 1 
        \end{align*}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        On sait que $\frac{1}{1-x} = \sum_{n=0}^{+\infty} x^n$ a pour RDC $R = 1$. En intégrant entre $0$ et $x$, on obtient 
        \[ -\ln(1-x) = \sum_{n=0}^{+\infty} \frac{x^{n+1}}{n+1} = \sum_{n=1}^{+\infty} \frac{x^n}{n} \]  
        puis remplacer par $-x$ pour le second.
    \end{demo}

    \begin{prop}{}{}
        Soit $\alpha \in \mathbb{R} \backslash \mathbb{N}$.
        \begin{align*}
            (1+x)^{\alpha} = \sum_{n=0}^{+\infty} \frac{\alpha(\alpha-1)\cdots(\alpha-n+1)}{n!} x^n 
        \end{align*}
    \end{prop}

    \begin{demo}{Démonstration}{myolive}
        OP $f(x) = (1+x)^{\alpha}$. $f$ est dérivable sur $\intervalleOO{-1}{1}$ avec $f'(x) = \alpha(1+x)^{\alpha - 1}$. Ainsi, $f$ est l’unique solution du système de Cauchy 
        \[ (C) : \et{(1+x)y' - \alpha y = 0}{y(0) = 1} \quad x \in \intervalleOO{-1}{1} \]    
        Soit $y(x) = \sum_{n=0}^{+\infty} a_n x^n$ pour $x \in \intervalleOO{-R}{R}$. En remplaçant dans $(C)$, on obtient 
        \begin{align*}
            (C)
            &\iff (1+x) \sum_{n=0}^{+\infty} (n+1) a_{n+1} x^n - \alpha \sum_{n=0}^{+\infty} a_n x^n = 0 \\
            &\iff \sum_{n=0}^{+\infty} (n+1) a_{n+1} x^n + \sum_{n=0}^{+\infty} n a_n x^n - \sum_{n=0}^{+\infty} \alpha a_n x^n = 0 \\
            &\iff \sum_{n=0}^{+\infty} \left((n+1)a_{n+1} + (n-\alpha) a_n\right)x^n = 0 
        \end{align*}
        Donc $y$ est solution de $(E)$ \textit{ssi} $\forall n \in \mathbb{N}$, $a_{n+1} = \frac{\alpha - n}{n+1} a_n$. En itérant, on trouve que $a_n = \frac{(\alpha - n + 1) \cdots (\alpha - 1) \alpha}{n!} a_0$. De plus, $y(0) = a_0 = 1$. Enfin, $\frac{a_{n+1}}{a_n} \limi{n}{+\infty} 1$ donc $R = 1$.

        Par unicité de la solution, on obtient le DSE voulu.
    \end{demo}

    \begin{coro}{Cas particuliers}{}
        \begin{align*}
            \frac{1}{\sqrt{1 + x}} &= \sum_{n=0}^{+\infty} (-1)^n \frac{1 \times 3 \times \cdots \times (2n-1)}{2 \times 4 \times \cdots 2n} x^n & R = 1 \\
            \sqrt{1 + x} &= 1 + \sum_{n=1}^{+\infty} (-1)^{n-1} \frac{1 \times 3 \times \cdots \times (2n-3)}{2 \times 4 \times \cdots \times 2n} x^n & R = 1
        \end{align*}
    \end{coro}

    \begin{prop}{}{}
        \begin{align*}
            \arctan(x) &= \sum_{n=0}^{+\infty} (-1)^n \frac{x^{2n+1}}{2n+1} & R = 1 \\
            \argtanh(x) &= \frac{1}{2} \ln\left(\frac{1 + x}{1 - x}\right) \\
            &= \sum_{n=0}^{+\infty} \frac{x^{2n+1}}{2n+1} & R = 1
        \end{align*}
    \end{prop}

    \begin{demo}{Preuve}{myolive}
        Dans un premier temps, on calcule le DSE de $\frac{1}{1 + x^2}$ sur $\intervalleOO{-1}{1}$, que l’on intègre. Celui-ci sera $\sum_{n=0}^{+\infty} (-1)^n x^{2n}$ et donc $\arctan(x) = 0 + \sum_{n=0}^{+\infty} (-1)^n \frac{x^{2n+1}}{2n+1}$. 

        Secondement, on part de l’expression de $\argtanh$, 
        \begin{align*}
            \argtanh 
            &= \frac{1}{2} \left(\ln(1+x) - \ln(1 - x)\right) \\
            &= \frac{1}{2} \left(\sum_{n=1}^{+\infty} (-1)^{n+1} \frac{x^n}{n} +- \sum_{n=1}^{+\infty} \frac{x^n}{n} \right) \\
            &= \sum_{n=1}^{+\infty} \frac{x^{2n+1}}{2n+1}
        \end{align*}
        On aurait aussi pu intégrer le DSE de la dériveé de $\argtanh$, qui vaut $\frac{1}{1-x^2}$.
    \end{demo}

    \subsubsection{Nouveaux exemples}

    \begin{omed}{Exemple}{mybrown}
        Soit $p \in \mathbb{N}$, et $f : x \mapsto \frac{1}{(1-x)^p}$. On cherche un DSE(0) de $f$.
        \begin{itemize}
            \item $f'(x) = \frac{p}{(1-x)^p}$ donc $f$ vérifie $(1-x)y' - py = 0$ avec $f(0) = 1$.
            \item On peut se ramener à la formule de $(1+x)^{\alpha}$, avec $\alpha = -p$ et $x \leftarrow -x$.
            \item On peut aussi voir que $\left(\frac{1}{1-x}\right)^{(p)} = \frac{p!}{(1-x)^{p+1}}$, d’où, pour tout $x \in \intervalleOO{-1}{1}$, 
            \begin{align*}
                f(x) 
                &= \frac{1}{(p-1)!} \left(\frac{1}{1-x}\right)^{(p-1)} \\
                &= \frac{1}{(p-1)!} \left(\sum_{n=0}^{+\infty} x^n \right)^{(p-1)} \\
                &= \frac{1}{(p-1)!} \left(\sum_{n=0}^{+\infty} \frac{(n+p-1)!}{n!}  x^n\right) \\
                &= \sum_{n=0}^{+\infty} \binom{n+p-1}{n} x^n & R = 1
            \end{align*}
        \end{itemize}
    \end{omed}

    \begin{omed}{Exemple 2}{mybrown}
        Calculons le DSE(0) de $f : x \mapsto \frac{\ln(1+x)}{1+x}$. 
        
        On peut utiliser le produit de Cauchy, en sachant que, sur l’intervalle $\intervalleOO{-1}{1}$,
            \[ \ln(1+x) = \sum_{n=1}^{+\infty} (-1)^{n+1} \frac{x^n}{n} \esp{et} \frac{1}{1+x} = \sum_{n=0}^{+\infty} (-1)^n x^n \]
            OP $a_n = \sisinon{0}{n=0}{\frac{(-1)^{n+1}}{n}}$ et $b_n = (-1)^n$. Alors lorsque $n \geq 1$, $c_n = \sum_{k=0}^{n} a_k b_{n-k} = (-1)^{n+1} \sum_{k=1}^{n} \frac{1}{k} = (-1)^{n+1} H_n$ et $c_0 = 0$. Finalement, 
            \begin{align*}
                f(x) &= \sum_{n=1}^{+\infty} (-1)^{n-1} H_n x^n & R > 1  \\
                &\quad \downarrow \quad \text{divergence en } x = 1 \\
                &= \sum_{n=1}^{+\infty} (-1)^{n-1} H_n x^n & R = 1
            \end{align*}
    \end{omed}

    \begin{omed}{Exemple 3}{mybrown}
        Calculons le DSE(0) de $f : x \mapsto \sqrt{2-x}$. OR que $f(x) = \sqrt{2}\sqrt{1 - \frac{x}{2}}$, et on sait que, pour $x \in \intervalleOO{-1}{1}$,
        \[ \sqrt{1 + x} = 1 + \sum_{n=1}^{+\infty} (-1)^{n-1} \frac{1 \times 3 \times \cdots \times (2n-3)}{2 \times 4 \times \cdots \times 2n} x^n \] 
        Donc pour $x \in \intervalleOO{-2}{2}$, 
        \begin{align*}
            \sqrt{1 - \frac{x}{2}} 
            &= 1 - \sum_{n=1}^{+\infty} \frac{1 \times 3 \times \cdots \times (2n-3)}{2 \times 4 \times \cdots \times 2n} \frac{1}{2^n} x^n \\
            f(x) 
            &= \sqrt{2} - \sqrt{2} \sum_{n=1}^{+\infty} \frac{1 \times 3 \times \cdots \times (2n-3)}{2 \times 4 \times \cdots \times 2n} \frac{x^n}{2^n}
        \end{align*}  
    \end{omed}

    \begin{omed}{Exemple 4}{mybrown}
        Quel est le RDC de la SE $\sum n^{((-1)^n)} x^n$ ? 

        OP $a_n = n^{((-1)^n)} = \sisi{n}{n \text{ pair}}{\frac{1}{n}}{n \text{ impair}}$. $a_n = \comp{\mathcal{O}}{n}{+\infty}{n}$, et $\sum n x^n$ est de RDC $1$, donc $R \geq 1$. Or, si $x = 1$, il y a divergence grossière, d’où $R = 1$.

        OP $x \in \intervalleOO{-1}{1}$
        \begin{align*}
            S(x) 
            &= \sum_{n=0}^{+\infty} n^{(-1)^n} x^n \\
            &= \sum_{p=0}^{+\infty} 2p x^{2p} + \sum_{p=1}^{+\infty} \frac{1}{2p+1} x^{2p+1} \\
            &= x\left(\sum_{p=0}^{+\infty} x^{2p}\right)' + \argtanh(x) \\
            &= x \left(\frac{1}{1 - x^2}\right)' + \argtanh(x) \\
            &= \frac{2x^2}{(1 - x^2)^2} + \argtanh(x)
        \end{align*}
    \end{omed}

    \begin{omed}{Exemple 5}{mybrown}
        On a vu précédemment qu’une solution de $4xy'' + 2 y' - y = 0$ est de la forme $f(x) = a_0 \sum_{n=0}^{+\infty} \frac{x^n}{(2n)!}$. On sait que 
        \[ \cosh(x) = \sum_{n=0}^{+\infty} \frac{x^{2n}}{(2n)!} \esp{avec} R = +\infty \]    
        \begin{itemize}
            \item Dans le cas où $x \geq 0$, $x = \left(\sqrt{x}\right)^2$ donc $f(x) = a_0 \cosh(\sqrt{x})$. 
            \item Dans le cas où $x < 0$, $x = - \left(\sqrt{-x}\right)^2$ donc $f(x) = a_0 \sum_{n=0}^{+\infty} (-1)^n \frac{(\sqrt{-x})^{2n}}{(2n)!} = a_0 \cos\sqrt{-x}$.
        \end{itemize}
        Comme $f$ est DSE sur $\mathbb{R}$, $f \in \mathcal{C}^{\infty}$.
    \end{omed}

    



